---
title: First Step to RL
date: 2021-12-03 16:48:41
tags:
- RL
categories:
- RL


---



强化学习的入门介绍

什么是强化学习，关于强化学习的几点疑问

深度强化学习算法：策略网络(Policy Network)和估值网络(Value Network)

对Policy Based和 Value Based的一些分析和对比

<!-- more -->

# 什么是强化学习，关于强化学习的几点疑问

<font color=green size=3>强化学习三要素：环境状态，行动，奖励</font>

<font color=green size=3>目标：尽量多的获得奖励</font>

<font color=green size=3>本质：连续决策</font>

基本的强化学习模型包括：

* 环境状态的集合S
* 动作的集合A
* 状态之间的转换规则（是环境的一部分）
* 规定转换后“即时奖励”的规则（是环境的一部分）
* 描述主体（智能体）能够观察到什么的规则（是环境的一部分）
* 能够做出决策/动作的主体（智能体）

## 区别于深度学习，强化学习的本质特点是什么？深度学习解决的是可导的问题，强化学习解决的是离散的问题，是这样吗？

**两个定义**

1，强化学习是机器学习的一个重要分支，主要用来解决连续决策问题。

2，强化学习又称 再励学习，评价学习或增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在于环境交互过程中通过学习策略以达成回报最大或实现特定目标的问题。

强化学习的本质是<font color=red> 描述和解决智能体（agent）在于环境交互过程中通过学习策略以达成回报最大或实现特定目标的问题。</font>它本质是这样一种场景，在这种场景中它为了达到某种目的，做出连续的决策。这样符合这种场景，那就是强化学习。

强化学习和深度学习这两个名词的维度是不一样的。深度学习描述的是算法本身的特点，深度够不够，是不是连接主义的模型。深度学习可以用来做无监督，也可以用来做半监督，也可以用来做弱监督，甚至可以用来作为强化学习算法的一部分。深度学习这个名词，不管应用场景只管模型本身是不是满足深度学习的特点。

而强化学习描述的是应用场景的特点，只要能提供智能体决策的算法，管它是什么模型，什么结构，那就是强化学习的算法。从这个角度上讲，我现在认为强化学习和监督学习，无监督学习，弱监督学习，是并列的，是对应用场景的描述。和深度学习不是同一维度的。

<font color=red>强化学习的本质在于目标给定的形式，不像无监督学习那样完全没有学习目标，也不像监督学习那样有非常明确的目标（label），强化学习的目标一般是变化的，不明确的，甚至可能不存在绝对正确的目标。，强化学习的问题都可以抽象成，环境状态，行动和奖励。应该说只要能抽象为这三个要素，目标是获得最大奖励的模型，就是强化学习的模型。</font>

强化学习的最大的特点是“试错”，是尝试各种可能，而强化结果好的可能。（策略网络的特点，估值网络的特点是修正和预测获益）

由于强化学习是一种决策学习，这个问题的特点就是离散型。但是离散并不就是强化学习（连续的决策才是，目标的模糊和不确定性是决策问题的特点）。深度学习本质是函数的拟合，所以连续可微是它的特点。并不能说连续可微的问题就是深度学习，离散的问题就是强化学习。 

## 深度学习用梯度下降的算法实现模型的优化，强化学习无法求导，甚至连学习目标都是模糊的如何优化模型参数？

这个问题要了解下一节，具体的强化学习方法。

## 强化学习与弱监督学习？

弱监督是属于有监督学习，只是它学习的目标不是被给予的标签，而是比被给予标签更强的标签（强弱是指标签做含有的信息量），也就是说弱监督是根据少量信息的标签，推测出更多的信息。

强化学习的本质是连续决策。连续决策的特点是目标的模糊和不确定性。

所以，虽然弱监督和强化学习都没有给出最终准确的目标，但是他们任然很不同的

## 深度学习是死的，没有智能的机器学习算法，强化学习是“活的”，有智能的机器学习算法

深度学习大部分我们用作有监督的学习算法。其实说深度学习是死的，不如说有监督学习是死的。

有监督学习其实是完全的复刻标签里面含有的知识，它的本质就是一个函数拟合的问题，它无法摆脱对绝对的标签的依赖，无法超越标签。

而强化学习，正由于它的目标是模糊和不确定的。使得算法在设计上必须具有随机性和探索性，它能够探索出人类从来没有到过的领域。就像在围棋上，下出人类完全无法理解的棋，人从来没有想过的一些下法。这就是强化学习算法探索出来的知识。所以我觉得它是活着的，拥有智能的算法。

<font color=blue>从感性的层面，强化学习算法很接近人脑的行为：感知环境，探索环境，强化有益行为</font>

## 关于深度学习，强化学习，连续可导性和离散不可导性的讨论

举一个下棋的例子，下棋可以用强化学习算法，下棋这个问题是离散的吗？

其实它不是离散的，如果说它是离散的那么图像分类也是离散的。

所以强化学习和离散，深度学习和连续可导的关系究竟是什么样的？

深度学习必须有标签（如果作为无监督学习的工具，标签是其自身）。从函数的角度，输入到标签的函数必须是连续可导的。深度学习其实就是函数的拟合，而拟合的函数必须是连续可导的。

那么强化学习呢？强化学习要学习到的环境状态与Action之间的函数关系，一定也是连续可导的，或者说一定是可以用连续可导的函数拟合的，不然无法用深度学习模型作为智能体。

所以强化学习和离散性有什么关系？

强化学习给人离散的感觉，是源于神经架构搜索。深度学习只能优化参数，不能自动优化网络结构，因为网络结构是不可导的，网络结构中某一模块的选用和丢弃是离散的。强化学习可以做 这件事情，所以把强化学习和离散，不可导问题挂上钩。

# 策略网络(Policy Network)和估值网络(Value Network)

AlphaGo 使用了快速走子，策略网络，估值网络和蒙特卡洛搜索树等技术。

强化学习算法的一个关键是<font color=blue>随机性和探索性</font>，我们需要让算法通过试验样本自己学习什么才是某个环境状态下比较好的Action，而不是像有监督学习一样，告诉模型什么是好的Action，因为我们也不知道什么是好的Action.

深度强化学习模型的本质是神经网络，神经网络是工具，根据问题转化以及建模的不同，主要分为策略网络和估值网络。

强化学习中最重要的两类方法**Policy-based,Value-based**。第一种直接预测在某个环境下应该采取的行动（直接输出改采取Action的概率）。第二种预测在某个环境下所有行动的期望价值，然后通过选择q值最高的行动执行策略。

他们都能完成决策，但由于建模的不同，估值网络包含有更多的信息，它不仅能提供决策，还预测了决策带来的收益。

<font color=red>策略网络是隐式的学习了某一Action所带来的全部获益（当前获益+后续获益），而估值网络直接显示的学习Action所带来的全部获益。</font>强化学习算法做出最佳抉择只需要知道哪个Action全部获益最大，策略网络就是这样做的，估值网络不仅学习了哪个Action全部获益最大，还把每个Action的全部获益给计算出来了

<font color=blue>相对来说，策略网络的性能会比估值网络好一些。</font>

<font color=blue>Value Based方法适合仅有少量Action的环境，而Policy Based方法更通用，适合Action种类非常多，或者具有连续取值的Action的环境。结合了深度学习之后，Policy Based方法就变成了策略网络，Value Based方法就变成了估值网络。</font>

## 策略网络(Policy Network)

直接看一个例子，[Policy_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py)

关键代码

 ```python
 score = tf.matmul(layer1,W2)
 probability = tf.nn.sigmoid(score)#网络输出采取Action 1的概率。
 input_y = tf.placeholder(tf.float32,[None,1], \\
                          name="input_y")# 输入采取过的行为，这个行为是随机生成的。
 advantages = tf.placeholder(tf.float32,name="reward_signal") 
 # 输入获益
 loglik = tf.log(input_y*(input_y - probability) + \\
                 (1 - input_y)*(input_y + probability)) 
 # 损失函数，如果行为是1，则增大
 
 #概率，如果行为是 0，则减小概率（相当于也是增加0的概率），也就是说这个损
 #失函数无论当前行为是什么都会增大当前行为的概率。
 loss = -tf.reduce_mean(loglik * advantages) # 这行代码很关键，
 #相当于给损失函数成了一个权重advantages，得到最终的损失函数。
 #advantages是当前试验的全部获益。如果全部获益大，将以更大的权重，增加 
 #当前行为的概率。
 
 #所以，策略网络其实也是一个对抗学习的过程，增加所有采取过行为的概率，只是
 #获益多的行为以更大的权重增加。
 
 ## 一个试验： 由初始状态开始，随机采取一连串的行为
 #（Policy_Network.py 中是根据当前模型输出的概率,来生成随机的行为，但
 #是我感觉直接用0.5的概率随机生成一连串的0和1的行为也是可以的，下面将实
 #验一下），直到任务结束。
 
 ## 由于每个试验，都可以一直行为到任务结束，所以每个action，我们都可以得
 #到它在该试验中的全部获益（当前获益 + 之后所有行为的获益）
 
 ## 随机生成了n个试验，其中又各种各样的决策（随机探索），全部获益大的
 #action，它的advantages也大，那么它的概率就增大的多，它被强化的厉害。
 
 ## 试验生成的过程，实际上就是数据集构建的过程。策略网络的数据集是由环境
 #和一系列随机的行为构成的。它提供了环境在各种行为下的反应（获益）。模型
 #学习为环境带来高获益的行为的规律。
 ```

由上面的代码可知，从策略网络的角度看强化学习的话，强化学习的关键其实是对数据集的构建---如何构建数据集。

在构建数据集的时候，随机探索肯定是必要的。随机探索的结果会得到一系列好的行为，也会得到一系列不好的行为。如何强化好的行为就是算法设计的时候需要注意的。

上面的代码在探索阶段借用了当前的模型，即根据当前模型输出的概率随机生成行为，从而形成数据集。如果完全的随机（一直使用0.5的概率随机的生成Action）会什么样呢？

### 数据集是否可以和模型无关（不随着模型变化）？

关键修改代码

```python
#基于当前模型，根据当前的状态x，生成Action 1的概率
tfprob = sess.run(probability,feed_dict={observations: x})
# 基于预测概率，随机生成行为，并试探环境。生成数据集。
action = 1 if np.random.uniform() < tfprob else 0
```

修改之后

```python
# 注释掉这句，并不需要根据当前模型生成概率
# tfprob = sess.run(probability,feed_dict={observations: x})
# 直接设置概率为0.5，随机完全随机探索生成数据集。
tfprob = 0.5
action = 1 if np.random.uniform() < tfprob else 0
```

## 估值网络(Value Network)

# 对Policy Based和 Value Based的一些分析和对比

