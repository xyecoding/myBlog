---
title: Personal Thought
top: false
cover: false
toc: true
mathjax: true
date: 2021-12-15 16:01:28
password:
summary:
description: 记录自己在学术方面的一些想法。
categories:
  - Machine Learning
  - About Papers
tags:
  - Personal Thought
  - Papers
  - private
---

# Deep Learning

## 泛化性能

### 泛化性能的本质是什么样的拟合曲面是好的，一定要依靠先验，现实中曲面有哪些特点。

深度学习的本质就是函数的拟合。泛化性能是衡量深度学习模型性能的指标。

在两个模型都完全拟合训练集的情况下，i.e.，两个模型所表示的函数在训练点处的函数值
都是完全正确的。泛化能力要求两函数在测试点处也具有正确的函数值。

这个问题其实只能依赖先验。对于拟合曲面的先验，也就是说，什么样的拟合曲面是好的。

比如，如果点 A 处函数值为 1，那么把 A 翻转，函数值也为 1。这是源于目标识别常识的
先验。

数据增广方法 mixup，发现不同图片之间函数值线性变换是好的。这是对函数曲面的另一个
先验。在样本之间，函数线性变换好。

所以，深度学习的本质问题是什么样的面是好的，如果我们知道什么样的面是好的。知道我
们需要拟合的曲面的一些特点，那么就可以利用这些先验来进行数据增广，甚至进行模型的
设计。

比如，要拟合的曲面是几次的。是否需要高次非线性。如果需要我们就要引入特张图之间的
乘法。强化学习的特征选择给模型带来另一种非线性。哪种非线性是好的？要回答这个问题
，就要知道我们需要拟合的那个函数是几次的，是哪种非线性。

**设想如果，有一种可视化方法，直接可视化高维数据。我们用它来可视化在 MNIST,
CIFAR-10 等数据集上都达到 100%准确率的函数。我们总结这些函数曲面有什么特点，光不
光滑？是几次的？有哪些对称性？。总结出适用于所有数据集的共性的曲面特点。那么我们
就能根据这些特点，这些先验，设计出好的算法，提高模型的泛化性能。卷积神经网络之所
以在图像上那么成功，也是源于，它拟合的函数一定具有某种对称性（因为对图像平移不变
），而这种对称性，就是真实图像对应的理想函数所具有的。**

洞察真实数据是什么样的，是提高模型泛化能力的最终途径。

### neural tangent kernel

**最新有论文表示[^1]，泛化性能和正切核有关，还有关于神经网络特征值方面的研究。值
得探索。**

[^1]: NEURAL TANGENT KERNEL EIGENVALUES ACCURATELY PREDICT GENERALIZATION

## 高次非线性与注意力机制

### 特征图乘法

特征图乘法会带来高次非线性。如果把 ResNet 里面的加法改成乘法。

```python
shortcut(x) + out
# 修改为
shortcut(x) * out
```

**可以从两个角度这个乘法**

1. 把 高次非线性，引入到 CNN 中
2. 把 ResNet 的分支做成一个 3D 的 mask，形成一种无参数增加的 attention module

### 和数据集有关

在有的数据集上效果挺好，比如 Cifar-100。但是即使在 Cifar-10 上表现都一般。

### 和优化有关？和表达的曲面性质有关？

原因可能有两个

1. 这样做乘法太粗糙，确实表达能力不行
2. 优化的问题，乘法带来了梯度的问题，比如方差不可控等，导致优化的情况不好。

现在其实也没能很清楚是是哪个问题，缺少研究这个问题的手段。

为了减少优化上的缺陷。尝试过几种方法

1. 为两端加 bn。`bn(shortcut(x)) * bn(out)`

2. 把 bn 的 bias 设置为 1，保证 bn(shortcut(x))和 bn(out)的均值都是 1，这样可以
   产生比较稳定的梯度。这个想法是源于

   **1. 原始的 shortcut(x) + out。相当于 shortcut(x)乘以了一个 1 的因子，out 也
   乘以了一个 1 的因子。**

   **2. 保持恒等映射。当 out 是 0 的时候 shorcut 可以容易的实现恒等映射。**

### sigmoid 容易饱和？

3. 像 attention 一样，把一边或者两边都用 sigmoid 函数
   。`bn(shortcut(x)) * out.sigmoid()`。这样的话，效果也不是很好，而且发现
   sigmoid 函数给人一种感觉。就是训练不充分的感觉，有点像学习率太小，或者减小的
   太快的感觉，所以陷入的局部最优点。猜想这是因为 sigmoid 函数太容易饱和了。
4. soft attention 经常利用 sigmoid 函数，是否也存在过饱和的情况。为什么它没有出
   现过饱和现象。1. 用的不多？ 2. 因为是 2d 或者 1d 的？就拿 SE-Net 举例，每个像
   素点其实也都是乘以了 attention 的。只是空间乘以的是同一个 attention。
