{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/next_8.8/source/images/apple-touch-icon-next.png","path":"images/apple-touch-icon-next.png","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/favicon-16x16-next.png","path":"images/favicon-16x16-next.png","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/favicon-32x32-next.png","path":"images/favicon-32x32-next.png","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/logo.svg","path":"images/logo.svg","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/logo-algolia-nebula-blue-full.svg","path":"images/logo-algolia-nebula-blue-full.svg","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/ye_16.ico","path":"images/ye_16.ico","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/images/ye_32.ico","path":"images/ye_32.ico","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/css/noscript.styl","path":"css/noscript.styl","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/bookmark.js","path":"js/bookmark.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/comments.js","path":"js/comments.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/config.js","path":"js/config.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/comments-buttons.js","path":"js/comments-buttons.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/motion.js","path":"js/motion.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/next-boot.js","path":"js/next-boot.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/pjax.js","path":"js/pjax.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/schedule.js","path":"js/schedule.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/fancybox.js","path":"js/third-party/fancybox.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/pace.js","path":"js/third-party/pace.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/rating.js","path":"js/third-party/rating.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/quicklink.js","path":"js/third-party/quicklink.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/baidu-analytics.js","path":"js/third-party/analytics/baidu-analytics.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/google-analytics.js","path":"js/third-party/analytics/google-analytics.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/analytics/growingio.js","path":"js/third-party/analytics/growingio.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/disqus.js","path":"js/third-party/comments/disqus.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/disqusjs.js","path":"js/third-party/comments/disqusjs.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/gitalk.js","path":"js/third-party/comments/gitalk.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/changyan.js","path":"js/third-party/comments/changyan.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/isso.js","path":"js/third-party/comments/isso.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/livere.js","path":"js/third-party/comments/livere.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/comments/utterances.js","path":"js/third-party/comments/utterances.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/gitter.js","path":"js/third-party/chat/gitter.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/chatra.js","path":"js/third-party/chat/chatra.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/chat/tidio.js","path":"js/third-party/chat/tidio.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/math/katex.js","path":"js/third-party/math/katex.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/math/mathjax.js","path":"js/third-party/math/mathjax.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/search/local-search.js","path":"js/third-party/search/local-search.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/statistics/firestore.js","path":"js/third-party/statistics/firestore.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/search/algolia-search.js","path":"js/third-party/search/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/statistics/lean-analytics.js","path":"js/third-party/statistics/lean-analytics.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/tags/mermaid.js","path":"js/third-party/tags/mermaid.js","modified":1,"renderable":1},{"_id":"themes/next_8.8/source/js/third-party/tags/pdf.js","path":"js/third-party/tags/pdf.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/_posts/Algorithm.md","hash":"d3c8265f341b1f479f940fd01f845a33c26cbc93","modified":1639730782799},{"_id":"source/_posts/An-Introduction-to-Git.md","hash":"5bffb32f7ea47f70f18c5367a9654afc2457ea19","modified":1639730786683},{"_id":"source/_posts/Construct-Your-Blog-with-Hexo-and-Github.md","hash":"72bd982e57011f68292f3f2d8d055bac89a37693","modified":1639712271057},{"_id":"source/_posts/Experiments.md","hash":"e4af00974e4c7a0b54c84c02ce10e45bcfdd8501","modified":1640312776249},{"_id":"source/_posts/First-Step-to-RL.md","hash":"15fb3d81dc2d8640f1dd7e9415be2f6c4133e2c7","modified":1639622354208},{"_id":"source/_posts/Transformer-and-BERT.md","hash":"9961d853a801477fb0ebc1738940c6ae4e7f4127","modified":1639792992773},{"_id":"source/_posts/Personal-Thought.md","hash":"16a7b4ab87f88a826c506fff8c68e8bd123df18b","modified":1640154693872},{"_id":"source/tags/index.md","hash":"3207ebf9794561395cf0c54633880ab070040ade","modified":1638946119840},{"_id":"source/_posts/Tips-in-Papers.md","hash":"8d1d1b6a08ee2b8e44f7682cf01b63292caf9967","modified":1640161796008},{"_id":"source/about/index.md","hash":"05b5e469a2cd6f7de545752294be494eab99936d","modified":1639728236458},{"_id":"source/categories/index.md","hash":"408ea9b07f3b1a1339731fe7b364d88bc5644aff","modified":1637754696896},{"_id":"source/_posts/First-Step-to-RL/q_learning.py","hash":"3a68ec1153d26f2c18c9d34ee5deb5e495b8dca9","modified":1638761670515},{"_id":"source/_posts/First-Step-to-RL/policy_network.py","hash":"b3e8f06360cad0084a0656edaed4539f1b01e327","modified":1638761670499},{"_id":"source/_posts/Tips-in-Papers/ADCM.jpg","hash":"ee5b75f3a063e07624c5f03111f98902d885cd8e","modified":1639703970059},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq1.jpg","hash":"2ea2a74f960377a6a9d0bc98ecfadd8ba09cd316","modified":1639834017991},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq2.jpg","hash":"cf5832e89b29e39275d2fa4a2e39592e97f9a39e","modified":1639834068705},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq3.jpg","hash":"8fa845242bb6f737736d91485ca237e7132e19b5","modified":1639833635328},{"_id":"source/_posts/Tips-in-Papers/Saccader_gl.jpg","hash":"fb8050f8b71a7c667cfa85672b505b3340d2231b","modified":1640144317644},{"_id":"source/_posts/Tips-in-Papers/Saccader_Over.jpg","hash":"deaccd942ffca9e163a9f2c9bdfb69251d1004cb","modified":1639817165825},{"_id":"source/_posts/Tips-in-Papers/Saccader_Cell.jpg","hash":"b17e79bf86bb3422527bf51d200ceceeb53ce936","modified":1639817097223},{"_id":"source/_posts/Tips-in-Papers/Tnet_over.jpg","hash":"ed04ca4049c6f909b0bf51ea57ffa9a2b445e1c8","modified":1640160184675},{"_id":"source/_posts/An-Introduction-to-Git/git.jpg","hash":"db9ed8bb86df7e73d5be3bcae4cc8656e4a7a0ed","modified":1639191445656},{"_id":"themes/next_8.8/.eslintrc.json","hash":"611e15c3fcb41dc68fa8532ee595a1262a1b5a8a","modified":1638944857842},{"_id":"themes/next_8.8/.gitattributes","hash":"aeeca2f1e987d83232d7870d1435a4e3ed66b648","modified":1638944857843},{"_id":"themes/next_8.8/_vendors.yml","hash":"ba72c575e627697a050614411706cb20206d4b71","modified":1638944857855},{"_id":"themes/next_8.8/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1638944857855},{"_id":"themes/next_8.8/package.json","hash":"e527d094273cf3be4766630bbfe6cc8cf1eeb529","modified":1638944857899},{"_id":"themes/next_8.8/renovate.json","hash":"767b077c7b615e20af3cf865813cd64674a9bea6","modified":1638944857899},{"_id":"themes/next_8.8/LICENSE.md","hash":"8cfb03967dd4cbaf3b825271ffce0039aa3fc22a","modified":1638944857853},{"_id":"themes/next_8.8/README.md","hash":"43fe29330352545446a532e6630866251129882a","modified":1638944857854},{"_id":"themes/next_8.8/.editorconfig","hash":"731c650ddad6eb0fc7c3d4a91cad1698fe7ad311","modified":1638944857842},{"_id":"themes/next_8.8/.gitignore","hash":"087b7677078303acb2acb47432165950e4d29b43","modified":1638944857853},{"_id":"themes/next_8.8/_config.yml","hash":"4ab5a431bed77cf248e5a6f2905e1bbedf1d7d3c","modified":1639637912097},{"_id":"themes/next_8.8/.github/issue_label_bot.yaml","hash":"533fbe6b2f87d7e7ec6949063bb7ea7eb4fbe52d","modified":1638944857847},{"_id":"themes/next_8.8/.stylintrc","hash":"6259e2a0b65d46865ab89564b88fc67638668295","modified":1638944857853},{"_id":"themes/next_8.8/.github/CODE_OF_CONDUCT.md","hash":"593ae64e72d43c020a697eac65b1f9c3483ff097","modified":1638944857844},{"_id":"themes/next_8.8/.github/config.yml","hash":"0956bf71b6f36632b63b14d26580458041a5abd2","modified":1638944857847},{"_id":"themes/next_8.8/.github/CONTRIBUTING.md","hash":"2fdca1040427cabfe27cae6754ec5e027ec7092e","modified":1638944857844},{"_id":"themes/next_8.8/.github/label-commenter-config.yml","hash":"a1aa85a2fc66ff0c52c65bd97b0fa282e297a73f","modified":1638944857848},{"_id":"themes/next_8.8/.github/PULL_REQUEST_TEMPLATE.md","hash":"a103e2d875f7434191859e5b42075cfa9a4cbcb3","modified":1638944857846},{"_id":"themes/next_8.8/.githooks/install.js","hash":"305c2a269818466eed9e381b866c6cd1ad7f8afd","modified":1638944857843},{"_id":"themes/next_8.8/.githooks/pre-commit","hash":"b69b9d0b51e27d5d4c87c3242f5067c2cda26e44","modified":1638944857844},{"_id":"themes/next_8.8/.github/labeler.yml","hash":"ff76a903609932a867082b8ccced906e9910533a","modified":1638944857848},{"_id":"themes/next_8.8/.github/release-drafter.yml","hash":"de38f816e3023e0a5c1fd1f3c2b626f78bc35246","modified":1638944857848},{"_id":"themes/next_8.8/docs/AUTHORS.md","hash":"579014d47f45b27fd1618b9709f0efe9585c7449","modified":1638944857856},{"_id":"themes/next_8.8/docs/LICENSE.txt","hash":"d1cd5a8e83d3bbdb50f902d2b487813da95ddfd3","modified":1638944857857},{"_id":"themes/next_8.8/languages/README.md","hash":"b1c96465b3bc139bf5ba6200974b66581d8ff85a","modified":1638944857859},{"_id":"themes/next_8.8/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1638944857856},{"_id":"themes/next_8.8/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1638944857860},{"_id":"themes/next_8.8/languages/ar.yml","hash":"cc7e3e2855348563d746f15c4752b9c63fcdd91a","modified":1638944857859},{"_id":"themes/next_8.8/languages/de.yml","hash":"83023c4246b93a2f89f342afe29a7b9e1185f74f","modified":1638944857859},{"_id":"themes/next_8.8/languages/en.yml","hash":"66445143decfbb5eb7031eb370698e31d5222a7a","modified":1638944857860},{"_id":"themes/next_8.8/languages/fa.yml","hash":"e09fad889ab3ae87874093e1acd51edc9297d869","modified":1638944857861},{"_id":"themes/next_8.8/languages/es.yml","hash":"07955d78028cea2a590c63fdc2c01ca3ee05a727","modified":1638944857860},{"_id":"themes/next_8.8/languages/id.yml","hash":"d7c337ca72efb0bd02ade8b5560c559384ad84dd","modified":1638944857861},{"_id":"themes/next_8.8/languages/fr.yml","hash":"328c255c82e9b561e20a9f51a4d84abc63d1b90a","modified":1638944857861},{"_id":"themes/next_8.8/languages/ja.yml","hash":"57a35b21aca04ce8bca64fb5933f35626c462ea3","modified":1638944857862},{"_id":"themes/next_8.8/languages/ko.yml","hash":"d6e2add7488065ec4f7d21cfcf7f0eaa877a84f4","modified":1638944857862},{"_id":"themes/next_8.8/languages/it.yml","hash":"c038ff0cadbe405750d980bcacfd3900acf96905","modified":1638944857862},{"_id":"themes/next_8.8/languages/nl.yml","hash":"e47858bd1e0d0622c15366ae6c0513d996f589e3","modified":1638944857863},{"_id":"themes/next_8.8/languages/pt.yml","hash":"ff93459250c33d3c7ba06c30164cc4208edf9b33","modified":1638944857863},{"_id":"themes/next_8.8/languages/pt-BR.yml","hash":"305025e932832328b7e2a8a584638a23c462e68f","modified":1638944857863},{"_id":"themes/next_8.8/languages/si.yml","hash":"c15ed758dbad890e856f4fc281208d7b78cc1a59","modified":1638944857864},{"_id":"themes/next_8.8/languages/ru.yml","hash":"7d13108f4a70ff6a162508a49678e4a477fa7b56","modified":1638944857863},{"_id":"themes/next_8.8/languages/zh-CN.yml","hash":"f8379d15038e22ef7039d91272cb4f36842dbbe1","modified":1638944857865},{"_id":"themes/next_8.8/languages/tr.yml","hash":"d3262d2221b0583a52e5d20a3cd1380f5dc49378","modified":1638944857864},{"_id":"themes/next_8.8/languages/uk.yml","hash":"f32871f67c63d26bc4e3e15df9b01f5a41236a50","modified":1638944857864},{"_id":"themes/next_8.8/languages/zh-HK.yml","hash":"c1ee97ceb56da76ecdc7b69fa975f28c8574441b","modified":1638944857865},{"_id":"themes/next_8.8/languages/vi.yml","hash":"e452ea8c48993262a3e8fce9d92072cafabfc734","modified":1638944857865},{"_id":"themes/next_8.8/layout/post.njk","hash":"707a50e50b90df5fbeaf8407d12895d04163a290","modified":1638944857898},{"_id":"themes/next_8.8/layout/_layout.njk","hash":"2842f3e9fdde5bbd14cac89629221e68d80c8ea1","modified":1638944857866},{"_id":"themes/next_8.8/languages/zh-TW.yml","hash":"70c45076ad722b777956048fcc430eac37844c11","modified":1638944857865},{"_id":"themes/next_8.8/layout/page.njk","hash":"fddfdee95f5da86eab8a85d6eb1901996d2153cf","modified":1638944857898},{"_id":"themes/next_8.8/layout/archive.njk","hash":"aa491dba8f746e626c273a920effedf7d0b32170","modified":1638944857897},{"_id":"themes/next_8.8/layout/category.njk","hash":"82f541452cae76a94ee15cb8d8a888f44260a0fd","modified":1638944857897},{"_id":"themes/next_8.8/test/index.js","hash":"983a505399796b9d9e174ba46d89abbdde38f8ee","modified":1638944857965},{"_id":"themes/next_8.8/layout/index.njk","hash":"fa52c3049871e879980cb6abccdea3792ca4ce70","modified":1638944857898},{"_id":"themes/next_8.8/layout/tag.njk","hash":"b6c017d30d08ddd30d66e9c6f3a71aa65d214eac","modified":1638944857899},{"_id":"themes/next_8.8/.github/workflows/label-commenter.yml","hash":"7dec949b13131783e726facb2f4acde0945db1b8","modified":1638944857849},{"_id":"themes/next_8.8/.github/workflows/lock.yml","hash":"58eca481fd71088a8ae1dbc04645bcfc03460b87","modified":1638944857849},{"_id":"themes/next_8.8/.github/workflows/release-drafter.yml","hash":"359b74890a47d784e35a5cc3c7885d5cdf302e82","modified":1638944857852},{"_id":"themes/next_8.8/.github/workflows/labeler.yml","hash":"46d0b29dc561fe571d91fd06a7c8ef606b984c72","modified":1638944857849},{"_id":"themes/next_8.8/.github/workflows/stale.yml","hash":"32e7dfb55ecf8af66aebfed471be09ef2eb10e18","modified":1638944857852},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/config.yml","hash":"daeedc5da2ee74ac31cf71846b766ca6499e9fc6","modified":1638944857845},{"_id":"themes/next_8.8/.github/workflows/linter.yml","hash":"b57d876c90d1645a52bbba8a52d47ad0b0c96140","modified":1638944857849},{"_id":"themes/next_8.8/.github/workflows/tester.yml","hash":"645bb69d0b6cc062c47fabb1ccb2297ccbcfa7f5","modified":1638944857852},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/other.md","hash":"618d07b49f4774cd79613d4001984a19d954a6ad","modified":1638944857846},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/bug-report.md","hash":"032194e7975564176f2109aa8b7c020fa6d5e6b1","modified":1638944857845},{"_id":"themes/next_8.8/.github/ISSUE_TEMPLATE/feature-request.md","hash":"4a7885fe2c8b25be02ab57c345cd862aeeeeacaf","modified":1638944857846},{"_id":"themes/next_8.8/docs/zh-CN/CONTRIBUTING.md","hash":"a09ceb82b45dd8b7da76c227f3d0bb7eebe7d5d1","modified":1638944857858},{"_id":"themes/next_8.8/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"7befb4325b107dd668d9eae3d7e86a34910ce3f2","modified":1638944857858},{"_id":"themes/next_8.8/docs/zh-CN/README.md","hash":"354b0b0a24cbe97cccf2ec8bd97eb7d624fa0dea","modified":1638944857858},{"_id":"themes/next_8.8/layout/_macro/post-collapse.njk","hash":"d9d8e6d7a6a8c80009dd5334cc17fd3e4977a008","modified":1638944857867},{"_id":"themes/next_8.8/layout/_scripts/index.njk","hash":"4eb65641b47ea9b23ed2ddfd69b18f21d7d8f214","modified":1638944857877},{"_id":"themes/next_8.8/layout/_macro/post.njk","hash":"367cafd3acc1c6a045d8a72de0479aabbf4a3559","modified":1638944857867},{"_id":"themes/next_8.8/layout/_scripts/vendors.njk","hash":"0a1470440f11362df2b1cd6b6228e273d9f999d6","modified":1638944857877},{"_id":"themes/next_8.8/docs/ru/README.md","hash":"e1d6bf38cf34972ca2ee5331a727787fe14082a3","modified":1638944857857},{"_id":"themes/next_8.8/layout/_macro/sidebar.njk","hash":"f3c1fc4b3333cb09a40b6b3b9042e5ab277fe885","modified":1639645479653},{"_id":"themes/next_8.8/layout/_partials/comments.njk","hash":"d6b7bb7764e3b471ed6b4e5715f6cbe2dd453f59","modified":1638944857868},{"_id":"themes/next_8.8/layout/_macro/sidebar.njk_backup","hash":"eec74e135d01948361020140c3798769e1e7363b","modified":1639644621600},{"_id":"themes/next_8.8/layout/_partials/pagination.njk","hash":"2de77d533c91532a8a4052000244d0c1693370df","modified":1638944857873},{"_id":"themes/next_8.8/layout/_third-party/fancybox.njk","hash":"53ad3c31762b74e5d29787b37d5e494cc4fded9b","modified":1638944857888},{"_id":"themes/next_8.8/layout/_third-party/index.njk","hash":"33a4a3275474bd3bb2e8d1b0ea01b42dda9ea608","modified":1638944857888},{"_id":"themes/next_8.8/layout/_partials/widgets.njk","hash":"967594ee64805e27b7ff9d957e23ab3f5c948600","modified":1638944857877},{"_id":"themes/next_8.8/layout/_partials/footer.njk","hash":"0347cb6077a969136aac26ebdc205a7817010ee7","modified":1639127865723},{"_id":"themes/next_8.8/layout/_third-party/rating.njk","hash":"d0444179fec512760ab1d4f76928d795b971c884","modified":1638944857891},{"_id":"themes/next_8.8/scripts/events/index.js","hash":"8bca7ae3cebb3857866d718a562c5d8820fcfbe5","modified":1638944857900},{"_id":"themes/next_8.8/scripts/filters/default-injects.js","hash":"0c9a1fe9906672724dbf274154a37bac1915ca2c","modified":1638944857905},{"_id":"themes/next_8.8/layout/_partials/languages.njk","hash":"537026fc120adeef9148c98ebf074207e3810538","modified":1638944857871},{"_id":"themes/next_8.8/scripts/filters/minify.js","hash":"9789307212d729c8cb65e3541348938a1965ff6f","modified":1638944857906},{"_id":"themes/next_8.8/layout/_third-party/quicklink.njk","hash":"73bc15a9c3c5c239ab90efa19a1e721f41f3cb93","modified":1638944857890},{"_id":"themes/next_8.8/scripts/filters/locals.js","hash":"8499b9c8c6cdae8aa7e4f5ec5b4b76037969db76","modified":1638944857905},{"_id":"themes/next_8.8/layout/_third-party/pace.njk","hash":"13b2a77b4858a127f458ea092b6f713b052befac","modified":1638944857890},{"_id":"themes/next_8.8/scripts/filters/number.js","hash":"63735cb9d02921e25b2606490340a70db89abbec","modified":1638945314670},{"_id":"themes/next_8.8/scripts/filters/post.js","hash":"5a132b7f9280a40b3d5fb40928c8cbbe071fe6f6","modified":1638944857906},{"_id":"themes/next_8.8/scripts/helpers/font.js","hash":"0a6fa582a0890ecaf5f03f758a730936e48aeca1","modified":1638944857907},{"_id":"themes/next_8.8/scripts/helpers/engine.js","hash":"18cc82558e7a9f3b6086c41ce9de0c46e807a66c","modified":1638944857906},{"_id":"themes/next_8.8/scripts/helpers/next-config.js","hash":"e73f43f1bcb46965e317285d6831e129a40ea59b","modified":1638944857907},{"_id":"themes/next_8.8/scripts/helpers/next-url.js","hash":"98fc68cf3fcd6253bbb94068ab1d86578a4ef9ea","modified":1638944857908},{"_id":"themes/next_8.8/scripts/helpers/next-vendors.js","hash":"52acbc74c1ead8a77cd3bbcba4e033053683f7d0","modified":1638944857908},{"_id":"themes/next_8.8/source/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1638944857949},{"_id":"themes/next_8.8/source/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":1638944857950},{"_id":"themes/next_8.8/source/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1638944857949},{"_id":"themes/next_8.8/source/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1638944857948},{"_id":"themes/next_8.8/source/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1638944857949},{"_id":"themes/next_8.8/source/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1638944857950},{"_id":"themes/next_8.8/source/css/noscript.styl","hash":"7dc97674c232f6ca71e48b95e3f66472cd8e9c05","modified":1638944857948},{"_id":"themes/next_8.8/source/images/ye_16.ico","hash":"b8fb01b5361da89831d232a831a1532e9822bd72","modified":1638945436254},{"_id":"themes/next_8.8/source/css/_colors.styl","hash":"a88430865c99f47ce1d8240f8895819b8b7b0c06","modified":1638944857912},{"_id":"themes/next_8.8/source/images/ye_32.ico","hash":"375c99cd785d93dd989c36604ffbd10ada71322a","modified":1638945436261},{"_id":"themes/next_8.8/scripts/tags/button.js","hash":"86c71c73a63744efbbbb367612871fede0d69529","modified":1638944857909},{"_id":"themes/next_8.8/scripts/tags/caniuse.js","hash":"8e912c715702addaf0cefe63e580e45b97ae8c3f","modified":1638944857909},{"_id":"themes/next_8.8/scripts/tags/group-pictures.js","hash":"1c609312a71d47f838226346aad5c2e1c35f15dd","modified":1638944857909},{"_id":"themes/next_8.8/scripts/tags/center-quote.js","hash":"b4d12e6fe29089be0f43bafc9eea736602cd16bf","modified":1638944857909},{"_id":"themes/next_8.8/scripts/tags/index.js","hash":"255dd1090e8319b557eeca43571f0e4f8aab013b","modified":1638944857910},{"_id":"themes/next_8.8/source/css/main.styl","hash":"38b8a12681a3a04bed02aa1659054912ed6def11","modified":1638944857948},{"_id":"themes/next_8.8/scripts/tags/label.js","hash":"c18b0e619a779ed40be7f014db92af18f45fbd5c","modified":1638944857910},{"_id":"themes/next_8.8/scripts/tags/link-grid.js","hash":"3f358bb78c5c6fdf45de287f3ead553e3a6a93c2","modified":1638944857910},{"_id":"themes/next_8.8/scripts/tags/note.js","hash":"a12fd53e421400836a3722ae69130969558d6ac0","modified":1638944857911},{"_id":"themes/next_8.8/scripts/tags/mermaid.js","hash":"b3844e168b51a99d495ca05562ffac47677f5728","modified":1638944857911},{"_id":"themes/next_8.8/scripts/tags/pdf.js","hash":"317ba4611020cc840854386dde098dbbe452777e","modified":1638944857911},{"_id":"themes/next_8.8/scripts/tags/tabs.js","hash":"e0ed5fe1bc9d2957952a1aacdf3252d6ef3f9743","modified":1638944857912},{"_id":"themes/next_8.8/scripts/tags/video.js","hash":"f6ad3f52779f0636251238d3cbdc5b6f91cc5aba","modified":1638944857912},{"_id":"themes/next_8.8/source/js/comments.js","hash":"0b4daf0ce610760bd52e95d423f61f3e1c72442a","modified":1638944857951},{"_id":"themes/next_8.8/source/css/_mixins.styl","hash":"2ca820b221fb7458e6ef4fbcff826e1d1cf4b473","modified":1638944857938},{"_id":"themes/next_8.8/source/js/bookmark.js","hash":"1457291a7244b7786ec35b949d97183e4fbd181d","modified":1638944857950},{"_id":"themes/next_8.8/source/js/config.js","hash":"211a9ab35205ccfa6b7c74394bade84da0d00af7","modified":1638944857951},{"_id":"themes/next_8.8/source/js/motion.js","hash":"20b979ebe3671cb415e6e7171485d65cc347086e","modified":1638944857952},{"_id":"themes/next_8.8/source/js/comments-buttons.js","hash":"81ea6cbcdf0357094753d7523919c1eafa38e79f","modified":1638944857951},{"_id":"themes/next_8.8/source/js/next-boot.js","hash":"b0bdb542a809932182cfbb8772328115142a0b77","modified":1638944857952},{"_id":"themes/next_8.8/source/js/pjax.js","hash":"85293c253e0f43540572c4e4615c712325a732e2","modified":1638944857952},{"_id":"themes/next_8.8/source/js/schedule.js","hash":"6dade4388aa6579576a35758075134f573985d57","modified":1638944857953},{"_id":"themes/next_8.8/test/helpers/index.js","hash":"2fb58dca3df2fe53116ee2b1232fa26ebe7b2ce5","modified":1638944857964},{"_id":"themes/next_8.8/test/helpers/next-url.js","hash":"08e84781f1cd54e5634b86877ad9cefae4a78e95","modified":1638944857964},{"_id":"themes/next_8.8/source/js/utils.js","hash":"c13fa66aae52f59f88881738c00ebdcaf0209496","modified":1638944857963},{"_id":"themes/next_8.8/test/helpers/font.js","hash":"6f5076bd3f2724e47b46ca69028393a9b6275cd1","modified":1638944857964},{"_id":"themes/next_8.8/test/tags/center-quote.js","hash":"2ac4b5a358681691a17e736de06fce0b640a7023","modified":1638944857966},{"_id":"themes/next_8.8/test/tags/caniuse.js","hash":"2852be850d9103c25114253a45e6c62e32517de4","modified":1638944857965},{"_id":"themes/next_8.8/test/tags/index.js","hash":"5cad001936a694bf32d59751cc2b68a66199f976","modified":1638944857966},{"_id":"themes/next_8.8/test/tags/group-pictures.js","hash":"8f66d3c6f03fb11d85aa2ab05c9b3c9aa2b4e994","modified":1638944857966},{"_id":"themes/next_8.8/test/tags/button.js","hash":"a50ca44eaec3d91c2958e3157d624cd3e68828c7","modified":1638944857965},{"_id":"themes/next_8.8/test/tags/label.js","hash":"6cad7d84c42511459a89cda3971e8ea5cdee0125","modified":1638944857967},{"_id":"themes/next_8.8/test/tags/mermaid.js","hash":"f718a3d0e303d842e2ca5a3b162539a49e45a520","modified":1638944857967},{"_id":"themes/next_8.8/test/tags/pdf.js","hash":"2d114596a8a180b2f3cd2a9c6528a328961f12d4","modified":1638944857968},{"_id":"themes/next_8.8/test/tags/link-grid.js","hash":"41730266306c02362258384cd73659223928361f","modified":1638944857967},{"_id":"themes/next_8.8/test/tags/video.js","hash":"88db9a3a26cd35525c43c0339fcd1c5965ec9518","modified":1638944857969},{"_id":"themes/next_8.8/test/tags/note.js","hash":"161a81ce749e239d2403681372d48ecc1b51d7b9","modified":1638944857968},{"_id":"themes/next_8.8/layout/_partials/head/head-unique.njk","hash":"bd87e3a877ebab4508fc2b48b41c96b45c4dd970","modified":1638944857869},{"_id":"themes/next_8.8/test/tags/tabs.js","hash":"b19d2592347eae5d6a7a97ca7e8cec03e8f25b51","modified":1638944857968},{"_id":"themes/next_8.8/test/validate/index.js","hash":"560862194991c5963da5a411629d8e6c71d20ee2","modified":1638944857969},{"_id":"themes/next_8.8/layout/_partials/header/index.njk","hash":"1b2ae17f3c394ce310fe2d9ed5f4d07d8cc74ae7","modified":1638944857870},{"_id":"themes/next_8.8/layout/_partials/head/head.njk","hash":"abcc550cb14374fb7452d6edee63967ad9583d1c","modified":1638944857869},{"_id":"themes/next_8.8/layout/_partials/header/brand.njk","hash":"8e08c19e1bd92f3179907b0ff3743d6e2371d7ae","modified":1638944857869},{"_id":"themes/next_8.8/layout/_partials/header/menu-item.njk","hash":"f066390762faf6684a523e2eb943420023aac2b1","modified":1638944857870},{"_id":"themes/next_8.8/layout/_partials/header/menu.njk","hash":"67372599fe025ebe442b73151e5bb56415758356","modified":1638944857870},{"_id":"themes/next_8.8/layout/_partials/page/categories.njk","hash":"b352346dd2cb42f7eeaec5e39d9a2a353b029775","modified":1638944857872},{"_id":"themes/next_8.8/layout/_partials/page/tags.njk","hash":"752df7d12360a077c51a25609916a3ecc1763bb3","modified":1638944857872},{"_id":"themes/next_8.8/layout/_partials/page/schedule.njk","hash":"130e776575d634201d4f8ef3d78dc12624f19fde","modified":1638944857872},{"_id":"themes/next_8.8/layout/_partials/page/page-header.njk","hash":"92553feb26f30f7fc9147bc4ef122908a9da06be","modified":1638944857872},{"_id":"themes/next_8.8/layout/_partials/page/breadcrumb.njk","hash":"9c136edd2248e2d50c1f6110b75e2b75c299bbd7","modified":1638944857871},{"_id":"themes/next_8.8/layout/_partials/header/sub-menu.njk","hash":"940cad08a67e6c361214045096bd3cdffdf44fcf","modified":1638944857870},{"_id":"themes/next_8.8/layout/_partials/post/post-footer.njk","hash":"e3502059bcc443ce932946a9891fcbe8b2bb362d","modified":1638944857874},{"_id":"themes/next_8.8/layout/_partials/post/post-copyright.njk","hash":"0ebc0142abebbeef4278e32abb543c7d7fa75d88","modified":1638944857873},{"_id":"themes/next_8.8/layout/_partials/post/post-reward.njk","hash":"58b3f657a47bae406e5fcf19cd5e42680785ac71","modified":1638944857875},{"_id":"themes/next_8.8/layout/_partials/post/post-followme.njk","hash":"ebf83083856f8bd81ad47ffb985d44e338b4e6bb","modified":1638944857873},{"_id":"themes/next_8.8/layout/_partials/search/algolia-search.njk","hash":"93fbb449fbd599cb4315d7eb0daeb239811b233f","modified":1638944857875},{"_id":"themes/next_8.8/layout/_partials/search/index.njk","hash":"9766852e72c1809d8c1eea71ac6116b4cc0886d2","modified":1638944857876},{"_id":"themes/next_8.8/layout/_partials/post/post-related.njk","hash":"80d3dac42740d2aef677e25165e31c05eb048887","modified":1638944857875},{"_id":"themes/next_8.8/layout/_partials/post/post-meta.njk","hash":"9a9c4fb7e7c4fe4b7d474bdfdb4ed2b0a5423df2","modified":1638944857874},{"_id":"themes/next_8.8/layout/_third-party/analytics/cloudflare.njk","hash":"c7cea42f6db2137c11ca1d83e43fcb7ad7ccfb89","modified":1638944857881},{"_id":"themes/next_8.8/layout/_third-party/analytics/baidu-analytics.njk","hash":"3e80332f88b101141be69f2a07f54ed8c053eabb","modified":1638944857880},{"_id":"themes/next_8.8/layout/_third-party/analytics/growingio.njk","hash":"9ff9ec05c2037beea229a6bb698f9e3546973220","modified":1638944857882},{"_id":"themes/next_8.8/layout/_third-party/analytics/google-analytics.njk","hash":"52ad137450f7b3d6a330e16b3ed1c6174290f0eb","modified":1638944857881},{"_id":"themes/next_8.8/layout/_partials/search/localsearch.njk","hash":"f73d25a8ccfdd5d4ca2953dc434ff8ce36034c57","modified":1638944857876},{"_id":"themes/next_8.8/layout/_third-party/chat/chatra.njk","hash":"09d2c9487d75894d45a823e3237ae9f90fd6ee01","modified":1638944857882},{"_id":"themes/next_8.8/layout/_partials/sidebar/site-overview.njk","hash":"c5c38b4fb137cc799a6ec31f391d1efc12234c8c","modified":1638944857876},{"_id":"themes/next_8.8/layout/_third-party/analytics/index.njk","hash":"465fcffd4216f8ca0ea2613fe9cf7308f71b9da5","modified":1638944857882},{"_id":"themes/next_8.8/layout/_third-party/chat/gitter.njk","hash":"375a86f0b19e130cfa7707007e3a53d9ae7c9b64","modified":1638944857883},{"_id":"themes/next_8.8/layout/_third-party/comments/disqus.njk","hash":"b0828dd1b1fd66ecd612d9e886a08e7579e9a4f7","modified":1638944857884},{"_id":"themes/next_8.8/layout/_third-party/comments/changyan.njk","hash":"5f7967bd946060f4102263a552ddfbae9975e7ea","modified":1638944857884},{"_id":"themes/next_8.8/layout/_third-party/chat/tidio.njk","hash":"3fbc72427c1211e5dcfd269af1a74852a7ba5c1a","modified":1638944857883},{"_id":"themes/next_8.8/layout/_third-party/comments/disqusjs.njk","hash":"c5086b4c35f730f82c99c4a8317f2f153ebde869","modified":1638944857884},{"_id":"themes/next_8.8/layout/_third-party/comments/isso.njk","hash":"38badcc7624a13961381c2465478056b9602aee5","modified":1638944857885},{"_id":"themes/next_8.8/layout/_third-party/comments/livere.njk","hash":"b8e0d5de584cece5e05b03db5b86145aa1e422b4","modified":1638944857885},{"_id":"themes/next_8.8/layout/_third-party/comments/gitalk.njk","hash":"6fd4df5c21cfe530dbb0c012bc0b202f2c362b9c","modified":1638944857884},{"_id":"themes/next_8.8/layout/_third-party/comments/utterances.njk","hash":"a7921be7328e1509d33b435175f5333a9aada66f","modified":1638944857888},{"_id":"themes/next_8.8/layout/_third-party/math/katex.njk","hash":"a84db8bc8804335f95609a221ac1746433dcdc89","modified":1638944857889},{"_id":"themes/next_8.8/layout/_third-party/search/algolia-search.njk","hash":"67f67a77f27103177b9940446f43610229536d82","modified":1638944857891},{"_id":"themes/next_8.8/scripts/events/lib/config.js","hash":"a912944cae0d864458d365867b8a9c89f348e68a","modified":1638944857900},{"_id":"themes/next_8.8/layout/_third-party/math/index.njk","hash":"1856c4b035c5b8e64300a11af0461b519dfc4cf4","modified":1638944857889},{"_id":"themes/next_8.8/layout/_third-party/search/localsearch.njk","hash":"210c32b654adae3d8076c4417d370b42af258cea","modified":1638944857891},{"_id":"themes/next_8.8/layout/_third-party/math/mathjax.njk","hash":"a62aa1ed4e35b8d0451d83f341bf0a97538bc9a4","modified":1638944857890},{"_id":"themes/next_8.8/scripts/events/lib/highlight.js","hash":"00cec6980cafd417def885f496371856cd524a25","modified":1638944857901},{"_id":"themes/next_8.8/scripts/events/lib/injects.js","hash":"1f1ea7b579a49f17574c31d78d663c54896133eb","modified":1638944857901},{"_id":"themes/next_8.8/scripts/events/lib/vendors.js","hash":"2f7057a8d3fce08aa7e2a17d7b7a1f03ac3d8ed6","modified":1638944857902},{"_id":"themes/next_8.8/scripts/events/lib/utils.js","hash":"8508e96a5f883a5a57d8c1b8b5ea438fa29aafd3","modified":1638944857901},{"_id":"themes/next_8.8/layout/_third-party/statistics/busuanzi-counter.njk","hash":"d97790e4b442a1e3ded7d7b4f84b8ee6cdb6e8ea","modified":1638944857892},{"_id":"themes/next_8.8/layout/_third-party/statistics/index.njk","hash":"866ffa15a3250678eb8a90aa6f609fa965db90fd","modified":1638944857895},{"_id":"themes/next_8.8/layout/_third-party/statistics/firestore.njk","hash":"af5336e8bbdc4638435971da115bb7443d374ade","modified":1638944857892},{"_id":"themes/next_8.8/scripts/filters/comment/changyan.js","hash":"cfff8331fdaa2ede4ab08c58cfc6d98c7d2374d9","modified":1638944857902},{"_id":"themes/next_8.8/layout/_third-party/statistics/lean-analytics.njk","hash":"8703d1855bb8d251c9b7c2940b7e3be525e53000","modified":1638944857895},{"_id":"themes/next_8.8/layout/_third-party/tags/mermaid.njk","hash":"dd8f963acd5a3685be46fd5319c06df0308d99b2","modified":1638944857896},{"_id":"themes/next_8.8/layout/_third-party/tags/pdf.njk","hash":"0386c708975cc5faea4f782611c5d2c6b8ac2850","modified":1638944857897},{"_id":"themes/next_8.8/scripts/filters/comment/common.js","hash":"713056d33dbcd8e9748205c5680b456c21174f4e","modified":1638944857903},{"_id":"themes/next_8.8/scripts/filters/comment/default-config.js","hash":"1cb58aa6b88f7461c3c3f9605273686adcc30979","modified":1638944857903},{"_id":"themes/next_8.8/scripts/filters/comment/disqusjs.js","hash":"70eb507ef7f1a4fc3ca71a3814cc57afe7f3f60c","modified":1638944857903},{"_id":"themes/next_8.8/scripts/filters/comment/gitalk.js","hash":"96e58efba0dc76af409cc7d2db225f0fe4526ea8","modified":1638944857904},{"_id":"themes/next_8.8/scripts/filters/comment/disqus.js","hash":"3283bdd6e5ac7d10376df8ddd5faaec5dc1bd667","modified":1638944857903},{"_id":"themes/next_8.8/scripts/filters/comment/livere.js","hash":"bb8ebb541c40362c0cbbd8e83d3b777302bb6c40","modified":1638944857904},{"_id":"themes/next_8.8/scripts/filters/comment/utterances.js","hash":"a50718c081685fd35ff8ea9ca13682c284399ed8","modified":1638944857905},{"_id":"themes/next_8.8/scripts/filters/comment/isso.js","hash":"c22cbccd7d514947e084eeac6a3af1aa41ec857a","modified":1638944857904},{"_id":"themes/next_8.8/source/css/_variables/Muse.styl","hash":"d3a8f6e71c86926d0c2a247a31d7446d829736d5","modified":1638944857946},{"_id":"themes/next_8.8/source/css/_variables/Mist.styl","hash":"ee5024be8e39605f0c6d71db038e15e0693d0f41","modified":1638944857946},{"_id":"themes/next_8.8/source/css/_variables/Gemini.styl","hash":"c4537fa2de33d98baff2c87a73801770414e0b69","modified":1638944857946},{"_id":"themes/next_8.8/source/css/_variables/Pisces.styl","hash":"58014a2d087c4126058a99b5b1cb7d8a2eb6224d","modified":1638944857947},{"_id":"themes/next_8.8/source/js/third-party/pace.js","hash":"0ebee77b2307bf4b260afb06c060171ef42b7141","modified":1638944857959},{"_id":"themes/next_8.8/source/css/_variables/base.styl","hash":"0876b50a58f114bc0b7982b85c5e5011730253b8","modified":1638944857947},{"_id":"themes/next_8.8/source/js/schemes/muse.js","hash":"e1b4bf9aa47d14c790a0920d7dbb3e9812d4358b","modified":1638944857953},{"_id":"themes/next_8.8/source/js/third-party/rating.js","hash":"a1f44247c18ac00ee3e0026560398429e4c77dd7","modified":1638944857960},{"_id":"themes/next_8.8/source/css/_common/outline/index.styl","hash":"7782dfae7a0f8cd61b936fa8ac980440a7bbd3bb","modified":1638944857927},{"_id":"themes/next_8.8/source/js/third-party/fancybox.js","hash":"8a847a7bbdbc0086dd1de12b82107a854b43f5e5","modified":1638944857958},{"_id":"themes/next_8.8/source/js/third-party/quicklink.js","hash":"539c5bb51244f7f4aa98884f3229d128c1cefc40","modified":1638944857960},{"_id":"themes/next_8.8/source/css/_common/components/index.styl","hash":"991c1f80995cec418dc00d3d6b13e2d911ac9894","modified":1638944857913},{"_id":"themes/next_8.8/source/css/_common/components/back-to-top.styl","hash":"2bbf9046ef2a8f99ef3668bbb8be4e52e9d97bb7","modified":1638944857913},{"_id":"themes/next_8.8/source/css/_common/scaffolding/base.styl","hash":"e2da25ff86d2be5ff0a0cee33c7d4c5e11046736","modified":1639712427384},{"_id":"themes/next_8.8/source/css/_common/outline/mobile.styl","hash":"2db4462e9cb87b8aef3f50f850fed407de16da3e","modified":1638944857927},{"_id":"themes/next_8.8/source/css/_common/scaffolding/base.styl_backup","hash":"1239f1b432a6932b2bb9ebcfbaabf724b8f4e59a","modified":1639711097302},{"_id":"themes/next_8.8/source/css/_common/scaffolding/index.styl","hash":"43045d115f8fe95732c446aa45bf1c97609ff2a5","modified":1638944857933},{"_id":"themes/next_8.8/source/css/_common/scaffolding/toggles.styl","hash":"90f7d3baab061e860172b536c9edc38c7fd2ef5c","modified":1638944857938},{"_id":"themes/next_8.8/source/css/_common/components/reading-progress.styl","hash":"f3defd56be33dba4866a695396d96c767ce63182","modified":1638944857920},{"_id":"themes/next_8.8/source/css/_common/scaffolding/comments.styl","hash":"cf8446f4378dcab27b55ede1635c608ae6b8a5c8","modified":1638944857932},{"_id":"themes/next_8.8/source/css/_common/scaffolding/buttons.styl","hash":"f768ecb2fe3e9384777c1c115cd7409e9155edd7","modified":1638944857932},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tables.styl","hash":"b9388016f8d9274703e77e306a1feaad1b7b9d6c","modified":1638944857934},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_header.styl","hash":"b1054313ca9419e76fea0451417c881616f50a38","modified":1638944857939},{"_id":"themes/next_8.8/source/css/_common/scaffolding/pagination.styl","hash":"34416a5792d0235caa8c0c7e59725f2df0fa614c","modified":1638944857934},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_layout.styl","hash":"00366a6bd1a66f99f845c5ebfc9e8cf56651b815","modified":1638944857939},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_menu.styl","hash":"f337981f8f20944ed366694aea88146c7b0a13ab","modified":1638944857940},{"_id":"themes/next_8.8/source/css/_common/scaffolding/normalize.styl","hash":"6d740699fb6a7640647a8fd77c4ea4992d8d6437","modified":1638944857933},{"_id":"themes/next_8.8/source/css/_schemes/Mist/index.styl","hash":"89bf3f6b82cb0fafbbd483431df8f450857c5a0b","modified":1638944857941},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_header.styl","hash":"fd89988442f380cba907752fe3f608e3498f8c93","modified":1638944857941},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_layout.styl","hash":"018b6a761e197086174c9f06b4d5ea21cc230951","modified":1638944857941},{"_id":"themes/next_8.8/source/css/_schemes/Mist/_posts-expand.styl","hash":"c9a9e07b721bb2376e24753ae0a9452431439114","modified":1638944857941},{"_id":"themes/next_8.8/source/css/_schemes/Muse/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1638944857943},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_menu.styl","hash":"28030c61288cc0e1321b18373a5c79029fd76a53","modified":1638944857942},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_sidebar.styl","hash":"134272cb8096156c9e32fbbe085394633c7509cd","modified":1638944857942},{"_id":"themes/next_8.8/source/css/_schemes/Muse/_sub-menu.styl","hash":"2d3e05015796a790abd9d68957a5c698c0c9f9b6","modified":1638944857943},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_header.styl","hash":"9b2cba0c9aa5a64957294f7548c199db1f63f0f4","modified":1638944857943},{"_id":"themes/next_8.8/source/css/_schemes/Gemini/index.styl","hash":"f51b6a4f06359ed56b2d10caa6f15362d3b3751d","modified":1638944857939},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_layout.styl","hash":"9f60d501808f67d151af437221d0dfacc27c180c","modified":1638944857944},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b5c3dd08c520a16ee49f85fa12b4935e725ef261","modified":1638944857945},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/index.styl","hash":"7905f428b46d100ac5928875cb1e2b99fa86fc0b","modified":1638944857945},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_menu.styl","hash":"1d29eca70fa686d895f8e98a283e4a159e40905a","modified":1638944857944},{"_id":"themes/next_8.8/source/css/_schemes/Pisces/_sidebar.styl","hash":"42bf453def88da82c842dca84e8f47087091f08e","modified":1638944857944},{"_id":"themes/next_8.8/source/js/third-party/analytics/growingio.js","hash":"f755e8537ccbbb0bd84c26923f320d4e206e7428","modified":1638944857954},{"_id":"themes/next_8.8/source/js/third-party/analytics/baidu-analytics.js","hash":"f9579a02599de063ccff336177ba964a2931a6e9","modified":1638944857954},{"_id":"themes/next_8.8/source/js/third-party/analytics/google-analytics.js","hash":"d77d4934d959e7125128754b568f1d041c3fbfff","modified":1638944857954},{"_id":"themes/next_8.8/source/js/third-party/comments/disqusjs.js","hash":"1c282d6c2151346d1f0aa95055d17abe77054ec9","modified":1638944857957},{"_id":"themes/next_8.8/source/js/third-party/comments/gitalk.js","hash":"1e8509356fb027d948d118ab220d9631f4d482fa","modified":1638944857957},{"_id":"themes/next_8.8/source/js/third-party/comments/disqus.js","hash":"5460de247c038d6cfbe774d7f8747f0a958d9017","modified":1638944857956},{"_id":"themes/next_8.8/source/js/third-party/comments/changyan.js","hash":"b1dd519dc3b1153c9d2ba2d35f68ca8f73f33bae","modified":1638944857956},{"_id":"themes/next_8.8/source/js/third-party/comments/livere.js","hash":"68892d74ef5fc308c6e7e6b4f190826d79f3055d","modified":1638944857957},{"_id":"themes/next_8.8/source/js/third-party/comments/isso.js","hash":"b9b9fd2f0e098a123b34a4932da912a9485ffe6c","modified":1638944857957},{"_id":"themes/next_8.8/source/js/third-party/comments/utterances.js","hash":"ec44d7f1c8b51b0aa3cccba099a78f3575ac828c","modified":1638944857958},{"_id":"themes/next_8.8/source/js/third-party/chat/gitter.js","hash":"14b024c920a8b359777d79dd8e1a849387f8f3ad","modified":1638944857955},{"_id":"themes/next_8.8/source/js/third-party/chat/chatra.js","hash":"72e0766752b78a723fb30e92d533a8b353104e2d","modified":1638944857955},{"_id":"themes/next_8.8/source/js/third-party/math/katex.js","hash":"5c63ec71458b4fe0cd98fd4a04e11c3746764f11","modified":1638944857959},{"_id":"themes/next_8.8/source/js/third-party/chat/tidio.js","hash":"77c231bcd64f1c09bd9989909e9fee703b65f47f","modified":1638944857955},{"_id":"themes/next_8.8/source/js/third-party/search/local-search.js","hash":"dc2b0e89aa32afc7f7a7e2d7a277dadb7f96e06d","modified":1638944857961},{"_id":"themes/next_8.8/source/js/third-party/statistics/firestore.js","hash":"d0829fe41d2fe86b8499e2a896556c1275ea0066","modified":1638944857961},{"_id":"themes/next_8.8/source/js/third-party/math/mathjax.js","hash":"d93556184b2c0aa1dbc4a6fb892d2f77b80d7d9f","modified":1638944857959},{"_id":"themes/next_8.8/source/js/third-party/search/algolia-search.js","hash":"ea94731438d8c518d946601f8f46a65b92381fac","modified":1638944857960},{"_id":"themes/next_8.8/source/js/third-party/tags/pdf.js","hash":"e109c2d6828f527f0289d5fa3bb02fce63ee6d93","modified":1638944857962},{"_id":"themes/next_8.8/source/css/_common/outline/header/bookmark.styl","hash":"c8648c8ea3105556be0068d9fb2735261d0d94bc","modified":1638944857923},{"_id":"themes/next_8.8/source/css/_common/outline/header/index.styl","hash":"67fc7a1eb59c8451eec34e572cbb2fd1424757bc","modified":1638944857924},{"_id":"themes/next_8.8/source/js/third-party/statistics/lean-analytics.js","hash":"6abdc209f4503d4efd676e18bc30ddea813b6ff9","modified":1638944857961},{"_id":"themes/next_8.8/source/css/_common/outline/header/menu.styl","hash":"2db695204d39e4c7daa7b91585a0ea4b06b49f11","modified":1638944857926},{"_id":"themes/next_8.8/source/css/_common/outline/footer/index.styl","hash":"02b6d1a53f7a02c6b0929b11f3ab904b5b873a0e","modified":1638944857923},{"_id":"themes/next_8.8/source/js/third-party/tags/mermaid.js","hash":"2618135cbcee6bf228f6734767de1995e5eaaac6","modified":1638944857962},{"_id":"themes/next_8.8/source/css/_common/outline/header/github-banner.styl","hash":"05af22f3edc2383a3d97ec4c05e9ac43b014bead","modified":1638944857924},{"_id":"themes/next_8.8/source/css/_common/outline/header/site-nav.styl","hash":"d9bc2b520636b9df7f946295cd430593df4118ff","modified":1638944857926},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"2c2bfbc34b6f19d262ae7c041474985e12f4f4ad","modified":1638944857928},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"57ed6770535ecb2e6485a0c87d4de6d6476368b9","modified":1638944857929},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"63d8f5f169c2b1c969928fc79244c5fe89ee484e","modified":1638944857929},{"_id":"themes/next_8.8/source/css/_common/outline/header/site-meta.styl","hash":"86b0925e968f35bbc76b473a861e8f9797f7580e","modified":1638944857926},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"d8a028f532d562e6a86bb3b9c7b992e4b6dbbb51","modified":1638944857929},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/index.styl","hash":"9964a96f9a647cfb16b97679eced79d07e084e6d","modified":1638944857928},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"1c324d56ae83e96db2c4c6d63edd7ee51c936fc1","modified":1638944857929},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"6681ffe283f8a7e3c86310ef4f6ca1e499c1a19f","modified":1638944857930},{"_id":"themes/next_8.8/source/css/_common/components/pages/breadcrumb.styl","hash":"fde10ce94e9ae21a03b60d41d532835b54abdcb1","modified":1638944857914},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/site-state.styl","hash":"2de038def2cb91da143b14696366c14a66e0e569","modified":1638944857931},{"_id":"themes/next_8.8/source/css/_common/components/pages/index.styl","hash":"6cf78a379bb656cc0abb4ab80fcae60152ce41ad","modified":1638944857915},{"_id":"themes/next_8.8/source/css/_common/components/pages/categories.styl","hash":"80595d274f593b321c0b644a06f3165fe07b16f5","modified":1638944857914},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"db4f3263b2b6551dd56bfdf33cceaf81661a3611","modified":1638944857930},{"_id":"themes/next_8.8/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"081345490271840855d1238b969dbf2e0a2bba8f","modified":1638944857930},{"_id":"themes/next_8.8/source/css/_common/components/pages/schedule.styl","hash":"091b8c763e43447d087c122a86538f290f83136a","modified":1638944857915},{"_id":"themes/next_8.8/source/css/_common/components/pages/tag-cloud.styl","hash":"56d719bcdcba3d725141c55bbd4b168f3942f912","modified":1638944857916},{"_id":"themes/next_8.8/source/css/_common/components/third-party/gitalk.styl","hash":"fb165c1a0d990c5cf98b87773e0dc50410229b96","modified":1638944857920},{"_id":"themes/next_8.8/source/css/_common/components/third-party/math.styl","hash":"1e5776ad4c5c8bcf7596ac74dcabc30704b3f5a0","modified":1638944857921},{"_id":"themes/next_8.8/source/css/_common/components/third-party/index.styl","hash":"25ea9a0af888355b3a046db1100b5cb0e2d6ef6e","modified":1638944857921},{"_id":"themes/next_8.8/source/css/_common/components/third-party/related-posts.styl","hash":"0527153aa821bdbdb84c7b47f60e3cefd95a742f","modified":1638944857921},{"_id":"themes/next_8.8/source/css/_common/components/third-party/utterances.styl","hash":"d28856f365a9373c4ae6fe1e5673d63df2dfd65f","modified":1638944857922},{"_id":"themes/next_8.8/source/css/_common/components/third-party/disqusjs.styl","hash":"c1e9edbfd1c3696b35d5452ae2e6d766f3fe91aa","modified":1638944857920},{"_id":"themes/next_8.8/source/css/_common/components/post/index.styl","hash":"df2fbd0ada00f37439b0de965c6f1c29d3c97429","modified":1638944857916},{"_id":"themes/next_8.8/source/css/_common/components/third-party/search.styl","hash":"49c26184580fde8a732899a4de5aae8662e289b8","modified":1638944857922},{"_id":"themes/next_8.8/source/css/_common/components/post/post-body.styl","hash":"7a34d020877273dcf11c25fa481409300efb8659","modified":1638944857917},{"_id":"themes/next_8.8/source/css/_common/components/post/post-gallery.styl","hash":"c34936a17c3d8af6c0988ac6746d7509dc0b50eb","modified":1638944857918},{"_id":"themes/next_8.8/source/css/_common/components/post/post-followme.styl","hash":"791bc9befb0d4d06e3e517eccfe0bc3551a02a60","modified":1638944857917},{"_id":"themes/next_8.8/source/css/_common/components/post/post-nav.styl","hash":"69dff7cf231d01f85671758455726dd666664a73","modified":1638944857918},{"_id":"themes/next_8.8/source/css/_common/components/post/post-collapse.styl","hash":"eebe3013a9a976011570dce2d04dfeae4c31d790","modified":1638944857917},{"_id":"themes/next_8.8/source/css/_common/components/post/post-footer.styl","hash":"e53a5eb1d1771e284044bdb0bc0ed2de27923669","modified":1638944857918},{"_id":"themes/next_8.8/source/css/_common/components/post/post-widgets.styl","hash":"0a779f955a0e25df0852e0731517dadb234aa181","modified":1638944857919},{"_id":"themes/next_8.8/source/css/_common/components/post/post-reward.styl","hash":"9043d9bc2db35ca000c79258ef89fdb161dc43fb","modified":1638944857919},{"_id":"themes/next_8.8/source/css/_common/components/post/post-header.styl","hash":"4d29b6ae7ed3dc44b10df851a4128b6441efa8be","modified":1638944857918},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"8d9218980e185210ce034e9769ab639b9630fd88","modified":1638944857935},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/index.styl","hash":"e22fde6f1657d311d46f64d868c4491d535c8caa","modified":1638944857935},{"_id":"themes/next_8.8/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"a4003e1408844568cb5102a5a111046cb19b2d31","modified":1638944857933},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/mermaid.styl","hash":"c7754dc6c866928b538f0863a05b96ec44b5e986","modified":1638944857936},{"_id":"themes/next_8.8/source/css/_common/scaffolding/highlight/index.styl","hash":"5f706f3382652835379cf9b9fec24ccd4513ab65","modified":1638944857933},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"6b3680e0dbea8e14c1cec24ef63b7fae5e37f7ef","modified":1638944857935},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/label.styl","hash":"531daf2612c6217950677a2d03924459ce57c291","modified":1638944857936},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/pdf.styl","hash":"77122986509a6b4968bae2729417b7016137534c","modified":1638944857937},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/note.styl","hash":"2e9dc3b3546e19e9de18050ad04b1741841116bc","modified":1638944857937},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/link-grid.styl","hash":"7075dd32dd70da1e161e4bd14b46f1e8be62fa3c","modified":1638944857936},{"_id":"themes/next_8.8/source/css/_common/scaffolding/tags/tabs.styl","hash":"40a38f2129617ffd4e8d5cd78e982fdfc9941acf","modified":1638944857937},{"_id":"public/sitemap.xml","hash":"d89096fbb3fffd398a7e2e1b5ed5751fb72540f4","modified":1640330174162},{"_id":"public/about/index.html","hash":"d28cb64ed42ec6525218eb0124c251a9f04e54fa","modified":1640330174162},{"_id":"public/tags/index.html","hash":"340fe3077c90ae15e792e14a2c082cb5320d9931","modified":1640330174162},{"_id":"public/categories/index.html","hash":"5ea42781517321598531135fd506ce0f53bf2cd8","modified":1640330174162},{"_id":"public/2021/12/18/Transformer-and-BERT/index.html","hash":"3963ceeb8ce6eb3c4e8ca009a6335c24624f2711","modified":1640330174162},{"_id":"public/2021/12/17/Algorithm/index.html","hash":"387bcce2e8a3b832f6f520e3525d92d317d3c765","modified":1640330174162},{"_id":"public/2021/12/10/An-Introduction-to-Git/index.html","hash":"199457bc1a10af0f1ecdaa2786bc7c1bb63ae709","modified":1640330174162},{"_id":"public/archives/index.html","hash":"a9b9e6ede9b08c1ee70f4c19fea79e2f012dc49e","modified":1640330174162},{"_id":"public/archives/2021/index.html","hash":"555c7173f577a41d6a9eaaad27f2c2e73cfc29bb","modified":1640330174162},{"_id":"public/archives/2021/11/index.html","hash":"1f14703964511a24f2b142afe3995e07313c4020","modified":1640330174162},{"_id":"public/archives/2021/12/index.html","hash":"032f7a07ba4ba0e45d747bbd4368365d7ba04bd3","modified":1640330174162},{"_id":"public/categories/Programming/index.html","hash":"f636f89a7d42a7007e6dd0e6bd9b4fd9476efa04","modified":1640330174162},{"_id":"public/categories/Little-Things/index.html","hash":"4dd1c0699d34f7a6477ca4bc2da82660b36a553a","modified":1640330174162},{"_id":"public/categories/Experiments/index.html","hash":"fa28bd576ce3cb6e3fec75f7454a76509ee95df4","modified":1640330174162},{"_id":"public/categories/Reinforcement-Learning/index.html","hash":"8277b43d2467b80a3fcfbd69ea043ad575c74030","modified":1640330174162},{"_id":"public/categories/About-Papers/index.html","hash":"715e2d9e722c601b3deb1fd7443f0800ff03cda7","modified":1640330174162},{"_id":"public/categories/Little-Things/Git/index.html","hash":"d6cde484d91b845205ae5ddc3ba9ace84a3dcc77","modified":1640330174162},{"_id":"public/categories/Natural-Language-Processing/index.html","hash":"7ae2c9af999506c35274a7b1d0043e24bb715b06","modified":1640330174162},{"_id":"public/categories/Little-Things/Hexo/index.html","hash":"05a95d88b96eccc566ed734d8fa126f649ca7ac6","modified":1640330174162},{"_id":"public/tags/Algorithm/index.html","hash":"d2b7dc4ffda784fde361f2cba675f81567ee8b17","modified":1640330174162},{"_id":"public/tags/Programming/index.html","hash":"001a2cd69de113e5c16caafb96943cbeb836bb54","modified":1640330174162},{"_id":"public/tags/Git/index.html","hash":"8c0200599169235237ae2e4f3ab9d4dc3c927cdc","modified":1640330174162},{"_id":"public/tags/Hexo/index.html","hash":"53632b50fd5fe7190c1821f27b697cdebfd094a0","modified":1640330174162},{"_id":"public/tags/Personal-Thought/index.html","hash":"337469a02ee2b166077b5b83f21759636aa6d95b","modified":1640330174162},{"_id":"public/tags/Experiments/index.html","hash":"6e089dcd5c63bbf5ae6c54b6eef397528f1d07ca","modified":1640330174162},{"_id":"public/tags/private/index.html","hash":"8faf220ad372e236efd40ea2b1d29ed7ee916f3d","modified":1640330174162},{"_id":"public/tags/Reinforcement-Learning/index.html","hash":"b0f883719c55e54381555ac2add832a6ef47f5af","modified":1640330174162},{"_id":"public/tags/Papers/index.html","hash":"20f6bd59b8e988ad2a8ea986d87a11c4e091ad13","modified":1640330174162},{"_id":"public/tags/Natural-Language-Processing/index.html","hash":"8e09f99c2d03cc09bae1263d603b80b477a7854c","modified":1640330174162},{"_id":"public/tags/Transformer/index.html","hash":"f6d5ac3047804f34300ed48267117820740ec765","modified":1640330174162},{"_id":"public/2021/12/17/Experiments/index.html","hash":"c67d0d20f1898a99ad8de76d92788bb79a873e3d","modified":1640330174162},{"_id":"public/2021/12/15/Personal-Thought/index.html","hash":"8a70650f504cb9119b71a6e541fca361ed6994d5","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/index.html","hash":"4469bd91b5514ff8ecca6e30fdd6011870286399","modified":1640330174162},{"_id":"public/2021/12/03/First-Step-to-RL/index.html","hash":"5f2a8e64aa8959283bcac107bab0581c03aa4feb","modified":1640330174162},{"_id":"public/2021/11/24/Construct-Your-Blog-with-Hexo-and-Github/index.html","hash":"fb1fc1c72c5f5533acbea0ae24c1d46fbe2ac11c","modified":1640330174162},{"_id":"public/index.html","hash":"e16cc64f9323544ea180687be8ef01933f0d8c3e","modified":1640330174162},{"_id":"public/tags/BERT/index.html","hash":"9627ca05f62a34823516da7f9056822bf696a4c9","modified":1640330174162},{"_id":"public/images/apple-touch-icon-next.png","hash":"2959dbc97f31c80283e67104fe0854e2369e40aa","modified":1640330174162},{"_id":"public/images/favicon-32x32-next.png","hash":"0749d7b24b0d2fae1c8eb7f671ad4646ee1894b1","modified":1640330174162},{"_id":"public/images/avatar.gif","hash":"2dbc3e2f2d624b2ca1afe6edc2ca17307f1950c8","modified":1640330174162},{"_id":"public/images/logo-algolia-nebula-blue-full.svg","hash":"b85e274207b1392782476a0430feac98db1e7da0","modified":1640330174162},{"_id":"public/images/logo.svg","hash":"2cb74fd3ea2635e015eabc58a8d488aed6cf6417","modified":1640330174162},{"_id":"public/images/favicon-16x16-next.png","hash":"943a0d67a9cdf8c198109b28f9dbd42f761d11c3","modified":1640330174162},{"_id":"public/images/ye_16.ico","hash":"b8fb01b5361da89831d232a831a1532e9822bd72","modified":1640330174162},{"_id":"public/images/ye_32.ico","hash":"375c99cd785d93dd989c36604ffbd10ada71322a","modified":1640330174162},{"_id":"public/2021/12/03/First-Step-to-RL/q_learning.py","hash":"3a68ec1153d26f2c18c9d34ee5deb5e495b8dca9","modified":1640330174162},{"_id":"public/2021/12/03/First-Step-to-RL/policy_network.py","hash":"b3e8f06360cad0084a0656edaed4539f1b01e327","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/ADCM.jpg","hash":"ee5b75f3a063e07624c5f03111f98902d885cd8e","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_eq1.jpg","hash":"2ea2a74f960377a6a9d0bc98ecfadd8ba09cd316","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_eq3.jpg","hash":"8fa845242bb6f737736d91485ca237e7132e19b5","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_gl.jpg","hash":"fb8050f8b71a7c667cfa85672b505b3340d2231b","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_eq2.jpg","hash":"cf5832e89b29e39275d2fa4a2e39592e97f9a39e","modified":1640330174162},{"_id":"public/lib/hbe.js","hash":"136dba00826bdd086153bf0acb5473aea7183ad1","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_Cell.jpg","hash":"b17e79bf86bb3422527bf51d200ceceeb53ce936","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Saccader_Over.jpg","hash":"deaccd942ffca9e163a9f2c9bdfb69251d1004cb","modified":1640330174162},{"_id":"public/css/hbe.style.css","hash":"b0a0077cb588c0941823905fcc383aa7509ade73","modified":1640330174162},{"_id":"public/css/noscript.css","hash":"54d14cd43dc297950a4a8d39ec9644dd5fc3499f","modified":1640330174162},{"_id":"public/js/bookmark.js","hash":"0f563ffbf05fad30e854e413ab17ff7164ab5a53","modified":1640330174162},{"_id":"public/js/motion.js","hash":"6d4bd07a6f8e1b4083119dca0acb5b289533b619","modified":1640330174162},{"_id":"public/js/comments.js","hash":"66ae2e26ea36a41b72c638ea8b220296638ae952","modified":1640330174162},{"_id":"public/js/schedule.js","hash":"2b43e2d576a308289880befc27580dbb2aa34439","modified":1640330174162},{"_id":"public/js/config.js","hash":"4c4ebbe3b3f3841a26f9d5af6d0ba8bc6da01c54","modified":1640330174162},{"_id":"public/js/next-boot.js","hash":"48497e2156a10155dc42311633a110c9685692c9","modified":1640330174162},{"_id":"public/js/pjax.js","hash":"919f5281c4a04d11cfd94573ecf57b3dbabd3cc8","modified":1640330174162},{"_id":"public/js/comments-buttons.js","hash":"1a7344440321713426a0b2ab17e276b5bdf85ade","modified":1640330174162},{"_id":"public/js/utils.js","hash":"e447160d342b1f93df5214b6a733441039ced439","modified":1640330174162},{"_id":"public/js/third-party/fancybox.js","hash":"c098d14e65dd170537134358d4b8359ad0539c2c","modified":1640330174162},{"_id":"public/js/schemes/muse.js","hash":"9a836d2bcc3defe4bd1ee51f5f4eb7006ebdd41b","modified":1640330174162},{"_id":"public/js/third-party/quicklink.js","hash":"6f58cd7aa8f6f1ab92d5a96551add293f4e55312","modified":1640330174162},{"_id":"public/js/third-party/rating.js","hash":"4e92c2d107ba47b47826829f9668030d5ea9bfb8","modified":1640330174162},{"_id":"public/js/third-party/pace.js","hash":"0ef04218b93561ba4d0ff420d556c3d90a756d32","modified":1640330174162},{"_id":"public/js/third-party/analytics/google-analytics.js","hash":"59684383385059dc4f8a1ff85dbbeb703bcdbcb5","modified":1640330174162},{"_id":"public/js/third-party/analytics/growingio.js","hash":"78dd3cf04082b7dbe6246e404b2aa8e726922402","modified":1640330174162},{"_id":"public/js/third-party/analytics/baidu-analytics.js","hash":"f629acc46ff40c071ffd31b77d5c7616f0fdd778","modified":1640330174162},{"_id":"public/js/third-party/comments/disqus.js","hash":"e1cc671b0d524864fd445e3ab4ade9ee6d07e565","modified":1640330174162},{"_id":"public/js/third-party/comments/disqusjs.js","hash":"b6c58f098473b526d6a3cd35655caf34b77f7cff","modified":1640330174162},{"_id":"public/js/third-party/comments/changyan.js","hash":"8c8ebec444c727b704ea41ad88b0b96ed2e4b8d4","modified":1640330174162},{"_id":"public/js/third-party/comments/gitalk.js","hash":"0ec038cf83e8ec067534f16a54041e47a3c1e59a","modified":1640330174162},{"_id":"public/js/third-party/comments/utterances.js","hash":"f67f90eb03e284c82da2b8cf2f1e31801813c16d","modified":1640330174162},{"_id":"public/js/third-party/comments/isso.js","hash":"753a873b6f566aff5ba77ca23f91b78eb880ca64","modified":1640330174162},{"_id":"public/js/third-party/comments/livere.js","hash":"2247d88c934c765c43013337860774aaa99f0b31","modified":1640330174162},{"_id":"public/js/third-party/chat/gitter.js","hash":"cc38c94125f90dadde11b5ebac7d8bf99a1a08a2","modified":1640330174162},{"_id":"public/js/third-party/chat/chatra.js","hash":"c32180522788c10e51df1803aa6842ef0432ddc9","modified":1640330174162},{"_id":"public/js/third-party/chat/tidio.js","hash":"b0079f6a4601e06ca6fe46e83a2f5af553e9bc3c","modified":1640330174162},{"_id":"public/js/third-party/math/katex.js","hash":"83c54ee536e487a1031783443fe0cb63b1b4767e","modified":1640330174162},{"_id":"public/js/third-party/search/local-search.js","hash":"45c485f82258d246f37deb66884bd2643323ef3a","modified":1640330174162},{"_id":"public/js/third-party/math/mathjax.js","hash":"5c749b9c1c3bb738122d0516211ecff6496d4907","modified":1640330174162},{"_id":"public/js/third-party/statistics/firestore.js","hash":"0960f16107ed61452fb0dffc6ed22dc143de34ef","modified":1640330174162},{"_id":"public/js/third-party/search/algolia-search.js","hash":"ac401e3736d56a3c9cb85ab885744cce0b813c55","modified":1640330174162},{"_id":"public/js/third-party/statistics/lean-analytics.js","hash":"5a928990856b8e456f0663cf3b6b406733672e39","modified":1640330174162},{"_id":"public/js/third-party/tags/mermaid.js","hash":"3dc4628efa6debd6490fc0ebddff2424a7b319d8","modified":1640330174162},{"_id":"public/js/third-party/tags/pdf.js","hash":"af78c22f0e61c8c8aa8794e585e0d632c6d4fcb8","modified":1640330174162},{"_id":"public/css/main.css","hash":"a11ba827a33ddb3529cc9d6ab60b000b66656094","modified":1640330174162},{"_id":"public/2021/12/14/Tips-in-Papers/Tnet_over.jpg","hash":"ed04ca4049c6f909b0bf51ea57ffa9a2b445e1c8","modified":1640330174162},{"_id":"public/2021/12/10/An-Introduction-to-Git/git.jpg","hash":"db9ed8bb86df7e73d5be3bcae4cc8656e4a7a0ed","modified":1640330174162}],"Category":[{"name":"Programming","_id":"ckxk28f620004vgulbzdg99xl"},{"name":"Little Things","_id":"ckxk28f67000avgul58bubcpm"},{"name":"Experiments","_id":"ckxk28f6c000kvguld0yedyr8"},{"name":"Reinforcement Learning","_id":"ckxk28f6d000nvguldl55d8rm"},{"name":"About Papers","_id":"ckxk28f6e000rvgulecm21eqa"},{"name":"Git","parent":"ckxk28f67000avgul58bubcpm","_id":"ckxk28f6f000uvgul8ijm9jpg"},{"name":"Natural Language Processing","_id":"ckxk28f6i0010vgul60i0cjje"},{"name":"Hexo","parent":"ckxk28f67000avgul58bubcpm","_id":"ckxk28f6j0018vgul53s85s60"}],"Data":[],"Page":[{"title":"about","date":"2021-12-12T13:52:49.000Z","_content":"\n\n\nI received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.\n\nI major in deep learning, computer vision, natural language processing, and reinforcement learning.\n\nI am also interested in high performance computing.\n\n<br/>\n\n<br/>\n\n---\n\n<br/>\n\n<br/>\n\n> ​     *There is a pleasure in the pathless woods;*\n> ​     *there is a rapture on the lonely shore;*\n> ​     *there is society, where none intrudes,*\n> ​     *by the deep sea, and music in its roar;*\n> ​     *I love not man the less, but nature more...*\n> ​                          *by George Gordon Byron* \n\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2021-12-12 21:52:49\n---\n\n\n\nI received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.\n\nI major in deep learning, computer vision, natural language processing, and reinforcement learning.\n\nI am also interested in high performance computing.\n\n<br/>\n\n<br/>\n\n---\n\n<br/>\n\n<br/>\n\n> ​     *There is a pleasure in the pathless woods;*\n> ​     *there is a rapture on the lonely shore;*\n> ​     *there is society, where none intrudes,*\n> ​     *by the deep sea, and music in its roar;*\n> ​     *I love not man the less, but nature more...*\n> ​                          *by George Gordon Byron* \n\n","updated":"2021-12-17T08:03:56.458Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckxk28f5t0000vgul90qh97xg","content":"<html><head></head><body><p>I received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.</p>\n<p>I major in deep learning, computer vision, natural language processing, and reinforcement learning.</p>\n<p>I am also interested in high performance computing.</p>\n<br>\n\n<br>\n\n<hr>\n<br>\n\n<br>\n\n<blockquote>\n<p>​     <em>There is a pleasure in the pathless woods;</em><br>​     <em>there is a rapture on the lonely shore;</em><br>​     <em>there is society, where none intrudes,</em><br>​     <em>by the deep sea, and music in its roar;</em><br>​     <em>I love not man the less, but nature more…</em><br>​                          <em>by George Gordon Byron</em> </p>\n</blockquote>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<p>I received the B.S. degree in the School of Engineering and Technology, China University of Geosciences (Beijing) in 2015. I am currently working towards the Ph.D degree in the Beijing Key Laboratory of Work Safety Intelligent Monitoring, the Department of EE, BUPT.</p>\n<p>I major in deep learning, computer vision, natural language processing, and reinforcement learning.</p>\n<p>I am also interested in high performance computing.</p>\n<br/>\n\n<br/>\n\n<hr>\n<br/>\n\n<br/>\n\n<blockquote>\n<p>​     <em>There is a pleasure in the pathless woods;</em><br>​     <em>there is a rapture on the lonely shore;</em><br>​     <em>there is society, where none intrudes,</em><br>​     <em>by the deep sea, and music in its roar;</em><br>​     <em>I love not man the less, but nature more…</em><br>​                          <em>by George Gordon Byron</em> </p>\n</blockquote>\n"},{"title":"tags","date":"2021-12-08T06:48:06.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2021-12-08 14:48:06\ntype: \"tags\"\n---\n","updated":"2021-12-08T06:48:39.840Z","path":"tags/index.html","comments":1,"layout":"page","_id":"ckxk28f5z0002vgulhrb44u31","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":""},{"title":"categories","date":"2021-11-24T11:46:36.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2021-11-24 19:46:36\ntype: \"categories\"\n---\n","updated":"2021-11-24T11:51:36.896Z","path":"categories/index.html","comments":1,"layout":"page","_id":"ckxk28f630006vgulgvfq5c19","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":""}],"Post":[{"title":"Algorithm","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-17T08:26:10.000Z","password":null,"summary":null,"description":"遇到过精巧的算法设计","_content":"","source":"_posts/Algorithm.md","raw":"---\ntitle: Algorithm\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-17 16:26:10\npassword:\nsummary:\ndescription: 遇到过精巧的算法设计\ncategories:\n- Programming\ntags:\n- Algorithm\n- Programming\n---\n","slug":"Algorithm","published":1,"updated":"2021-12-17T08:46:22.799Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f5w0001vgul3cz5ei18","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":""},{"title":"An Introduction to Git","date":"2021-12-10T13:55:33.000Z","description":"介绍git里面的一些基本概念，了解git运行的基本原理。","summary":null,"_content":"\n\n\n\n\n\n\n![The Structure of Git](git.jpg)\n\ngit checkout用于切换分支或恢复工作数文件，它是一个危险的命令，因为这条命令会重写工作区。\n\ngit ls-files查看缓存区中文件信息，它的参数有，括号里面是简写\n\n--cached (-c) 查看缓存区中所有文件\n\n--midified (-m)查看修改过的文件\n\n--delete (-d)查看删除过的文件\n\n--other (-o)查看没有被git跟踪的文件\n\n","source":"_posts/An-Introduction-to-Git.md","raw":"---\ntitle: An Introduction to Git\ndate: 2021-12-10 21:55:33\ndescription: 介绍git里面的一些基本概念，了解git运行的基本原理。\nsummary:\ncategories:\n- Little Things\n- Git\ntags:\n- Git\n---\n\n\n\n\n\n\n\n![The Structure of Git](git.jpg)\n\ngit checkout用于切换分支或恢复工作数文件，它是一个危险的命令，因为这条命令会重写工作区。\n\ngit ls-files查看缓存区中文件信息，它的参数有，括号里面是简写\n\n--cached (-c) 查看缓存区中所有文件\n\n--midified (-m)查看修改过的文件\n\n--delete (-d)查看删除过的文件\n\n--other (-o)查看没有被git跟踪的文件\n\n","slug":"An-Introduction-to-Git","published":1,"updated":"2021-12-17T08:46:26.683Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f600003vgulcnsib3rm","content":"<html><head></head><body><p><img src=\"git.jpg\" alt=\"The Structure of Git\"></p>\n<p>git checkout用于切换分支或恢复工作数文件，它是一个危险的命令，因为这条命令会重写工作区。</p>\n<p>git ls-files查看缓存区中文件信息，它的参数有，括号里面是简写</p>\n<p>–cached (-c) 查看缓存区中所有文件</p>\n<p>–midified (-m)查看修改过的文件</p>\n<p>–delete (-d)查看删除过的文件</p>\n<p>–other (-o)查看没有被git跟踪的文件</p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<p><img src=\"git.jpg\" alt=\"The Structure of Git\"></p>\n<p>git checkout用于切换分支或恢复工作数文件，它是一个危险的命令，因为这条命令会重写工作区。</p>\n<p>git ls-files查看缓存区中文件信息，它的参数有，括号里面是简写</p>\n<p>–cached (-c) 查看缓存区中所有文件</p>\n<p>–midified (-m)查看修改过的文件</p>\n<p>–delete (-d)查看删除过的文件</p>\n<p>–other (-o)查看没有被git跟踪的文件</p>\n"},{"title":"Construct Your Blog with Hexo and Github","date":"2021-11-24T08:20:43.000Z","description":"Hexo, Next等的使用和介绍。","_content":"\n\n# 主要参考博客\n\nhttps://segmentfault.com/a/1190000017986794\n\nhttps://godweiyang.com/2018/04/13/hexo-blog/\n\nhttps://blog.guaoxiaohei.com/posts/Hexo-Level/\n\nhttps://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/\n\nhttps://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/\n\n# 实现的时候也遇到了一些问题\n\n## 本地显示没问题，hexo d之后 在GitHub无法显示主题样式\n\n更改主目录下面的_config.yml文件\n\n\n\nurl: github远程仓库的地址, 如 https://xyegithub.github.io/myBlog/\n\nroot: url的最后一段，如/myBlog/\n\n更改保存之后，hexo clean; hexo g; hexo d\n\n##  git在推送的时候很容易出现网络错误\n\n参考博客\n\nhttps://juejin.cn/post/6844904193170341896\n\n刷新dns管理员cmd下运行 ipconfig /flushdns\n\n但是这个方法不是很管用\n\n### 更好的方法\n\n可以不用代理，将hexo _config.yml里的git地址由`https://github.com/xxx`修改为ssh `git@github.com:xxx/xxx`也可以\n\n## 将github page 设置为谷歌可搜索\n\n参考博客\n\nhttps://mizeri.github.io/2021/04/18/hexo-sitemap-google/\n","source":"_posts/Construct-Your-Blog-with-Hexo-and-Github.md","raw":"---\ntitle: Construct Your Blog with Hexo and Github\ndate: 2021-11-24 16:20:43\ndescription:  Hexo, Next等的使用和介绍。\ntags: \n- Hexo\ncategories:\n- Little Things\n- Hexo\n---\n\n\n# 主要参考博客\n\nhttps://segmentfault.com/a/1190000017986794\n\nhttps://godweiyang.com/2018/04/13/hexo-blog/\n\nhttps://blog.guaoxiaohei.com/posts/Hexo-Level/\n\nhttps://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/\n\nhttps://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/\n\n# 实现的时候也遇到了一些问题\n\n## 本地显示没问题，hexo d之后 在GitHub无法显示主题样式\n\n更改主目录下面的_config.yml文件\n\n\n\nurl: github远程仓库的地址, 如 https://xyegithub.github.io/myBlog/\n\nroot: url的最后一段，如/myBlog/\n\n更改保存之后，hexo clean; hexo g; hexo d\n\n##  git在推送的时候很容易出现网络错误\n\n参考博客\n\nhttps://juejin.cn/post/6844904193170341896\n\n刷新dns管理员cmd下运行 ipconfig /flushdns\n\n但是这个方法不是很管用\n\n### 更好的方法\n\n可以不用代理，将hexo _config.yml里的git地址由`https://github.com/xxx`修改为ssh `git@github.com:xxx/xxx`也可以\n\n## 将github page 设置为谷歌可搜索\n\n参考博客\n\nhttps://mizeri.github.io/2021/04/18/hexo-sitemap-google/\n","slug":"Construct-Your-Blog-with-Hexo-and-Github","published":1,"updated":"2021-12-17T03:37:51.057Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f640007vgul77d21ugl","content":"<html><head></head><body><h1 id=\"主要参考博客\"><span class=\"post-title-index\">1. </span><a href=\"#主要参考博客\" class=\"headerlink\" title=\"主要参考博客\"></a>主要参考博客</h1><p><a href=\"https://segmentfault.com/a/1190000017986794\">https://segmentfault.com/a/1190000017986794</a></p>\n<p><a href=\"https://godweiyang.com/2018/04/13/hexo-blog/\">https://godweiyang.com/2018/04/13/hexo-blog/</a></p>\n<p><a href=\"https://blog.guaoxiaohei.com/posts/Hexo-Level/\">https://blog.guaoxiaohei.com/posts/Hexo-Level/</a></p>\n<p><a href=\"https://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/\">https://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/</a></p>\n<p><a href=\"https://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/\">https://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/</a></p>\n<h1 id=\"实现的时候也遇到了一些问题\"><span class=\"post-title-index\">2. </span><a href=\"#实现的时候也遇到了一些问题\" class=\"headerlink\" title=\"实现的时候也遇到了一些问题\"></a>实现的时候也遇到了一些问题</h1><h2 id=\"本地显示没问题，hexo-d之后-在GitHub无法显示主题样式\"><span class=\"post-title-index\">2.1. </span><a href=\"#本地显示没问题，hexo-d之后-在GitHub无法显示主题样式\" class=\"headerlink\" title=\"本地显示没问题，hexo d之后 在GitHub无法显示主题样式\"></a>本地显示没问题，hexo d之后 在GitHub无法显示主题样式</h2><p>更改主目录下面的_config.yml文件</p>\n<p>url: github远程仓库的地址, 如 <a href=\"https://xyegithub.github.io/myBlog/\">https://xyegithub.github.io/myBlog/</a></p>\n<p>root: url的最后一段，如/myBlog/</p>\n<p>更改保存之后，hexo clean; hexo g; hexo d</p>\n<h2 id=\"git在推送的时候很容易出现网络错误\"><span class=\"post-title-index\">2.2. </span><a href=\"#git在推送的时候很容易出现网络错误\" class=\"headerlink\" title=\"git在推送的时候很容易出现网络错误\"></a>git在推送的时候很容易出现网络错误</h2><p>参考博客</p>\n<p><a href=\"https://juejin.cn/post/6844904193170341896\">https://juejin.cn/post/6844904193170341896</a></p>\n<p>刷新dns管理员cmd下运行 ipconfig /flushdns</p>\n<p>但是这个方法不是很管用</p>\n<h3 id=\"更好的方法\"><span class=\"post-title-index\">2.2.1. </span><a href=\"#更好的方法\" class=\"headerlink\" title=\"更好的方法\"></a>更好的方法</h3><p>可以不用代理，将hexo _config.yml里的git地址由<code>https://github.com/xxx</code>修改为ssh <code>git@github.com:xxx/xxx</code>也可以</p>\n<h2 id=\"将github-page-设置为谷歌可搜索\"><span class=\"post-title-index\">2.3. </span><a href=\"#将github-page-设置为谷歌可搜索\" class=\"headerlink\" title=\"将github page 设置为谷歌可搜索\"></a>将github page 设置为谷歌可搜索</h2><p>参考博客</p>\n<p><a href=\"https://mizeri.github.io/2021/04/18/hexo-sitemap-google/\">https://mizeri.github.io/2021/04/18/hexo-sitemap-google/</a></p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"主要参考博客\"><a href=\"#主要参考博客\" class=\"headerlink\" title=\"主要参考博客\"></a>主要参考博客</h1><p><a href=\"https://segmentfault.com/a/1190000017986794\">https://segmentfault.com/a/1190000017986794</a></p>\n<p><a href=\"https://godweiyang.com/2018/04/13/hexo-blog/\">https://godweiyang.com/2018/04/13/hexo-blog/</a></p>\n<p><a href=\"https://blog.guaoxiaohei.com/posts/Hexo-Level/\">https://blog.guaoxiaohei.com/posts/Hexo-Level/</a></p>\n<p><a href=\"https://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/\">https://www.itfanr.cc/2021/04/16/hexo-blog-article-encryption/</a></p>\n<p><a href=\"https://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/\">https://www.zhangxinxu.com/wordpress/2021/05/css-html-hr/</a></p>\n<h1 id=\"实现的时候也遇到了一些问题\"><a href=\"#实现的时候也遇到了一些问题\" class=\"headerlink\" title=\"实现的时候也遇到了一些问题\"></a>实现的时候也遇到了一些问题</h1><h2 id=\"本地显示没问题，hexo-d之后-在GitHub无法显示主题样式\"><a href=\"#本地显示没问题，hexo-d之后-在GitHub无法显示主题样式\" class=\"headerlink\" title=\"本地显示没问题，hexo d之后 在GitHub无法显示主题样式\"></a>本地显示没问题，hexo d之后 在GitHub无法显示主题样式</h2><p>更改主目录下面的_config.yml文件</p>\n<p>url: github远程仓库的地址, 如 <a href=\"https://xyegithub.github.io/myBlog/\">https://xyegithub.github.io/myBlog/</a></p>\n<p>root: url的最后一段，如/myBlog/</p>\n<p>更改保存之后，hexo clean; hexo g; hexo d</p>\n<h2 id=\"git在推送的时候很容易出现网络错误\"><a href=\"#git在推送的时候很容易出现网络错误\" class=\"headerlink\" title=\"git在推送的时候很容易出现网络错误\"></a>git在推送的时候很容易出现网络错误</h2><p>参考博客</p>\n<p><a href=\"https://juejin.cn/post/6844904193170341896\">https://juejin.cn/post/6844904193170341896</a></p>\n<p>刷新dns管理员cmd下运行 ipconfig /flushdns</p>\n<p>但是这个方法不是很管用</p>\n<h3 id=\"更好的方法\"><a href=\"#更好的方法\" class=\"headerlink\" title=\"更好的方法\"></a>更好的方法</h3><p>可以不用代理，将hexo _config.yml里的git地址由<code>https://github.com/xxx</code>修改为ssh <code>git@github.com:xxx/xxx</code>也可以</p>\n<h2 id=\"将github-page-设置为谷歌可搜索\"><a href=\"#将github-page-设置为谷歌可搜索\" class=\"headerlink\" title=\"将github page 设置为谷歌可搜索\"></a>将github page 设置为谷歌可搜索</h2><p>参考博客</p>\n<p><a href=\"https://mizeri.github.io/2021/04/18/hexo-sitemap-google/\">https://mizeri.github.io/2021/04/18/hexo-sitemap-google/</a></p>\n"},{"title":"Experiments","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-17T02:00:56.000Z","password":null,"summary":null,"description":"发现统计规律，记录一些自己diy的实验。","_content":"\n\n\n# Deep Learning\n\n## Feature Map Multiplication\n\n[source code](https://github.com/xyegithub/Featrue-map-multiplication)\n\ndataset: Caltech101\n\n**shortcut使用bn，而Res分支使用sigmoid的情况。**\n\n| 配置                                                         | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (out.sigmoid() + 1)* self.bn_s(self.shortcut(x))`     | 78.23    |\n| `out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))` | 78.69    |\n| `(self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 78.57    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))    ` | 82.26    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 84.10    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x))             ` | 84.85    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 85.83    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 81.57    |\n\n1. 从这个结果看出shortcut的均值为1，会使得优化更好\n2. Res分支的sigmoid不需要加0.5或者1，性能提高了。乘以sigmoid本身有恒等的特性。sigmoid分支输出都为0时，sigmoid输入都是0.5。\n3. 在Res分支的sigmoid之前，先对out进行bn归一化，会优化的更好，而且让归一化的均值为0，会优化的更好。但是如果同时也控制归一化的方差，效果变差。无参的bn限制了表达能力。\n\n**shortcut使用bn，而Res分支使用sigmoid的情况。**\n\n| 配置                                                         | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (self.shortcut(x).sigmoid() + 1)* self.bn_s(out)`     | 83.93    |\n| `out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)` | 85.54    |\n| `(self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 85.14    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)    ` | 85.43    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 87.44    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out)             ` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.91    |\n\n**shortcut使用bn，而Res分支使用bn的情况。**\n\n| 配置                                                         | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out)* self.bn_s(self.shortcut(x))`            | 77.13    |\n| `self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x)) ` | 75.75    |\n| `self.bn.bias.data[:] = 1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 79.55    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.39    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.05    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 80.24    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.22    |\n\n均值为1的话，一般来说还是会获益。但是bn的效果不是很好。\n\n**shortcut使用sigmoid，而Res分支使用sigmoid的情况。**\n\n| 配置                                                         | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * out.sigmoid())`        | 79.44    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 0.5)` | 70.05    |\n| `out = (self.shortcut(x).sigmoid() + 0.5) * (out.sigmoid())` | 76.61    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 1)`   | 72.64    |\n| `out = (self.shortcut(x).sigmoid() + 1) * (out.sigmoid())`   | 71.77    |\n\n没有加bn，sigmoid会过饱和，效果不是很好。一边加bn\n\n| 配置                                                         | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 86.69    |\n| `out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 79.90    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 76.32    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 83.99    |\n| `self.bn.bias.data[:]=0 `<br>` out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 0.5)` | 86.52    |\n| `self.bn.bias.data[:]=0 `<br>`out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 1) ` | 82.83    |\n| `self.bn.weight.data[:]=1`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 78.74    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * out.sigmoid()` | 73.39    |\n\n两边加bn\n\n| 配置                                                         | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| ` out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.23    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.41    |\n\n","source":"_posts/Experiments.md","raw":"---\ntitle: Experiments\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-17 10:00:56\npassword:\nsummary:\ndescription: 发现统计规律，记录一些自己diy的实验。\ncategories:\n- Experiments\ntags:\n- Personal Thought\n- Experiments\n- private\n---\n\n\n\n# Deep Learning\n\n## Feature Map Multiplication\n\n[source code](https://github.com/xyegithub/Featrue-map-multiplication)\n\ndataset: Caltech101\n\n**shortcut使用bn，而Res分支使用sigmoid的情况。**\n\n| 配置                                                         | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (out.sigmoid() + 1)* self.bn_s(self.shortcut(x))`     | 78.23    |\n| `out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))` | 78.69    |\n| `(self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 78.57    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))    ` | 82.26    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) ` | 84.10    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x))             ` | 84.85    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 85.83    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))` | 81.57    |\n\n1. 从这个结果看出shortcut的均值为1，会使得优化更好\n2. Res分支的sigmoid不需要加0.5或者1，性能提高了。乘以sigmoid本身有恒等的特性。sigmoid分支输出都为0时，sigmoid输入都是0.5。\n3. 在Res分支的sigmoid之前，先对out进行bn归一化，会优化的更好，而且让归一化的均值为0，会优化的更好。但是如果同时也控制归一化的方差，效果变差。无参的bn限制了表达能力。\n\n**shortcut使用bn，而Res分支使用sigmoid的情况。**\n\n| 配置                                                         | accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out) + self.shortcut(x) `                     | 87.62    |\n| `out = (self.shortcut(x).sigmoid() + 1)* self.bn_s(out)`     | 83.93    |\n| `out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)` | 85.54    |\n| `(self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 85.14    |\n| `self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)    ` | 85.43    |\n| `self.bn_s.bias.data[:]=1`  <br>`out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) ` | 87.44    |\n| ` self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>` out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out)             ` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.39    |\n| `self.bn.bias.data[:]=0`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)` | 84.91    |\n\n**shortcut使用bn，而Res分支使用bn的情况。**\n\n| 配置                                                         | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = self.bn(out)* self.bn_s(self.shortcut(x))`            | 77.13    |\n| `self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x)) ` | 75.75    |\n| `self.bn.bias.data[:] = 1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 79.55    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.39    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.05    |\n| `self.bn.bias.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 80.24    |\n| `self.bn.bias.data[:]=1`<br>`self.bn.weight.data[:]=1`<br>`self.bn_s.bias.data[:]=1`<br>`self.bn_s.weight.data[:]=1`<br>`out = self.bn(out)* self.bn_s(self.shortcut(x))` | 81.22    |\n\n均值为1的话，一般来说还是会获益。但是bn的效果不是很好。\n\n**shortcut使用sigmoid，而Res分支使用sigmoid的情况。**\n\n| 配置                                                         | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * out.sigmoid())`        | 79.44    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 0.5)` | 70.05    |\n| `out = (self.shortcut(x).sigmoid() + 0.5) * (out.sigmoid())` | 76.61    |\n| `out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 1)`   | 72.64    |\n| `out = (self.shortcut(x).sigmoid() + 1) * (out.sigmoid())`   | 71.77    |\n\n没有加bn，sigmoid会过饱和，效果不是很好。一边加bn\n\n| 配置                                                         | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| `out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 86.69    |\n| `out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 79.90    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()` | 76.32    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 83.99    |\n| `self.bn.bias.data[:]=0 `<br>` out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 0.5)` | 86.52    |\n| `self.bn.bias.data[:]=0 `<br>`out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 1) ` | 82.83    |\n| `self.bn.weight.data[:]=1`<br>`out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()` | 78.74    |\n| `self.bn_s.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * out.sigmoid()` | 73.39    |\n\n两边加bn\n\n| 配置                                                         | Accuracy |\n| ------------------------------------------------------------ | -------- |\n| ` out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.23    |\n| `self.bn.bias.data[:]=0`<br>`out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()` | 86.41    |\n\n","slug":"Experiments","published":1,"updated":"2021-12-24T02:26:16.249Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f650008vgulfvge1air","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"43f76d29e12f98024948db4784d2fbf850ed17de1e50d322dd0ae255a3fffa3b\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/myBlog/lib/hbe.js\"></script><link href=\"/myBlog/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{}},"excerpt":"这是一篇加密文章，需要密码才能继续阅读。","more":"这是一篇加密文章，需要密码才能继续阅读。","origin":"<html><head></head><body><h1 id=\"Deep-Learning\"><span class=\"post-title-index\">1. </span><a href=\"#Deep-Learning\" class=\"headerlink\" title=\"Deep Learning\"></a>Deep Learning</h1><h2 id=\"Feature-Map-Multiplication\"><span class=\"post-title-index\">1.1. </span><a href=\"#Feature-Map-Multiplication\" class=\"headerlink\" title=\"Feature Map Multiplication\"></a>Feature Map Multiplication</h2><p><a href=\"https://github.com/xyegithub/Featrue-map-multiplication\">source code</a></p>\n<p>dataset: Caltech101</p>\n<p><strong>shortcut使用bn，而Res分支使用sigmoid的情况。</strong></p>\n<table>\n<thead>\n<tr>\n<th>配置</th>\n<th>accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>out = self.bn(out) + self.shortcut(x) </code></td>\n<td>87.62</td>\n</tr>\n<tr>\n<td><code>out = (out.sigmoid() + 1)* self.bn_s(self.shortcut(x))</code></td>\n<td>78.23</td>\n</tr>\n<tr>\n<td><code>out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))</code></td>\n<td>78.69</td>\n</tr>\n<tr>\n<td><code>(self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) </code></td>\n<td>78.57</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid() + 1) * self.bn_s(self.shortcut(x))    </code></td>\n<td>82.26</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=1</code>  <br><code>out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x)) </code></td>\n<td>84.10</td>\n</tr>\n<tr>\n<td><code> self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid() + 0.5) * self.bn_s(self.shortcut(x))            </code></td>\n<td>84.85</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))</code></td>\n<td>85.83</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(out).sigmoid()) * self.bn_s(self.shortcut(x))</code></td>\n<td>81.57</td>\n</tr>\n</tbody></table>\n<ol>\n<li>从这个结果看出shortcut的均值为1，会使得优化更好</li>\n<li>Res分支的sigmoid不需要加0.5或者1，性能提高了。乘以sigmoid本身有恒等的特性。sigmoid分支输出都为0时，sigmoid输入都是0.5。</li>\n<li>在Res分支的sigmoid之前，先对out进行bn归一化，会优化的更好，而且让归一化的均值为0，会优化的更好。但是如果同时也控制归一化的方差，效果变差。无参的bn限制了表达能力。</li>\n</ol>\n<p><strong>shortcut使用bn，而Res分支使用sigmoid的情况。</strong></p>\n<table>\n<thead>\n<tr>\n<th>配置</th>\n<th>accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>out = self.bn(out) + self.shortcut(x) </code></td>\n<td>87.62</td>\n</tr>\n<tr>\n<td><code>out = (self.shortcut(x).sigmoid() + 1)* self.bn_s(out)</code></td>\n<td>83.93</td>\n</tr>\n<tr>\n<td><code>out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)</code></td>\n<td>85.54</td>\n</tr>\n<tr>\n<td><code>(self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) </code></td>\n<td>85.14</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid() + 1) * self.bn_s(out)    </code></td>\n<td>85.43</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=1</code>  <br><code>out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out) </code></td>\n<td>87.44</td>\n</tr>\n<tr>\n<td><code> self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid() + 0.5) * self.bn_s(out)            </code></td>\n<td>84.39</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)</code></td>\n<td>84.39</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = (self.bn(self.shortcut(x)).sigmoid()) * self.bn_s(out)</code></td>\n<td>84.91</td>\n</tr>\n</tbody></table>\n<p><strong>shortcut使用bn，而Res分支使用bn的情况。</strong></p>\n<table>\n<thead>\n<tr>\n<th>配置</th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>77.13</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x)) </code></td>\n<td>75.75</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:] = 1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>79.55</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>81.39</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>81.05</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>self.bn_s.weight.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>80.24</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=1</code><br><code>self.bn.weight.data[:]=1</code><br><code>self.bn_s.bias.data[:]=1</code><br><code>self.bn_s.weight.data[:]=1</code><br><code>out = self.bn(out)* self.bn_s(self.shortcut(x))</code></td>\n<td>81.22</td>\n</tr>\n</tbody></table>\n<p>均值为1的话，一般来说还是会获益。但是bn的效果不是很好。</p>\n<p><strong>shortcut使用sigmoid，而Res分支使用sigmoid的情况。</strong></p>\n<table>\n<thead>\n<tr>\n<th>配置</th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>out = (self.shortcut(x).sigmoid()) * out.sigmoid())</code></td>\n<td>79.44</td>\n</tr>\n<tr>\n<td><code>out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 0.5)</code></td>\n<td>70.05</td>\n</tr>\n<tr>\n<td><code>out = (self.shortcut(x).sigmoid() + 0.5) * (out.sigmoid())</code></td>\n<td>76.61</td>\n</tr>\n<tr>\n<td><code>out = (self.shortcut(x).sigmoid()) * (out.sigmoid() + 1)</code></td>\n<td>72.64</td>\n</tr>\n<tr>\n<td><code>out = (self.shortcut(x).sigmoid() + 1) * (out.sigmoid())</code></td>\n<td>71.77</td>\n</tr>\n</tbody></table>\n<p>没有加bn，sigmoid会过饱和，效果不是很好。一边加bn</p>\n<table>\n<thead>\n<tr>\n<th>配置</th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>86.69</td>\n</tr>\n<tr>\n<td><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()</code></td>\n<td>79.90</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * out.sigmoid()</code></td>\n<td>76.32</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>83.99</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0 </code><br><code> out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 0.5)</code></td>\n<td>86.52</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0 </code><br><code>out = (self.shortcut(x).sigmoid()) * (self.bn(out).sigmoid() + 1) </code></td>\n<td>82.83</td>\n</tr>\n<tr>\n<td><code>self.bn.weight.data[:]=1</code><br><code>out = (self.shortcut(x).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>78.74</td>\n</tr>\n<tr>\n<td><code>self.bn_s.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid() + 0.5) * out.sigmoid()</code></td>\n<td>73.39</td>\n</tr>\n</tbody></table>\n<p>两边加bn</p>\n<table>\n<thead>\n<tr>\n<th>配置</th>\n<th>Accuracy</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code> out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>86.23</td>\n</tr>\n<tr>\n<td><code>self.bn.bias.data[:]=0</code><br><code>out = (self.bn_s(self.shortcut(x)).sigmoid()) * self.bn(out).sigmoid()</code></td>\n<td>86.41</td>\n</tr>\n</tbody></table>\n</body></html>","encrypt":true},{"title":"First Step to Reinforcement Learning","date":"2021-12-03T08:48:41.000Z","description":"强化学习的入门介绍；什么是强化学习；深度强化学习算法：策略网络(Policy Network)和估值网络(Value Network)。","_content":"\n\n\n# 什么是强化学习，关于强化学习的几点疑问\n\n<font color=green size=3>强化学习三要素：环境状态，行动，奖励</font>\n\n<font color=green size=3>目标：尽量多的获得奖励</font>\n\n<font color=green size=3>本质：连续决策</font>\n\n基本的强化学习模型包括：\n\n* 环境状态的集合S\n* 动作的集合A\n* 状态之间的转换规则（是环境的一部分）\n* 规定转换后“即时奖励”的规则（是环境的一部分）\n* 描述主体（智能体）能够观察到什么的规则（是环境的一部分）\n* 能够做出决策/动作的主体（智能体）\n\n## 区别于深度学习，强化学习的本质特点是什么？\n\n**两个定义**\n\n1，强化学习是机器学习的一个重要分支，主要用来解决连续决策问题。\n\n2，强化学习又称 再励学习，评价学习或增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在于环境交互过程中通过学习策略以达成回报最大或实现特定目标的问题。\n\n强化学习的本质是<font color=red> 描述和解决智能体（agent）在于环境交互过程中通过学习策略以达成回报最大或实现特定目标的问题。</font>它本质是这样一种场景，在这种场景中它为了达到某种目的，做出连续的决策。这样符合这种场景，那就是强化学习。\n\n强化学习和深度学习这两个名词的维度是不一样的。深度学习描述的是算法本身的特点，深度够不够，是不是连接主义的模型。深度学习可以用来做无监督，也可以用来做半监督，也可以用来做弱监督，甚至可以用来作为强化学习算法的一部分。深度学习这个名词，不管应用场景只管模型本身是不是满足深度学习的特点。\n\n而强化学习描述的是应用场景的特点，只要能提供智能体决策的算法，管它是什么模型，什么结构，那就是强化学习的算法。从这个角度上讲，我现在认为强化学习和监督学习，无监督学习，弱监督学习，是并列的，是对应用场景的描述。和深度学习不是同一维度的。\n\n<font color=red>强化学习的本质在于目标给定的形式，不像无监督学习那样完全没有学习目标，也不像监督学习那样有非常明确的目标（label），强化学习的目标一般是变化的，不明确的，甚至可能不存在绝对正确的目标。，强化学习的问题都可以抽象成，环境状态，行动和奖励。应该说只要能抽象为这三个要素，目标是获得最大奖励的模型，就是强化学习的模型。</font>\n\n强化学习的最大的特点是“试错”，是尝试各种可能，而强化结果好的可能。（策略网络的特点，估值网络的特点是修正和预测获益）\n\n由于强化学习是一种决策学习，这个问题的特点就是离散型。但是离散并不就是强化学习（连续的决策才是，目标的模糊和不确定性是决策问题的特点）。深度学习本质是函数的拟合，所以连续可微是它的特点。并不能说连续可微的问题就是深度学习，离散的问题就是强化学习。 \n\n## 深度学习用梯度下降的算法实现模型的优化，强化学习无法求导，甚至连学习目标都是模糊的如何优化模型参数？\n\n这个问题要了解下一节，具体的强化学习方法。\n\n## 强化学习与弱监督学习？\n\n弱监督是属于有监督学习，只是它学习的目标不是被给予的标签，而是比被给予标签更强的标签（强弱是指标签做含有的信息量），也就是说弱监督是根据少量信息的标签，推测出更多的信息。\n\n强化学习的本质是连续决策。连续决策的特点是目标的模糊和不确定性。\n\n所以，虽然弱监督和强化学习都没有给出最终准确的目标，但是他们任然很不同的\n\n## 深度学习是死的，没有智能的机器学习算法，强化学习是“活的”，有智能的机器学习算法\n\n深度学习大部分我们用作有监督的学习算法。其实说深度学习是死的，不如说有监督学习是死的。\n\n有监督学习其实是完全的复刻标签里面含有的知识，它的本质就是一个函数拟合的问题，它无法摆脱对绝对的标签的依赖，无法超越标签。\n\n而强化学习，正由于它的目标是模糊和不确定的。使得算法在设计上必须具有随机性和探索性，它能够探索出人类从来没有到过的领域。就像在围棋上，下出人类完全无法理解的棋，人从来没有想过的一些下法。这就是强化学习算法探索出来的知识。所以我觉得它是活着的，拥有智能的算法。\n\n<font color=green>从感性的层面，强化学习算法很接近人脑的行为：感知环境，探索环境，强化有益行为</font>\n\n## 关于深度学习，强化学习，连续可导性和离散不可导性的讨论\n\n1. 从函数的角度，深度学习和强化学习都需要学习一个函数映射。深度学习是从输入到target的映射。强化学习是学从环境状态到Action的映射。这两个映射可以看成性质一样的，因为深度学习可以作为强化学习的智能体。所以从函数的角度，他们没有连续和离散的区别。（PS. 深度学习模型和强化学习的智能体都是连续可导的函数。target和Action都可以是离散的或者连续的。）\n\n2. 深度学习作为用梯度下降算法优化的模型，无法优化对loss不可导的参数。如在深度学习中，设计一个分支来决定模型是否应该包含某个模块，这个分支的参数是不可优化的。因为包含和不包含是离散的，对loss不可导。\n\n   不过假设，如果包含与不包含是连续的。也就是说，可以以0.1的权重包含。那么，这些参数是可以优化的。从这点来看，离散就是导致模型参数不可优化的原因。\n\n3. 强化学习在离散的情况下解决2中的问题。由此，我得到了一个概念，强化学习解决离散的问题。\n\n4. 下面我们来分析一下这个场景。首先，决定一个模块该不该被使用，这个场景是一次Action的场景，不是连续决策的场景。也就是说Action一次，我就能知道最终reward多少了，只有单步的reward。\n\n   限于这个单次决策的场景来看，如果Action相对于reward是连续可导的，那么深度学习就能解决这个问题。如果Action相对于reward是离散的，那么仅仅深度学习无法解决这个问题，要靠强化学习。\n\n   <font color=green>这里单次Action就知道reward，实际上这个问题就退化为了有监督学习，因为这个单步的reward就可以看成我的标签了。所以深度学习解决这个问题是很自然的。</font>\n\n5. 多次决策的问题，无论是连续的还是离散的都只能用强化学习的方法。因为多次决策，这个问题就不可以退化为监督学习的问题了。它是一个真正的强化学习的问题。\n\n   \n\n综上所诉，强化学习解决深度学习解决不了的离散问题，那只是在单次决策的时候，这个问题退化为了有监督学习。强化学习的方法，恰好可以提供离散变量的学习。\n\n强化学习方法解决深度学习中的离散问题，仅仅是强化学习附带的一个小福利。\n\n**<font color=green>因为它能把经验转化为可导的目标，就拿策略网络来说，从梯度的角度，它只管增加当前随机Action的概率，而加入advatage之后，自动就优化除了想要的大reward的行为。</font>**\n\n## 深度学习优化和强化学习优化的感性理解\n\n前面说了深度学习优化可导的参数，强化学习可以优化不可导的参数。这里说一下对深度学习优化方法和强化学习优化方法的感性理解。\n\n还是说前面包不会包含某模块的例子，由于连续可导，对于每一个参数值，深度学习模型其实都同时参与了两种Action（包含和不包含）。score = 0.1 包含，其实其中包含了含有的成分，也包含了不含有的成分。所以我们可以连续的变动score，看看包含多好，还是不好含多好。这其实就是梯度下降算法的方式。得益于每一个参数，其实我都对包含和不包含的情况都有了解，我当然知道哪个更好，就往那边移动（优化）。\n\n然而，对于离散的情况，要么只能包含，要么只能不包含。当选择包含的时候，模型对不包含的情况完全是无知的。可能更好，也可能差。当不包含的时候，也是一样的。无论哪种情况，我都没有办法优化，两种情况是完全隔离开的，信息不沟通的，是离散的。所以梯度下降算法无法优化它。\n\n强化学习用随机探索的方法让两者信息又沟通起来。包含一下试一下，然后，不包含也试一下。尝试的结果是哪种reward多，就增大哪种的概率。\n\n**<font color=green>所以，无论哪种优化方法，信息的沟通都是必要的。要对所有的action都了解，才能知道选择哪种action。 只是深度学习是连续的，它的每一种参数，都包含了所有Action的信息（reward），每一种Action都参与了，所以它能直接连续的梯度下降的优化，不需要随机探索了。而对于离散的，每种action只能知道自己的reward，对其他Action一无所知的时候，梯度的优化是不行的。必须要探索各种Action，还是要知道了每一种Action的情况(reward)之后，才能优化。这是方法论。</font>**\n\n更进一步，离散的地方，相对于reward一定是不可导的，所以深度学习不行。而强化学习，更准确的说是策略网络，相当于给离散的地方加了标签，这样它就在离散的地方有监督了，它就可以根据增加的标签优化。而标签的设计就是根据探索的结果，增大reward大的Action，reward大的Action就是它的标签，而且这个标签是动态的，是对抗得出的。\n\n强化学习方法算出的梯度是策略梯度。\n\n**<font color=green>强化学习： 不知道选哪边了； 试试呗；按试出来reward大的Action优化它。</font>**\n\n**<font color=green>强化学习：它离散，对于reward不可导；不直接用reward优化它，给它加个标签，把试出来reward大的Action，作为标签去优化</font>**\n\n在不可导的地方加标签。\n\n**<font color=green>由此，强化学习算法的本质是制作标签，无论是连续决策，标签不确定的情况，还是它能解决离散问题的情况，它都是用制作标签的方法解决的。</font>**\n\n# 策略网络(Policy Network)和估值网络(Value Network)\n\nAlphaGo 使用了快速走子，策略网络，估值网络和蒙特卡洛搜索树等技术。\n\n强化学习算法的一个关键是<font color=green>随机性和探索性</font>，我们需要让算法通过试验样本自己学习什么才是某个环境状态下比较好的Action，而不是像有监督学习一样，告诉模型什么是好的Action，因为我们也不知道什么是好的Action.\n\n深度强化学习模型的本质是神经网络，神经网络是工具，根据问题转化以及建模的不同，主要分为策略网络和估值网络。\n\n强化学习中最重要的两类方法**Policy-based,Value-based**。第一种直接预测在某个环境下应该采取的行动（直接输出改采取Action的概率）。第二种预测在某个环境下所有行动的期望价值，然后通过选择q值最高的行动执行策略。\n\n他们都能完成决策，但由于建模的不同，估值网络包含有更多的信息，它不仅能提供决策，还预测了决策带来的收益。\n\n<font color=red>策略网络是隐式的学习了某一Action所带来的全部获益（当前获益+后续获益），而估值网络直接显示的学习Action所带来的全部获益。</font>强化学习算法做出最佳抉择只需要知道哪个Action全部获益最大，策略网络就是这样做的，估值网络不仅学习了哪个Action全部获益最大，还把每个Action的全部获益给计算出来了。\n\n<font color=green>相对来说，策略网络的性能会比估值网络好一些。</font>\n\n<font color=green>Value Based方法适合仅有少量Action的环境，而Policy Based方法更通用，适合Action种类非常多，或者具有连续取值的Action的环境。结合了深度学习之后，Policy Based方法就变成了策略网络，Value Based方法就变成了估值网络。</font>\n\n## 策略网络(Policy Network)\n\n直接看一个例子，学习的目标是，左右用力使得木棍不倒地，[Policy_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py) [Policy_Network.py](policy_network.py)\n\n关键代码\n\n ```python\n score = tf.matmul(layer1,W2)\n probability = tf.nn.sigmoid(score)#网络输出采取Action 1的概率。\n input_y = tf.placeholder(tf.float32,[None,1], \\\\\n                          name=\"input_y\")# 输入采取过的行为，这个行为是随机生成的。\n advantages = tf.placeholder(tf.float32,name=\"reward_signal\") \n # 输入获益\n loglik = tf.log(input_y*(input_y - probability) + \\\\\n                 (1 - input_y)*(input_y + probability)) \n # 损失函数，如果行为是1，则增大\n \n #概率，如果行为是 0，则减小概率（相当于也是增加0的概率），也就是说这个损\n #失函数无论当前行为是什么都会增大当前行为的概率。\n loss = -tf.reduce_mean(loglik * advantages) # 这行代码很关键，\n #相当于给损失函数成了一个权重advantages，得到最终的损失函数。\n #advantages是当前试验的全部获益。如果全部获益大，将以更大的权重，增加 \n #当前行为的概率。\n \n #所以，策略网络其实也是一个对抗学习的过程，增加所有采取过行为的概率，只是\n #获益多的行为以更大的权重增加。\n \n ## 一个试验： 由初始状态开始，随机采取一连串的行为\n #（Policy_Network.py 中是根据当前模型输出的概率,来生成随机的行为，但\n #是我感觉直接用0.5的概率随机生成一连串的0和1的行为也是可以的，下面将实\n #验一下），直到任务结束。\n \n ## 由于每个试验，都可以一直行为到任务结束，所以每个action，我们都可以得\n #到它在该试验中的全部获益（当前获益 + 之后所有行为的获益）\n \n ## 随机生成了n个试验，其中又各种各样的决策（随机探索），全部获益大的\n #action，它的advantages也大，那么它的概率就增大的多，它被强化的厉害。\n \n ## 试验生成的过程，实际上就是数据集构建的过程。策略网络的数据集是由环境\n #和一系列随机的行为构成的。它提供了环境在各种行为下的反应（获益）。模型\n #学习为环境带来高获益的行为的规律。\n ```\n\n由上面的代码可知，从策略网络的角度看强化学习的话，强化学习的关键其实是对数据集的构建---如何构建数据集。\n\n在构建数据集的时候，随机探索肯定是必要的。随机探索的结果会得到一系列好的行为，也会得到一系列不好的行为。如何强化好的行为就是算法设计的时候需要注意的。\n\n上面的代码在探索阶段借用了当前的模型，即根据当前模型输出的概率随机生成行为，从而形成数据集。如果完全的随机（一直使用0.5的概率随机的生成Action）会什么样呢？\n\n### 数据集是否可以和模型无关（不随着模型变化）？\n\n关键修改代码\n\n```python\n#基于当前模型，根据当前的状态x，生成Action 1的概率\ntfprob = sess.run(probability,feed_dict={observations: x})\n# 基于预测概率，随机生成行为，并试探环境。生成数据集。\naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n修改后\n\n```python\n# 注释掉这句，并不需要根据当前模型生成概率\n# tfprob = sess.run(probability,feed_dict={observations: x})\n# 直接设置概率为0.5，随机完全随机探索生成数据集。\ntfprob = 0.5\naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n结果：修改后，模型无法收敛。\n\n完全随机很小的概率能探索出很好的试验，这些好的行为也很难持续的得到强化。\n\n所以，强化学习也有一种效果叠加的感觉。在完全随机的情况下 ，探索出相对好的action，再在这个相对好的action的基础上，在探索探索出更好的action。\n\n如果数据集不依赖模型，就是一直在完全随机的基础上探索。这样很难收敛。\n\n也可以这样看，完全随机的话，最多能学到前几步的策略（因为完全随机就走不了几步，探索的经验就只有那几步）。依赖于模型，探索的行为更有价值，因为是依赖于学到过的知识的，一方面确认了，按学到的知识走，确实获益多，一方面又在学到的知识的基础上，做了一些随机，探索更好的知识。\n\n## 估值网络(Value Network，Q-learning)\n\nQ-Learing用神经网络实现，得到的模型就是估值网络。\n\n也看一个例子，[Value_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py)\n\n学习每个Action所对应的reward的期望。\n\n我们先看看数据集的结构\n\n```python\n#Save the experience to our episode buffer.\nepisodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n# 其中s是当前时刻的环境状态，a是当前随机采取的Action，r是这个Action的当前reward\n# s1是采取Action之后的下一状态，d是布尔型表示是否任务结束。  \n```\n\n1. 现在目标是学习Q(s<sub>t</sub>, a<sub>t</sub>)，也就是当前环境状态，采取行为a的全部reward的期望。\n\n2. 现在假设我们有模型Q<sub>desird</sub>，可以预测全部reward了，那么这个模型应该满足条件，Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)，这有点递归的感觉了。\n\n3. 现在我们能不能根据这个公式 ， 来优化出Q<sub>desired</sub>。肯定是能的。对于探索过的所有试验，公式都满足的话，此时的模型就可以看成我们想要的模型了。我感觉这就是估值网络方法的核心。\n\n   \n\n直接看关键代码\n\n```python\n#Choose an action by greedily (with e chance of random action)\n# from the Q-network\nif np.random.rand(1) < e or total_steps < pre_train_steps:\n    a = np.random.randint(0,4)\nelse:\n    a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n# 这段代码其实是探索的代码，当最开始的时候是完全随机探索（total_steps < pre_train_steps的时候）\n# 当total_steps >= pre_train_steps之后呢，就不是完全随机探索了。\n# 有e的概率是随机探索的，（1-e）的概率是由训练好的模型决定之后的Action。 \n# 这里是估值网络和策略网络的不同，策略网络本身就具有随机性，所以不需要引入\n# 额外的参数e和pre_train_steps来控制随机探索的强度。\n# 策略网络在训练的过程中，本身就是由随机性大，到慢慢的收敛到好的Action\n# 所以可以直接得到好的探索的训练样本。估值网络没有这样好的性质，它连\n# 随机性都没有，就需要人为的制造，满足从完全随机探索，到在好的Action的\n# 基础上具有一定的随机性进行探索。\n```\n\n**这是策略网络和估值网络的共通之处，其实这也最上面那个注释\"by greedily (with e chance of random action) from the Q-network\"的意思。**\n\n**<font color=green>“贪心”两个字完美的诠释了强化学习，无论是策略网络还是估值网络，在探索阶段，在生成数据集上的特点。</font>**\n\n在策略网络那一节，我做的那个试验，和模型无关生成数据集。其实就是不贪心了，不贪心不行。\n\n```python\nif total_steps > pre_train_steps:\n    if e > endE:\n        e -= stepDrop\n# 完全随机了之后，开始慢慢减小随机性。\n# 模型约不可靠的时候，探索性和随机性越强。后来模型慢慢变得可靠就减弱随机性。\n# 因为模型越来越可靠的时候，随机性大就会得到很多远远低于当前模型性能的试验\n# 这些试验都是早就被pass了的，学不到什么东西，损坏模型的探索。\n# endE=0.1 说明无论训练的多好，模型都保持了随机性，保持了探索性\n#　人永远要有好奇心，永远要觉得自己的知识还可能不是最好的\n```\n\n\n\n下面的代码是将在数据集弄好的情况下，如何训练模型的。\n\n```python \nif total_steps % (update_freq) == 0:\n    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n    #Below we perform the Double-DQN update to the target Q-values\n    # 主网络预测了下一刻需要采取的Action，trainBatch[:,3]是当前的下一刻的环境\n    # 回顾公式，Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # 这里主函数预测的Action就是t+1时刻（下一时刻）获益值最大的Action\n    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n    # target网络预测了下一时刻的reward\n    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n    end_multiplier = -(trainBatch[:,4] - 1)\n    # 用主网络预测出的Action以及target网络预测出的所有行为的reward\n    # 选择了最大的reward,也就是公式中的 Max Q(s_t+1, a)\n    doubleQ = Q2[range(batch_size),Q1]\n    # 这里得到的就是公式Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # 的右边，当前reward加上乘以衰减系数之后的，下一步最大reward\n    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n    #Update the network with our target values.\n    # 公式右边得到了之后，在把真正的当前状态输入进去，得到左边\n    # 左边以右边作为标签进行学习。更新主网络的参数\n    _ = sess.run(mainQN.updateModel, \\\n                 feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n    # 更新target网络的参数。\n    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n```\n\n\n\ntarget网络参数的更新方式代码\n\n```python \ndef updateTargetGraph(tfVars,tau):\n    total_vars = len(tfVars)\n    op_holder = [] \n    # 主网络是和target网络一样的，前一半参数正好是主网络的参数\n    for idx,var in enumerate(tfVars[0:total_vars//2]):\n        # idx+total_vars对应的时候后一半的参数也就是target网络的参数\n        # 这里相当于是target参数 = 主网络参数 * tau + （1- tau）*target参数\n        # 也就是说target网络在以一定的速度向主网络靠近\n        # 结合前面的代码，主网络才是真正学习的网络，target网络的作用仅仅是得到等式\n        # 右边的值，即标签，即使是等式右边，也不是target网络完全决定的\n        # target网络得到了所有Action的reward，最大的Action是主网络选择的\n        # 为什么要这么做，target网络也是在模仿主网络，只用主网络也能得到等式的右边\n        # 理论上其实右边也应该是主网络决定，现在搞了个主网络的模仿者target网络\n        # 是出于优化的考虑。我们后面叙述。\n        op_holder.append(tfVars[idx+total_vars//2].assign\\\\\n                         ((var.value()*tau) \\\\\n                      + ((1-tau)*tfVars[idx+total_vars//2].value())))\n        return op_holder\n\n    def updateTarget(op_holder,sess):\n        for op in op_holder:\n            sess.run(op)\n\n```\n\n现在其实我们把关键的代码都看了，在这段代码实现中，引入了一些state of the art的trick，下面我们结合看过的代码在提一遍。\n\n1. 引入卷积层，这段代码比较简单，我们没有看。环境状态是用图片的形式给的，用CNN提取特征是比较自然的。\n2. Experience replay。估值网络不像策略网络一样得到试验之后，用一次就扔掉再去制作新的试验（数据集）来训练。它把每次试验都放在一个试验池里面。试验池长度为N，如果超过了N，那就把最老的试验样本扔掉。每次 训练的时候从试验池里面随机选择batchsize个样本进行训练，保持了对样本的利用率，同时其实也增加了模型的稳定性，因为数据集是相对稳定的（相比于N=1而言，每次训练了就扔掉，进来的都是新的，不那么稳定）。\n3. 使用target网络来辅助训练。**之所以要用target网络来制造训练目标，用主网络来实际训练，是为了让Q-Learing训练的目标保持平稳。**强化学习不像普通的监督学习，它的目标是变化的，**因为学习目标的一部分就是模型本身输出的。**每次更新模型参数都会导致学习目标发生变化，如果更新频繁，幅度很大，我们的训练过程就会变得非常不稳定并且失控。**DQN的训练会陷入目标Q与预测Q的反馈循环中，震荡发散。**所以用target网络来制造目标，target网络和主网络又不是矛盾的，因为target网络会逼近主网络，它是主网络的模仿者，所以它提供的目标Q也是有权威的。\n4. Double DQN。这个trick源于target网络选的最大Action不准。模仿的不够好，现在就让主网络来帮它选。也就是上面代码中我们看过了主网络输出action，选择target网络输出的reward，得到公式的右边。\n5. Dueling DQN。上代码\n\n```python \nself.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\nself.VW = tf.Variable(xavier_init([h_size//2,1]))\nself.Advantage = tf.matmul(self.streamA,self.AW)\nself.Value = tf.matmul(self.streamV,self.VW)\n\n#Then combine them together to get our final Q-values.\n# 看这里Qout是网络最终预测的所有Action的reward。它由两部分组成Value和Advantage\n# 由最上面两行可以看出Value是一维的，是实数，advantage是#action维度的向量\n# 所以，Dueling DQN就是将reward裁成了两部分，一部分是环境状态本身具有的价值Value\n# 另一部分就是Action本身具有的价值，相加起来就是在这个环境下Action具有的价值。\n# 其实我感觉这些解释都是人为的，具体是不是这样谁也不知道，可能只是这样优化的好。\n#　因为即使不分为两部分，网络输出Ｑout的时候，输入也是环境状态，肯定都会把环境考虑\n# 进去才有Action的价值。然而直接输出这个价值，发现优化的不好，分为两部分之后，发现\n# 优化的好了。其实谁也不知道其中到底是什么原因起作用。\nself.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n\n```\n\n## 策略网络和估值网络的不同\n\n从策略网络和估值网络看，强化学习都有探索和对抗两个过程。通过探索，得到数据集，包含了各种可能性。通过对抗，让数据集中好的（reward高的试验胜出）。他们的不同点在于对抗的方法。\n\n1. 策略网络：对抗的焦点在于选择Action的概率。\n2. 估值网络：对抗的焦点在于评估Action的reward。\n3. 两者都会影响数据集的制作，而且影响的方式是相同的：贪心和随机探索\n4. 比起策略网络，估值网络更加没有对抗的感觉。因为策略网络在提高某一个Action的概率的时候，会抑制其他Action的概率（总的概率为1）。而估值网络在提高某一个Action的reward的时候，和其他Action是无关的。**而且其实不能说是提高reward，它是预测出正确的reward。**（这是两者的很大的不同处，策略网络更像是一个分类问题，而估值网络像是一个回归问题）\n\n\n\n# 回顾\n\n强化学习的本质是连续决策。强化学习算法的关键是标签制作，数据集制作。\n\n连续决策问题是没有确定的标签的，它通过探索试验得到数据集和标签，为没有提供标签的问题，做了标签，让问题可以解决。\n\n深度学习无法优化离散的不可导的参数，强化学习也可以通过在离散的地方做数据集做标签，把它转换为可导的，可用sgd优化的问题。\n\n做标签和数据集的关键是随机性以及贪心，贪心让它立足于以往的知识，随机让它不刚愎自用，保持谦卑，给新的可能保留空间。\n\n\n\n无论是策略网络还是估值网络，在数据集的制作上都是一样的，随机性和贪心。估值网络的数据集制作，可以看出强化学习探索的本质（由于估值网络本身没有随机性，它在制作数据集的时候，显示的暴露了，探索和贪心的本质。策略网络这方面还不太好看出来，因为它是隐式的利用探索和贪心）。\n\n\n\n策略网络，增大所有行为的概率，但是对于reward大 的行为增大的权重大。这个思路在我得感觉上更加符合强化两个字。强化好的行为嘛。\n\n估值网络的本质是公式Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)，有点递归的感觉。\n\n策略网络和估值网络数据集也有一点不同，策略网络的数据集是纵向的，一串行为一起的。而估值网络的数据集是单个单个的。\n\n由下面代码可以看出，策略网络中，每一窜试验就会训练一次，只是网络参数更新会积累了好几次试验之后才更新。策略网络关心从开始到结束一系列行为。而估值网络只关心当前和下一状态。\n\n```python \nif done:\n    episode_number += 1\n    epx = np.vstack(xs)\n    epy = np.vstack(ys)\n    epr = np.vstack(drs)\n    tfp = tfps\n    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n\n    discounted_epr = discount_rewards(epr)\n    discounted_epr -= np.mean(discounted_epr)\n    discounted_epr //= np.std(discounted_epr)\n\n    tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n    for ix,grad in enumerate(tGrad):\n        gradBuffer[ix] += grad\n\n        if episode_number % batch_size == 0:\n            sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W1_1Grad:gradBuffer[1],W2Grad:gradBuffer[2]})\n```\n\n估值网络人为的控制随机强度，也是一个值得考虑的问题。\n\n\n\n","source":"_posts/First-Step-to-RL.md","raw":"---\ntitle: First Step to Reinforcement Learning\ndate: 2021-12-03 16:48:41\ndescription: 强化学习的入门介绍；什么是强化学习；深度强化学习算法：策略网络(Policy Network)和估值网络(Value Network)。\ntags:\n- Reinforcement Learning\ncategories:\n- Reinforcement Learning\n\n\n---\n\n\n\n# 什么是强化学习，关于强化学习的几点疑问\n\n<font color=green size=3>强化学习三要素：环境状态，行动，奖励</font>\n\n<font color=green size=3>目标：尽量多的获得奖励</font>\n\n<font color=green size=3>本质：连续决策</font>\n\n基本的强化学习模型包括：\n\n* 环境状态的集合S\n* 动作的集合A\n* 状态之间的转换规则（是环境的一部分）\n* 规定转换后“即时奖励”的规则（是环境的一部分）\n* 描述主体（智能体）能够观察到什么的规则（是环境的一部分）\n* 能够做出决策/动作的主体（智能体）\n\n## 区别于深度学习，强化学习的本质特点是什么？\n\n**两个定义**\n\n1，强化学习是机器学习的一个重要分支，主要用来解决连续决策问题。\n\n2，强化学习又称 再励学习，评价学习或增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在于环境交互过程中通过学习策略以达成回报最大或实现特定目标的问题。\n\n强化学习的本质是<font color=red> 描述和解决智能体（agent）在于环境交互过程中通过学习策略以达成回报最大或实现特定目标的问题。</font>它本质是这样一种场景，在这种场景中它为了达到某种目的，做出连续的决策。这样符合这种场景，那就是强化学习。\n\n强化学习和深度学习这两个名词的维度是不一样的。深度学习描述的是算法本身的特点，深度够不够，是不是连接主义的模型。深度学习可以用来做无监督，也可以用来做半监督，也可以用来做弱监督，甚至可以用来作为强化学习算法的一部分。深度学习这个名词，不管应用场景只管模型本身是不是满足深度学习的特点。\n\n而强化学习描述的是应用场景的特点，只要能提供智能体决策的算法，管它是什么模型，什么结构，那就是强化学习的算法。从这个角度上讲，我现在认为强化学习和监督学习，无监督学习，弱监督学习，是并列的，是对应用场景的描述。和深度学习不是同一维度的。\n\n<font color=red>强化学习的本质在于目标给定的形式，不像无监督学习那样完全没有学习目标，也不像监督学习那样有非常明确的目标（label），强化学习的目标一般是变化的，不明确的，甚至可能不存在绝对正确的目标。，强化学习的问题都可以抽象成，环境状态，行动和奖励。应该说只要能抽象为这三个要素，目标是获得最大奖励的模型，就是强化学习的模型。</font>\n\n强化学习的最大的特点是“试错”，是尝试各种可能，而强化结果好的可能。（策略网络的特点，估值网络的特点是修正和预测获益）\n\n由于强化学习是一种决策学习，这个问题的特点就是离散型。但是离散并不就是强化学习（连续的决策才是，目标的模糊和不确定性是决策问题的特点）。深度学习本质是函数的拟合，所以连续可微是它的特点。并不能说连续可微的问题就是深度学习，离散的问题就是强化学习。 \n\n## 深度学习用梯度下降的算法实现模型的优化，强化学习无法求导，甚至连学习目标都是模糊的如何优化模型参数？\n\n这个问题要了解下一节，具体的强化学习方法。\n\n## 强化学习与弱监督学习？\n\n弱监督是属于有监督学习，只是它学习的目标不是被给予的标签，而是比被给予标签更强的标签（强弱是指标签做含有的信息量），也就是说弱监督是根据少量信息的标签，推测出更多的信息。\n\n强化学习的本质是连续决策。连续决策的特点是目标的模糊和不确定性。\n\n所以，虽然弱监督和强化学习都没有给出最终准确的目标，但是他们任然很不同的\n\n## 深度学习是死的，没有智能的机器学习算法，强化学习是“活的”，有智能的机器学习算法\n\n深度学习大部分我们用作有监督的学习算法。其实说深度学习是死的，不如说有监督学习是死的。\n\n有监督学习其实是完全的复刻标签里面含有的知识，它的本质就是一个函数拟合的问题，它无法摆脱对绝对的标签的依赖，无法超越标签。\n\n而强化学习，正由于它的目标是模糊和不确定的。使得算法在设计上必须具有随机性和探索性，它能够探索出人类从来没有到过的领域。就像在围棋上，下出人类完全无法理解的棋，人从来没有想过的一些下法。这就是强化学习算法探索出来的知识。所以我觉得它是活着的，拥有智能的算法。\n\n<font color=green>从感性的层面，强化学习算法很接近人脑的行为：感知环境，探索环境，强化有益行为</font>\n\n## 关于深度学习，强化学习，连续可导性和离散不可导性的讨论\n\n1. 从函数的角度，深度学习和强化学习都需要学习一个函数映射。深度学习是从输入到target的映射。强化学习是学从环境状态到Action的映射。这两个映射可以看成性质一样的，因为深度学习可以作为强化学习的智能体。所以从函数的角度，他们没有连续和离散的区别。（PS. 深度学习模型和强化学习的智能体都是连续可导的函数。target和Action都可以是离散的或者连续的。）\n\n2. 深度学习作为用梯度下降算法优化的模型，无法优化对loss不可导的参数。如在深度学习中，设计一个分支来决定模型是否应该包含某个模块，这个分支的参数是不可优化的。因为包含和不包含是离散的，对loss不可导。\n\n   不过假设，如果包含与不包含是连续的。也就是说，可以以0.1的权重包含。那么，这些参数是可以优化的。从这点来看，离散就是导致模型参数不可优化的原因。\n\n3. 强化学习在离散的情况下解决2中的问题。由此，我得到了一个概念，强化学习解决离散的问题。\n\n4. 下面我们来分析一下这个场景。首先，决定一个模块该不该被使用，这个场景是一次Action的场景，不是连续决策的场景。也就是说Action一次，我就能知道最终reward多少了，只有单步的reward。\n\n   限于这个单次决策的场景来看，如果Action相对于reward是连续可导的，那么深度学习就能解决这个问题。如果Action相对于reward是离散的，那么仅仅深度学习无法解决这个问题，要靠强化学习。\n\n   <font color=green>这里单次Action就知道reward，实际上这个问题就退化为了有监督学习，因为这个单步的reward就可以看成我的标签了。所以深度学习解决这个问题是很自然的。</font>\n\n5. 多次决策的问题，无论是连续的还是离散的都只能用强化学习的方法。因为多次决策，这个问题就不可以退化为监督学习的问题了。它是一个真正的强化学习的问题。\n\n   \n\n综上所诉，强化学习解决深度学习解决不了的离散问题，那只是在单次决策的时候，这个问题退化为了有监督学习。强化学习的方法，恰好可以提供离散变量的学习。\n\n强化学习方法解决深度学习中的离散问题，仅仅是强化学习附带的一个小福利。\n\n**<font color=green>因为它能把经验转化为可导的目标，就拿策略网络来说，从梯度的角度，它只管增加当前随机Action的概率，而加入advatage之后，自动就优化除了想要的大reward的行为。</font>**\n\n## 深度学习优化和强化学习优化的感性理解\n\n前面说了深度学习优化可导的参数，强化学习可以优化不可导的参数。这里说一下对深度学习优化方法和强化学习优化方法的感性理解。\n\n还是说前面包不会包含某模块的例子，由于连续可导，对于每一个参数值，深度学习模型其实都同时参与了两种Action（包含和不包含）。score = 0.1 包含，其实其中包含了含有的成分，也包含了不含有的成分。所以我们可以连续的变动score，看看包含多好，还是不好含多好。这其实就是梯度下降算法的方式。得益于每一个参数，其实我都对包含和不包含的情况都有了解，我当然知道哪个更好，就往那边移动（优化）。\n\n然而，对于离散的情况，要么只能包含，要么只能不包含。当选择包含的时候，模型对不包含的情况完全是无知的。可能更好，也可能差。当不包含的时候，也是一样的。无论哪种情况，我都没有办法优化，两种情况是完全隔离开的，信息不沟通的，是离散的。所以梯度下降算法无法优化它。\n\n强化学习用随机探索的方法让两者信息又沟通起来。包含一下试一下，然后，不包含也试一下。尝试的结果是哪种reward多，就增大哪种的概率。\n\n**<font color=green>所以，无论哪种优化方法，信息的沟通都是必要的。要对所有的action都了解，才能知道选择哪种action。 只是深度学习是连续的，它的每一种参数，都包含了所有Action的信息（reward），每一种Action都参与了，所以它能直接连续的梯度下降的优化，不需要随机探索了。而对于离散的，每种action只能知道自己的reward，对其他Action一无所知的时候，梯度的优化是不行的。必须要探索各种Action，还是要知道了每一种Action的情况(reward)之后，才能优化。这是方法论。</font>**\n\n更进一步，离散的地方，相对于reward一定是不可导的，所以深度学习不行。而强化学习，更准确的说是策略网络，相当于给离散的地方加了标签，这样它就在离散的地方有监督了，它就可以根据增加的标签优化。而标签的设计就是根据探索的结果，增大reward大的Action，reward大的Action就是它的标签，而且这个标签是动态的，是对抗得出的。\n\n强化学习方法算出的梯度是策略梯度。\n\n**<font color=green>强化学习： 不知道选哪边了； 试试呗；按试出来reward大的Action优化它。</font>**\n\n**<font color=green>强化学习：它离散，对于reward不可导；不直接用reward优化它，给它加个标签，把试出来reward大的Action，作为标签去优化</font>**\n\n在不可导的地方加标签。\n\n**<font color=green>由此，强化学习算法的本质是制作标签，无论是连续决策，标签不确定的情况，还是它能解决离散问题的情况，它都是用制作标签的方法解决的。</font>**\n\n# 策略网络(Policy Network)和估值网络(Value Network)\n\nAlphaGo 使用了快速走子，策略网络，估值网络和蒙特卡洛搜索树等技术。\n\n强化学习算法的一个关键是<font color=green>随机性和探索性</font>，我们需要让算法通过试验样本自己学习什么才是某个环境状态下比较好的Action，而不是像有监督学习一样，告诉模型什么是好的Action，因为我们也不知道什么是好的Action.\n\n深度强化学习模型的本质是神经网络，神经网络是工具，根据问题转化以及建模的不同，主要分为策略网络和估值网络。\n\n强化学习中最重要的两类方法**Policy-based,Value-based**。第一种直接预测在某个环境下应该采取的行动（直接输出改采取Action的概率）。第二种预测在某个环境下所有行动的期望价值，然后通过选择q值最高的行动执行策略。\n\n他们都能完成决策，但由于建模的不同，估值网络包含有更多的信息，它不仅能提供决策，还预测了决策带来的收益。\n\n<font color=red>策略网络是隐式的学习了某一Action所带来的全部获益（当前获益+后续获益），而估值网络直接显示的学习Action所带来的全部获益。</font>强化学习算法做出最佳抉择只需要知道哪个Action全部获益最大，策略网络就是这样做的，估值网络不仅学习了哪个Action全部获益最大，还把每个Action的全部获益给计算出来了。\n\n<font color=green>相对来说，策略网络的性能会比估值网络好一些。</font>\n\n<font color=green>Value Based方法适合仅有少量Action的环境，而Policy Based方法更通用，适合Action种类非常多，或者具有连续取值的Action的环境。结合了深度学习之后，Policy Based方法就变成了策略网络，Value Based方法就变成了估值网络。</font>\n\n## 策略网络(Policy Network)\n\n直接看一个例子，学习的目标是，左右用力使得木棍不倒地，[Policy_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py) [Policy_Network.py](policy_network.py)\n\n关键代码\n\n ```python\n score = tf.matmul(layer1,W2)\n probability = tf.nn.sigmoid(score)#网络输出采取Action 1的概率。\n input_y = tf.placeholder(tf.float32,[None,1], \\\\\n                          name=\"input_y\")# 输入采取过的行为，这个行为是随机生成的。\n advantages = tf.placeholder(tf.float32,name=\"reward_signal\") \n # 输入获益\n loglik = tf.log(input_y*(input_y - probability) + \\\\\n                 (1 - input_y)*(input_y + probability)) \n # 损失函数，如果行为是1，则增大\n \n #概率，如果行为是 0，则减小概率（相当于也是增加0的概率），也就是说这个损\n #失函数无论当前行为是什么都会增大当前行为的概率。\n loss = -tf.reduce_mean(loglik * advantages) # 这行代码很关键，\n #相当于给损失函数成了一个权重advantages，得到最终的损失函数。\n #advantages是当前试验的全部获益。如果全部获益大，将以更大的权重，增加 \n #当前行为的概率。\n \n #所以，策略网络其实也是一个对抗学习的过程，增加所有采取过行为的概率，只是\n #获益多的行为以更大的权重增加。\n \n ## 一个试验： 由初始状态开始，随机采取一连串的行为\n #（Policy_Network.py 中是根据当前模型输出的概率,来生成随机的行为，但\n #是我感觉直接用0.5的概率随机生成一连串的0和1的行为也是可以的，下面将实\n #验一下），直到任务结束。\n \n ## 由于每个试验，都可以一直行为到任务结束，所以每个action，我们都可以得\n #到它在该试验中的全部获益（当前获益 + 之后所有行为的获益）\n \n ## 随机生成了n个试验，其中又各种各样的决策（随机探索），全部获益大的\n #action，它的advantages也大，那么它的概率就增大的多，它被强化的厉害。\n \n ## 试验生成的过程，实际上就是数据集构建的过程。策略网络的数据集是由环境\n #和一系列随机的行为构成的。它提供了环境在各种行为下的反应（获益）。模型\n #学习为环境带来高获益的行为的规律。\n ```\n\n由上面的代码可知，从策略网络的角度看强化学习的话，强化学习的关键其实是对数据集的构建---如何构建数据集。\n\n在构建数据集的时候，随机探索肯定是必要的。随机探索的结果会得到一系列好的行为，也会得到一系列不好的行为。如何强化好的行为就是算法设计的时候需要注意的。\n\n上面的代码在探索阶段借用了当前的模型，即根据当前模型输出的概率随机生成行为，从而形成数据集。如果完全的随机（一直使用0.5的概率随机的生成Action）会什么样呢？\n\n### 数据集是否可以和模型无关（不随着模型变化）？\n\n关键修改代码\n\n```python\n#基于当前模型，根据当前的状态x，生成Action 1的概率\ntfprob = sess.run(probability,feed_dict={observations: x})\n# 基于预测概率，随机生成行为，并试探环境。生成数据集。\naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n修改后\n\n```python\n# 注释掉这句，并不需要根据当前模型生成概率\n# tfprob = sess.run(probability,feed_dict={observations: x})\n# 直接设置概率为0.5，随机完全随机探索生成数据集。\ntfprob = 0.5\naction = 1 if np.random.uniform() < tfprob else 0\n```\n\n结果：修改后，模型无法收敛。\n\n完全随机很小的概率能探索出很好的试验，这些好的行为也很难持续的得到强化。\n\n所以，强化学习也有一种效果叠加的感觉。在完全随机的情况下 ，探索出相对好的action，再在这个相对好的action的基础上，在探索探索出更好的action。\n\n如果数据集不依赖模型，就是一直在完全随机的基础上探索。这样很难收敛。\n\n也可以这样看，完全随机的话，最多能学到前几步的策略（因为完全随机就走不了几步，探索的经验就只有那几步）。依赖于模型，探索的行为更有价值，因为是依赖于学到过的知识的，一方面确认了，按学到的知识走，确实获益多，一方面又在学到的知识的基础上，做了一些随机，探索更好的知识。\n\n## 估值网络(Value Network，Q-learning)\n\nQ-Learing用神经网络实现，得到的模型就是估值网络。\n\n也看一个例子，[Value_Network.py](https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py)\n\n学习每个Action所对应的reward的期望。\n\n我们先看看数据集的结构\n\n```python\n#Save the experience to our episode buffer.\nepisodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n# 其中s是当前时刻的环境状态，a是当前随机采取的Action，r是这个Action的当前reward\n# s1是采取Action之后的下一状态，d是布尔型表示是否任务结束。  \n```\n\n1. 现在目标是学习Q(s<sub>t</sub>, a<sub>t</sub>)，也就是当前环境状态，采取行为a的全部reward的期望。\n\n2. 现在假设我们有模型Q<sub>desird</sub>，可以预测全部reward了，那么这个模型应该满足条件，Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)，这有点递归的感觉了。\n\n3. 现在我们能不能根据这个公式 ， 来优化出Q<sub>desired</sub>。肯定是能的。对于探索过的所有试验，公式都满足的话，此时的模型就可以看成我们想要的模型了。我感觉这就是估值网络方法的核心。\n\n   \n\n直接看关键代码\n\n```python\n#Choose an action by greedily (with e chance of random action)\n# from the Q-network\nif np.random.rand(1) < e or total_steps < pre_train_steps:\n    a = np.random.randint(0,4)\nelse:\n    a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n# 这段代码其实是探索的代码，当最开始的时候是完全随机探索（total_steps < pre_train_steps的时候）\n# 当total_steps >= pre_train_steps之后呢，就不是完全随机探索了。\n# 有e的概率是随机探索的，（1-e）的概率是由训练好的模型决定之后的Action。 \n# 这里是估值网络和策略网络的不同，策略网络本身就具有随机性，所以不需要引入\n# 额外的参数e和pre_train_steps来控制随机探索的强度。\n# 策略网络在训练的过程中，本身就是由随机性大，到慢慢的收敛到好的Action\n# 所以可以直接得到好的探索的训练样本。估值网络没有这样好的性质，它连\n# 随机性都没有，就需要人为的制造，满足从完全随机探索，到在好的Action的\n# 基础上具有一定的随机性进行探索。\n```\n\n**这是策略网络和估值网络的共通之处，其实这也最上面那个注释\"by greedily (with e chance of random action) from the Q-network\"的意思。**\n\n**<font color=green>“贪心”两个字完美的诠释了强化学习，无论是策略网络还是估值网络，在探索阶段，在生成数据集上的特点。</font>**\n\n在策略网络那一节，我做的那个试验，和模型无关生成数据集。其实就是不贪心了，不贪心不行。\n\n```python\nif total_steps > pre_train_steps:\n    if e > endE:\n        e -= stepDrop\n# 完全随机了之后，开始慢慢减小随机性。\n# 模型约不可靠的时候，探索性和随机性越强。后来模型慢慢变得可靠就减弱随机性。\n# 因为模型越来越可靠的时候，随机性大就会得到很多远远低于当前模型性能的试验\n# 这些试验都是早就被pass了的，学不到什么东西，损坏模型的探索。\n# endE=0.1 说明无论训练的多好，模型都保持了随机性，保持了探索性\n#　人永远要有好奇心，永远要觉得自己的知识还可能不是最好的\n```\n\n\n\n下面的代码是将在数据集弄好的情况下，如何训练模型的。\n\n```python \nif total_steps % (update_freq) == 0:\n    trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n    #Below we perform the Double-DQN update to the target Q-values\n    # 主网络预测了下一刻需要采取的Action，trainBatch[:,3]是当前的下一刻的环境\n    # 回顾公式，Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # 这里主函数预测的Action就是t+1时刻（下一时刻）获益值最大的Action\n    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n    # target网络预测了下一时刻的reward\n    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n    end_multiplier = -(trainBatch[:,4] - 1)\n    # 用主网络预测出的Action以及target网络预测出的所有行为的reward\n    # 选择了最大的reward,也就是公式中的 Max Q(s_t+1, a)\n    doubleQ = Q2[range(batch_size),Q1]\n    # 这里得到的就是公式Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)\n    # 的右边，当前reward加上乘以衰减系数之后的，下一步最大reward\n    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n    #Update the network with our target values.\n    # 公式右边得到了之后，在把真正的当前状态输入进去，得到左边\n    # 左边以右边作为标签进行学习。更新主网络的参数\n    _ = sess.run(mainQN.updateModel, \\\n                 feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n    # 更新target网络的参数。\n    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n```\n\n\n\ntarget网络参数的更新方式代码\n\n```python \ndef updateTargetGraph(tfVars,tau):\n    total_vars = len(tfVars)\n    op_holder = [] \n    # 主网络是和target网络一样的，前一半参数正好是主网络的参数\n    for idx,var in enumerate(tfVars[0:total_vars//2]):\n        # idx+total_vars对应的时候后一半的参数也就是target网络的参数\n        # 这里相当于是target参数 = 主网络参数 * tau + （1- tau）*target参数\n        # 也就是说target网络在以一定的速度向主网络靠近\n        # 结合前面的代码，主网络才是真正学习的网络，target网络的作用仅仅是得到等式\n        # 右边的值，即标签，即使是等式右边，也不是target网络完全决定的\n        # target网络得到了所有Action的reward，最大的Action是主网络选择的\n        # 为什么要这么做，target网络也是在模仿主网络，只用主网络也能得到等式的右边\n        # 理论上其实右边也应该是主网络决定，现在搞了个主网络的模仿者target网络\n        # 是出于优化的考虑。我们后面叙述。\n        op_holder.append(tfVars[idx+total_vars//2].assign\\\\\n                         ((var.value()*tau) \\\\\n                      + ((1-tau)*tfVars[idx+total_vars//2].value())))\n        return op_holder\n\n    def updateTarget(op_holder,sess):\n        for op in op_holder:\n            sess.run(op)\n\n```\n\n现在其实我们把关键的代码都看了，在这段代码实现中，引入了一些state of the art的trick，下面我们结合看过的代码在提一遍。\n\n1. 引入卷积层，这段代码比较简单，我们没有看。环境状态是用图片的形式给的，用CNN提取特征是比较自然的。\n2. Experience replay。估值网络不像策略网络一样得到试验之后，用一次就扔掉再去制作新的试验（数据集）来训练。它把每次试验都放在一个试验池里面。试验池长度为N，如果超过了N，那就把最老的试验样本扔掉。每次 训练的时候从试验池里面随机选择batchsize个样本进行训练，保持了对样本的利用率，同时其实也增加了模型的稳定性，因为数据集是相对稳定的（相比于N=1而言，每次训练了就扔掉，进来的都是新的，不那么稳定）。\n3. 使用target网络来辅助训练。**之所以要用target网络来制造训练目标，用主网络来实际训练，是为了让Q-Learing训练的目标保持平稳。**强化学习不像普通的监督学习，它的目标是变化的，**因为学习目标的一部分就是模型本身输出的。**每次更新模型参数都会导致学习目标发生变化，如果更新频繁，幅度很大，我们的训练过程就会变得非常不稳定并且失控。**DQN的训练会陷入目标Q与预测Q的反馈循环中，震荡发散。**所以用target网络来制造目标，target网络和主网络又不是矛盾的，因为target网络会逼近主网络，它是主网络的模仿者，所以它提供的目标Q也是有权威的。\n4. Double DQN。这个trick源于target网络选的最大Action不准。模仿的不够好，现在就让主网络来帮它选。也就是上面代码中我们看过了主网络输出action，选择target网络输出的reward，得到公式的右边。\n5. Dueling DQN。上代码\n\n```python \nself.AW = tf.Variable(xavier_init([h_size//2,env.actions]))\nself.VW = tf.Variable(xavier_init([h_size//2,1]))\nself.Advantage = tf.matmul(self.streamA,self.AW)\nself.Value = tf.matmul(self.streamV,self.VW)\n\n#Then combine them together to get our final Q-values.\n# 看这里Qout是网络最终预测的所有Action的reward。它由两部分组成Value和Advantage\n# 由最上面两行可以看出Value是一维的，是实数，advantage是#action维度的向量\n# 所以，Dueling DQN就是将reward裁成了两部分，一部分是环境状态本身具有的价值Value\n# 另一部分就是Action本身具有的价值，相加起来就是在这个环境下Action具有的价值。\n# 其实我感觉这些解释都是人为的，具体是不是这样谁也不知道，可能只是这样优化的好。\n#　因为即使不分为两部分，网络输出Ｑout的时候，输入也是环境状态，肯定都会把环境考虑\n# 进去才有Action的价值。然而直接输出这个价值，发现优化的不好，分为两部分之后，发现\n# 优化的好了。其实谁也不知道其中到底是什么原因起作用。\nself.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n\n```\n\n## 策略网络和估值网络的不同\n\n从策略网络和估值网络看，强化学习都有探索和对抗两个过程。通过探索，得到数据集，包含了各种可能性。通过对抗，让数据集中好的（reward高的试验胜出）。他们的不同点在于对抗的方法。\n\n1. 策略网络：对抗的焦点在于选择Action的概率。\n2. 估值网络：对抗的焦点在于评估Action的reward。\n3. 两者都会影响数据集的制作，而且影响的方式是相同的：贪心和随机探索\n4. 比起策略网络，估值网络更加没有对抗的感觉。因为策略网络在提高某一个Action的概率的时候，会抑制其他Action的概率（总的概率为1）。而估值网络在提高某一个Action的reward的时候，和其他Action是无关的。**而且其实不能说是提高reward，它是预测出正确的reward。**（这是两者的很大的不同处，策略网络更像是一个分类问题，而估值网络像是一个回归问题）\n\n\n\n# 回顾\n\n强化学习的本质是连续决策。强化学习算法的关键是标签制作，数据集制作。\n\n连续决策问题是没有确定的标签的，它通过探索试验得到数据集和标签，为没有提供标签的问题，做了标签，让问题可以解决。\n\n深度学习无法优化离散的不可导的参数，强化学习也可以通过在离散的地方做数据集做标签，把它转换为可导的，可用sgd优化的问题。\n\n做标签和数据集的关键是随机性以及贪心，贪心让它立足于以往的知识，随机让它不刚愎自用，保持谦卑，给新的可能保留空间。\n\n\n\n无论是策略网络还是估值网络，在数据集的制作上都是一样的，随机性和贪心。估值网络的数据集制作，可以看出强化学习探索的本质（由于估值网络本身没有随机性，它在制作数据集的时候，显示的暴露了，探索和贪心的本质。策略网络这方面还不太好看出来，因为它是隐式的利用探索和贪心）。\n\n\n\n策略网络，增大所有行为的概率，但是对于reward大 的行为增大的权重大。这个思路在我得感觉上更加符合强化两个字。强化好的行为嘛。\n\n估值网络的本质是公式Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)，有点递归的感觉。\n\n策略网络和估值网络数据集也有一点不同，策略网络的数据集是纵向的，一串行为一起的。而估值网络的数据集是单个单个的。\n\n由下面代码可以看出，策略网络中，每一窜试验就会训练一次，只是网络参数更新会积累了好几次试验之后才更新。策略网络关心从开始到结束一系列行为。而估值网络只关心当前和下一状态。\n\n```python \nif done:\n    episode_number += 1\n    epx = np.vstack(xs)\n    epy = np.vstack(ys)\n    epr = np.vstack(drs)\n    tfp = tfps\n    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n\n    discounted_epr = discount_rewards(epr)\n    discounted_epr -= np.mean(discounted_epr)\n    discounted_epr //= np.std(discounted_epr)\n\n    tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n    for ix,grad in enumerate(tGrad):\n        gradBuffer[ix] += grad\n\n        if episode_number % batch_size == 0:\n            sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W1_1Grad:gradBuffer[1],W2Grad:gradBuffer[2]})\n```\n\n估值网络人为的控制随机强度，也是一个值得考虑的问题。\n\n\n\n","slug":"First-Step-to-RL","published":1,"updated":"2021-12-16T02:39:14.208Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f660009vgul7dlkdslm","content":"<html><head></head><body><h1 id=\"什么是强化学习，关于强化学习的几点疑问\"><span class=\"post-title-index\">1. </span><a href=\"#什么是强化学习，关于强化学习的几点疑问\" class=\"headerlink\" title=\"什么是强化学习，关于强化学习的几点疑问\"></a>什么是强化学习，关于强化学习的几点疑问</h1><p><font color=\"green\" size=\"3\">强化学习三要素：环境状态，行动，奖励</font></p>\n<p><font color=\"green\" size=\"3\">目标：尽量多的获得奖励</font></p>\n<p><font color=\"green\" size=\"3\">本质：连续决策</font></p>\n<p>基本的强化学习模型包括：</p>\n<ul>\n<li>环境状态的集合S</li>\n<li>动作的集合A</li>\n<li>状态之间的转换规则（是环境的一部分）</li>\n<li>规定转换后“即时奖励”的规则（是环境的一部分）</li>\n<li>描述主体（智能体）能够观察到什么的规则（是环境的一部分）</li>\n<li>能够做出决策/动作的主体（智能体）</li>\n</ul>\n<h2 id=\"区别于深度学习，强化学习的本质特点是什么？\"><span class=\"post-title-index\">1.1. </span><a href=\"#区别于深度学习，强化学习的本质特点是什么？\" class=\"headerlink\" title=\"区别于深度学习，强化学习的本质特点是什么？\"></a>区别于深度学习，强化学习的本质特点是什么？</h2><p><strong>两个定义</strong></p>\n<p>1，强化学习是机器学习的一个重要分支，主要用来解决连续决策问题。</p>\n<p>2，强化学习又称 再励学习，评价学习或增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在于环境交互过程中通过学习策略以达成回报最大或实现特定目标的问题。</p>\n<p>强化学习的本质是<font color=\"red\"> 描述和解决智能体（agent）在于环境交互过程中通过学习策略以达成回报最大或实现特定目标的问题。</font>它本质是这样一种场景，在这种场景中它为了达到某种目的，做出连续的决策。这样符合这种场景，那就是强化学习。</p>\n<p>强化学习和深度学习这两个名词的维度是不一样的。深度学习描述的是算法本身的特点，深度够不够，是不是连接主义的模型。深度学习可以用来做无监督，也可以用来做半监督，也可以用来做弱监督，甚至可以用来作为强化学习算法的一部分。深度学习这个名词，不管应用场景只管模型本身是不是满足深度学习的特点。</p>\n<p>而强化学习描述的是应用场景的特点，只要能提供智能体决策的算法，管它是什么模型，什么结构，那就是强化学习的算法。从这个角度上讲，我现在认为强化学习和监督学习，无监督学习，弱监督学习，是并列的，是对应用场景的描述。和深度学习不是同一维度的。</p>\n<p><font color=\"red\">强化学习的本质在于目标给定的形式，不像无监督学习那样完全没有学习目标，也不像监督学习那样有非常明确的目标（label），强化学习的目标一般是变化的，不明确的，甚至可能不存在绝对正确的目标。，强化学习的问题都可以抽象成，环境状态，行动和奖励。应该说只要能抽象为这三个要素，目标是获得最大奖励的模型，就是强化学习的模型。</font></p>\n<p>强化学习的最大的特点是“试错”，是尝试各种可能，而强化结果好的可能。（策略网络的特点，估值网络的特点是修正和预测获益）</p>\n<p>由于强化学习是一种决策学习，这个问题的特点就是离散型。但是离散并不就是强化学习（连续的决策才是，目标的模糊和不确定性是决策问题的特点）。深度学习本质是函数的拟合，所以连续可微是它的特点。并不能说连续可微的问题就是深度学习，离散的问题就是强化学习。 </p>\n<h2 id=\"深度学习用梯度下降的算法实现模型的优化，强化学习无法求导，甚至连学习目标都是模糊的如何优化模型参数？\"><span class=\"post-title-index\">1.2. </span><a href=\"#深度学习用梯度下降的算法实现模型的优化，强化学习无法求导，甚至连学习目标都是模糊的如何优化模型参数？\" class=\"headerlink\" title=\"深度学习用梯度下降的算法实现模型的优化，强化学习无法求导，甚至连学习目标都是模糊的如何优化模型参数？\"></a>深度学习用梯度下降的算法实现模型的优化，强化学习无法求导，甚至连学习目标都是模糊的如何优化模型参数？</h2><p>这个问题要了解下一节，具体的强化学习方法。</p>\n<h2 id=\"强化学习与弱监督学习？\"><span class=\"post-title-index\">1.3. </span><a href=\"#强化学习与弱监督学习？\" class=\"headerlink\" title=\"强化学习与弱监督学习？\"></a>强化学习与弱监督学习？</h2><p>弱监督是属于有监督学习，只是它学习的目标不是被给予的标签，而是比被给予标签更强的标签（强弱是指标签做含有的信息量），也就是说弱监督是根据少量信息的标签，推测出更多的信息。</p>\n<p>强化学习的本质是连续决策。连续决策的特点是目标的模糊和不确定性。</p>\n<p>所以，虽然弱监督和强化学习都没有给出最终准确的目标，但是他们任然很不同的</p>\n<h2 id=\"深度学习是死的，没有智能的机器学习算法，强化学习是“活的”，有智能的机器学习算法\"><span class=\"post-title-index\">1.4. </span><a href=\"#深度学习是死的，没有智能的机器学习算法，强化学习是“活的”，有智能的机器学习算法\" class=\"headerlink\" title=\"深度学习是死的，没有智能的机器学习算法，强化学习是“活的”，有智能的机器学习算法\"></a>深度学习是死的，没有智能的机器学习算法，强化学习是“活的”，有智能的机器学习算法</h2><p>深度学习大部分我们用作有监督的学习算法。其实说深度学习是死的，不如说有监督学习是死的。</p>\n<p>有监督学习其实是完全的复刻标签里面含有的知识，它的本质就是一个函数拟合的问题，它无法摆脱对绝对的标签的依赖，无法超越标签。</p>\n<p>而强化学习，正由于它的目标是模糊和不确定的。使得算法在设计上必须具有随机性和探索性，它能够探索出人类从来没有到过的领域。就像在围棋上，下出人类完全无法理解的棋，人从来没有想过的一些下法。这就是强化学习算法探索出来的知识。所以我觉得它是活着的，拥有智能的算法。</p>\n<p><font color=\"green\">从感性的层面，强化学习算法很接近人脑的行为：感知环境，探索环境，强化有益行为</font></p>\n<h2 id=\"关于深度学习，强化学习，连续可导性和离散不可导性的讨论\"><span class=\"post-title-index\">1.5. </span><a href=\"#关于深度学习，强化学习，连续可导性和离散不可导性的讨论\" class=\"headerlink\" title=\"关于深度学习，强化学习，连续可导性和离散不可导性的讨论\"></a>关于深度学习，强化学习，连续可导性和离散不可导性的讨论</h2><ol>\n<li><p>从函数的角度，深度学习和强化学习都需要学习一个函数映射。深度学习是从输入到target的映射。强化学习是学从环境状态到Action的映射。这两个映射可以看成性质一样的，因为深度学习可以作为强化学习的智能体。所以从函数的角度，他们没有连续和离散的区别。（PS. 深度学习模型和强化学习的智能体都是连续可导的函数。target和Action都可以是离散的或者连续的。）</p>\n</li>\n<li><p>深度学习作为用梯度下降算法优化的模型，无法优化对loss不可导的参数。如在深度学习中，设计一个分支来决定模型是否应该包含某个模块，这个分支的参数是不可优化的。因为包含和不包含是离散的，对loss不可导。</p>\n<p>不过假设，如果包含与不包含是连续的。也就是说，可以以0.1的权重包含。那么，这些参数是可以优化的。从这点来看，离散就是导致模型参数不可优化的原因。</p>\n</li>\n<li><p>强化学习在离散的情况下解决2中的问题。由此，我得到了一个概念，强化学习解决离散的问题。</p>\n</li>\n<li><p>下面我们来分析一下这个场景。首先，决定一个模块该不该被使用，这个场景是一次Action的场景，不是连续决策的场景。也就是说Action一次，我就能知道最终reward多少了，只有单步的reward。</p>\n<p>限于这个单次决策的场景来看，如果Action相对于reward是连续可导的，那么深度学习就能解决这个问题。如果Action相对于reward是离散的，那么仅仅深度学习无法解决这个问题，要靠强化学习。</p>\n<p><font color=\"green\">这里单次Action就知道reward，实际上这个问题就退化为了有监督学习，因为这个单步的reward就可以看成我的标签了。所以深度学习解决这个问题是很自然的。</font></p>\n</li>\n<li><p>多次决策的问题，无论是连续的还是离散的都只能用强化学习的方法。因为多次决策，这个问题就不可以退化为监督学习的问题了。它是一个真正的强化学习的问题。</p>\n</li>\n</ol>\n<p>综上所诉，强化学习解决深度学习解决不了的离散问题，那只是在单次决策的时候，这个问题退化为了有监督学习。强化学习的方法，恰好可以提供离散变量的学习。</p>\n<p>强化学习方法解决深度学习中的离散问题，仅仅是强化学习附带的一个小福利。</p>\n<p><strong><font color=\"green\">因为它能把经验转化为可导的目标，就拿策略网络来说，从梯度的角度，它只管增加当前随机Action的概率，而加入advatage之后，自动就优化除了想要的大reward的行为。</font></strong></p>\n<h2 id=\"深度学习优化和强化学习优化的感性理解\"><span class=\"post-title-index\">1.6. </span><a href=\"#深度学习优化和强化学习优化的感性理解\" class=\"headerlink\" title=\"深度学习优化和强化学习优化的感性理解\"></a>深度学习优化和强化学习优化的感性理解</h2><p>前面说了深度学习优化可导的参数，强化学习可以优化不可导的参数。这里说一下对深度学习优化方法和强化学习优化方法的感性理解。</p>\n<p>还是说前面包不会包含某模块的例子，由于连续可导，对于每一个参数值，深度学习模型其实都同时参与了两种Action（包含和不包含）。score = 0.1 包含，其实其中包含了含有的成分，也包含了不含有的成分。所以我们可以连续的变动score，看看包含多好，还是不好含多好。这其实就是梯度下降算法的方式。得益于每一个参数，其实我都对包含和不包含的情况都有了解，我当然知道哪个更好，就往那边移动（优化）。</p>\n<p>然而，对于离散的情况，要么只能包含，要么只能不包含。当选择包含的时候，模型对不包含的情况完全是无知的。可能更好，也可能差。当不包含的时候，也是一样的。无论哪种情况，我都没有办法优化，两种情况是完全隔离开的，信息不沟通的，是离散的。所以梯度下降算法无法优化它。</p>\n<p>强化学习用随机探索的方法让两者信息又沟通起来。包含一下试一下，然后，不包含也试一下。尝试的结果是哪种reward多，就增大哪种的概率。</p>\n<p><strong><font color=\"green\">所以，无论哪种优化方法，信息的沟通都是必要的。要对所有的action都了解，才能知道选择哪种action。 只是深度学习是连续的，它的每一种参数，都包含了所有Action的信息（reward），每一种Action都参与了，所以它能直接连续的梯度下降的优化，不需要随机探索了。而对于离散的，每种action只能知道自己的reward，对其他Action一无所知的时候，梯度的优化是不行的。必须要探索各种Action，还是要知道了每一种Action的情况(reward)之后，才能优化。这是方法论。</font></strong></p>\n<p>更进一步，离散的地方，相对于reward一定是不可导的，所以深度学习不行。而强化学习，更准确的说是策略网络，相当于给离散的地方加了标签，这样它就在离散的地方有监督了，它就可以根据增加的标签优化。而标签的设计就是根据探索的结果，增大reward大的Action，reward大的Action就是它的标签，而且这个标签是动态的，是对抗得出的。</p>\n<p>强化学习方法算出的梯度是策略梯度。</p>\n<p><strong><font color=\"green\">强化学习： 不知道选哪边了； 试试呗；按试出来reward大的Action优化它。</font></strong></p>\n<p><strong><font color=\"green\">强化学习：它离散，对于reward不可导；不直接用reward优化它，给它加个标签，把试出来reward大的Action，作为标签去优化</font></strong></p>\n<p>在不可导的地方加标签。</p>\n<p><strong><font color=\"green\">由此，强化学习算法的本质是制作标签，无论是连续决策，标签不确定的情况，还是它能解决离散问题的情况，它都是用制作标签的方法解决的。</font></strong></p>\n<h1 id=\"策略网络-Policy-Network-和估值网络-Value-Network\"><span class=\"post-title-index\">2. </span><a href=\"#策略网络-Policy-Network-和估值网络-Value-Network\" class=\"headerlink\" title=\"策略网络(Policy Network)和估值网络(Value Network)\"></a>策略网络(Policy Network)和估值网络(Value Network)</h1><p>AlphaGo 使用了快速走子，策略网络，估值网络和蒙特卡洛搜索树等技术。</p>\n<p>强化学习算法的一个关键是<font color=\"green\">随机性和探索性</font>，我们需要让算法通过试验样本自己学习什么才是某个环境状态下比较好的Action，而不是像有监督学习一样，告诉模型什么是好的Action，因为我们也不知道什么是好的Action.</p>\n<p>深度强化学习模型的本质是神经网络，神经网络是工具，根据问题转化以及建模的不同，主要分为策略网络和估值网络。</p>\n<p>强化学习中最重要的两类方法<strong>Policy-based,Value-based</strong>。第一种直接预测在某个环境下应该采取的行动（直接输出改采取Action的概率）。第二种预测在某个环境下所有行动的期望价值，然后通过选择q值最高的行动执行策略。</p>\n<p>他们都能完成决策，但由于建模的不同，估值网络包含有更多的信息，它不仅能提供决策，还预测了决策带来的收益。</p>\n<p><font color=\"red\">策略网络是隐式的学习了某一Action所带来的全部获益（当前获益+后续获益），而估值网络直接显示的学习Action所带来的全部获益。</font>强化学习算法做出最佳抉择只需要知道哪个Action全部获益最大，策略网络就是这样做的，估值网络不仅学习了哪个Action全部获益最大，还把每个Action的全部获益给计算出来了。</p>\n<p><font color=\"green\">相对来说，策略网络的性能会比估值网络好一些。</font></p>\n<p><font color=\"green\">Value Based方法适合仅有少量Action的环境，而Policy Based方法更通用，适合Action种类非常多，或者具有连续取值的Action的环境。结合了深度学习之后，Policy Based方法就变成了策略网络，Value Based方法就变成了估值网络。</font></p>\n<h2 id=\"策略网络-Policy-Network\"><span class=\"post-title-index\">2.1. </span><a href=\"#策略网络-Policy-Network\" class=\"headerlink\" title=\"策略网络(Policy Network)\"></a>策略网络(Policy Network)</h2><p>直接看一个例子，学习的目标是，左右用力使得木棍不倒地，<a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Policy_Network.py</a> <a href=\"policy_network.py\">Policy_Network.py</a></p>\n<p>关键代码</p>\n <figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">score = tf.matmul(layer1,W2)</span><br><span class=\"line\">probability = tf.nn.sigmoid(score)<span class=\"comment\">#网络输出采取Action 1的概率。</span></span><br><span class=\"line\">input_y = tf.placeholder(tf.float32,[<span class=\"literal\">None</span>,<span class=\"number\">1</span>], \\\\</span><br><span class=\"line\">                         name=<span class=\"string\">\"input_y\"</span>)<span class=\"comment\"># 输入采取过的行为，这个行为是随机生成的。</span></span><br><span class=\"line\">advantages = tf.placeholder(tf.float32,name=<span class=\"string\">\"reward_signal\"</span>) </span><br><span class=\"line\"><span class=\"comment\"># 输入获益</span></span><br><span class=\"line\">loglik = tf.log(input_y*(input_y - probability) + \\\\</span><br><span class=\"line\">                (<span class=\"number\">1</span> - input_y)*(input_y + probability)) </span><br><span class=\"line\"><span class=\"comment\"># 损失函数，如果行为是1，则增大</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#概率，如果行为是 0，则减小概率（相当于也是增加0的概率），也就是说这个损</span></span><br><span class=\"line\"><span class=\"comment\">#失函数无论当前行为是什么都会增大当前行为的概率。</span></span><br><span class=\"line\">loss = -tf.reduce_mean(loglik * advantages) <span class=\"comment\"># 这行代码很关键，</span></span><br><span class=\"line\"><span class=\"comment\">#相当于给损失函数成了一个权重advantages，得到最终的损失函数。</span></span><br><span class=\"line\"><span class=\"comment\">#advantages是当前试验的全部获益。如果全部获益大，将以更大的权重，增加 </span></span><br><span class=\"line\"><span class=\"comment\">#当前行为的概率。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#所以，策略网络其实也是一个对抗学习的过程，增加所有采取过行为的概率，只是</span></span><br><span class=\"line\"><span class=\"comment\">#获益多的行为以更大的权重增加。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 一个试验： 由初始状态开始，随机采取一连串的行为</span></span><br><span class=\"line\"><span class=\"comment\">#（Policy_Network.py 中是根据当前模型输出的概率,来生成随机的行为，但</span></span><br><span class=\"line\"><span class=\"comment\">#是我感觉直接用0.5的概率随机生成一连串的0和1的行为也是可以的，下面将实</span></span><br><span class=\"line\"><span class=\"comment\">#验一下），直到任务结束。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 由于每个试验，都可以一直行为到任务结束，所以每个action，我们都可以得</span></span><br><span class=\"line\"><span class=\"comment\">#到它在该试验中的全部获益（当前获益 + 之后所有行为的获益）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 随机生成了n个试验，其中又各种各样的决策（随机探索），全部获益大的</span></span><br><span class=\"line\"><span class=\"comment\">#action，它的advantages也大，那么它的概率就增大的多，它被强化的厉害。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 试验生成的过程，实际上就是数据集构建的过程。策略网络的数据集是由环境</span></span><br><span class=\"line\"><span class=\"comment\">#和一系列随机的行为构成的。它提供了环境在各种行为下的反应（获益）。模型</span></span><br><span class=\"line\"><span class=\"comment\">#学习为环境带来高获益的行为的规律。</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p>由上面的代码可知，从策略网络的角度看强化学习的话，强化学习的关键其实是对数据集的构建—如何构建数据集。</p>\n<p>在构建数据集的时候，随机探索肯定是必要的。随机探索的结果会得到一系列好的行为，也会得到一系列不好的行为。如何强化好的行为就是算法设计的时候需要注意的。</p>\n<p>上面的代码在探索阶段借用了当前的模型，即根据当前模型输出的概率随机生成行为，从而形成数据集。如果完全的随机（一直使用0.5的概率随机的生成Action）会什么样呢？</p>\n<h3 id=\"数据集是否可以和模型无关（不随着模型变化）？\"><span class=\"post-title-index\">2.1.1. </span><a href=\"#数据集是否可以和模型无关（不随着模型变化）？\" class=\"headerlink\" title=\"数据集是否可以和模型无关（不随着模型变化）？\"></a>数据集是否可以和模型无关（不随着模型变化）？</h3><p>关键修改代码</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#基于当前模型，根据当前的状态x，生成Action 1的概率</span></span><br><span class=\"line\">tfprob = sess.run(probability,feed_dict={observations: x})</span><br><span class=\"line\"><span class=\"comment\"># 基于预测概率，随机生成行为，并试探环境。生成数据集。</span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p>修改后</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 注释掉这句，并不需要根据当前模型生成概率</span></span><br><span class=\"line\"><span class=\"comment\"># tfprob = sess.run(probability,feed_dict={observations: x})</span></span><br><span class=\"line\"><span class=\"comment\"># 直接设置概率为0.5，随机完全随机探索生成数据集。</span></span><br><span class=\"line\">tfprob = <span class=\"number\">0.5</span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p>结果：修改后，模型无法收敛。</p>\n<p>完全随机很小的概率能探索出很好的试验，这些好的行为也很难持续的得到强化。</p>\n<p>所以，强化学习也有一种效果叠加的感觉。在完全随机的情况下 ，探索出相对好的action，再在这个相对好的action的基础上，在探索探索出更好的action。</p>\n<p>如果数据集不依赖模型，就是一直在完全随机的基础上探索。这样很难收敛。</p>\n<p>也可以这样看，完全随机的话，最多能学到前几步的策略（因为完全随机就走不了几步，探索的经验就只有那几步）。依赖于模型，探索的行为更有价值，因为是依赖于学到过的知识的，一方面确认了，按学到的知识走，确实获益多，一方面又在学到的知识的基础上，做了一些随机，探索更好的知识。</p>\n<h2 id=\"估值网络-Value-Network，Q-learning\"><span class=\"post-title-index\">2.2. </span><a href=\"#估值网络-Value-Network，Q-learning\" class=\"headerlink\" title=\"估值网络(Value Network，Q-learning)\"></a>估值网络(Value Network，Q-learning)</h2><p>Q-Learing用神经网络实现，得到的模型就是估值网络。</p>\n<p>也看一个例子，<a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Value_Network.py</a></p>\n<p>学习每个Action所对应的reward的期望。</p>\n<p>我们先看看数据集的结构</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Save the experience to our episode buffer.</span></span><br><span class=\"line\">episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[<span class=\"number\">1</span>,<span class=\"number\">5</span>]))</span><br><span class=\"line\"><span class=\"comment\"># 其中s是当前时刻的环境状态，a是当前随机采取的Action，r是这个Action的当前reward</span></span><br><span class=\"line\"><span class=\"comment\"># s1是采取Action之后的下一状态，d是布尔型表示是否任务结束。  </span></span><br></pre></td></tr></tbody></table></figure>\n\n<ol>\n<li><p>现在目标是学习Q(s<sub>t</sub>, a<sub>t</sub>)，也就是当前环境状态，采取行为a的全部reward的期望。</p>\n</li>\n<li><p>现在假设我们有模型Q<sub>desird</sub>，可以预测全部reward了，那么这个模型应该满足条件，Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)，这有点递归的感觉了。</p>\n</li>\n<li><p>现在我们能不能根据这个公式 ， 来优化出Q<sub>desired</sub>。肯定是能的。对于探索过的所有试验，公式都满足的话，此时的模型就可以看成我们想要的模型了。我感觉这就是估值网络方法的核心。</p>\n</li>\n</ol>\n<p>直接看关键代码</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Choose an action by greedily (with e chance of random action)</span></span><br><span class=\"line\"><span class=\"comment\"># from the Q-network</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> np.random.rand(<span class=\"number\">1</span>) &lt; e <span class=\"keyword\">or</span> total_steps &lt; pre_train_steps:</span><br><span class=\"line\">    a = np.random.randint(<span class=\"number\">0</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># 这段代码其实是探索的代码，当最开始的时候是完全随机探索（total_steps &lt; pre_train_steps的时候）</span></span><br><span class=\"line\"><span class=\"comment\"># 当total_steps &gt;= pre_train_steps之后呢，就不是完全随机探索了。</span></span><br><span class=\"line\"><span class=\"comment\"># 有e的概率是随机探索的，（1-e）的概率是由训练好的模型决定之后的Action。 </span></span><br><span class=\"line\"><span class=\"comment\"># 这里是估值网络和策略网络的不同，策略网络本身就具有随机性，所以不需要引入</span></span><br><span class=\"line\"><span class=\"comment\"># 额外的参数e和pre_train_steps来控制随机探索的强度。</span></span><br><span class=\"line\"><span class=\"comment\"># 策略网络在训练的过程中，本身就是由随机性大，到慢慢的收敛到好的Action</span></span><br><span class=\"line\"><span class=\"comment\"># 所以可以直接得到好的探索的训练样本。估值网络没有这样好的性质，它连</span></span><br><span class=\"line\"><span class=\"comment\"># 随机性都没有，就需要人为的制造，满足从完全随机探索，到在好的Action的</span></span><br><span class=\"line\"><span class=\"comment\"># 基础上具有一定的随机性进行探索。</span></span><br></pre></td></tr></tbody></table></figure>\n\n<p><strong>这是策略网络和估值网络的共通之处，其实这也最上面那个注释”by greedily (with e chance of random action) from the Q-network”的意思。</strong></p>\n<p><strong><font color=\"green\">“贪心”两个字完美的诠释了强化学习，无论是策略网络还是估值网络，在探索阶段，在生成数据集上的特点。</font></strong></p>\n<p>在策略网络那一节，我做的那个试验，和模型无关生成数据集。其实就是不贪心了，不贪心不行。</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps &gt; pre_train_steps:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> e &gt; endE:</span><br><span class=\"line\">        e -= stepDrop</span><br><span class=\"line\"><span class=\"comment\"># 完全随机了之后，开始慢慢减小随机性。</span></span><br><span class=\"line\"><span class=\"comment\"># 模型约不可靠的时候，探索性和随机性越强。后来模型慢慢变得可靠就减弱随机性。</span></span><br><span class=\"line\"><span class=\"comment\"># 因为模型越来越可靠的时候，随机性大就会得到很多远远低于当前模型性能的试验</span></span><br><span class=\"line\"><span class=\"comment\"># 这些试验都是早就被pass了的，学不到什么东西，损坏模型的探索。</span></span><br><span class=\"line\"><span class=\"comment\"># endE=0.1 说明无论训练的多好，模型都保持了随机性，保持了探索性</span></span><br><span class=\"line\"><span class=\"comment\">#　人永远要有好奇心，永远要觉得自己的知识还可能不是最好的</span></span><br></pre></td></tr></tbody></table></figure>\n\n\n\n<p>下面的代码是将在数据集弄好的情况下，如何训练模型的。</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps % (update_freq) == <span class=\"number\">0</span>:</span><br><span class=\"line\">    trainBatch = myBuffer.sample(batch_size) <span class=\"comment\">#Get a random batch of experiences.</span></span><br><span class=\"line\">    <span class=\"comment\">#Below we perform the Double-DQN update to the target Q-values</span></span><br><span class=\"line\">    <span class=\"comment\"># 主网络预测了下一刻需要采取的Action，trainBatch[:,3]是当前的下一刻的环境</span></span><br><span class=\"line\">    <span class=\"comment\"># 回顾公式，Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># 这里主函数预测的Action就是t+1时刻（下一时刻）获益值最大的Action</span></span><br><span class=\"line\">    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])})</span><br><span class=\"line\">    <span class=\"comment\"># target网络预测了下一时刻的reward</span></span><br><span class=\"line\">    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])})</span><br><span class=\"line\">    end_multiplier = -(trainBatch[:,<span class=\"number\">4</span>] - <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 用主网络预测出的Action以及target网络预测出的所有行为的reward</span></span><br><span class=\"line\">    <span class=\"comment\"># 选择了最大的reward,也就是公式中的 Max Q(s_t+1, a)</span></span><br><span class=\"line\">    doubleQ = Q2[<span class=\"built_in\">range</span>(batch_size),Q1]</span><br><span class=\"line\">    <span class=\"comment\"># 这里得到的就是公式Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># 的右边，当前reward加上乘以衰减系数之后的，下一步最大reward</span></span><br><span class=\"line\">    targetQ = trainBatch[:,<span class=\"number\">2</span>] + (y*doubleQ * end_multiplier)</span><br><span class=\"line\">    <span class=\"comment\">#Update the network with our target values.</span></span><br><span class=\"line\">    <span class=\"comment\"># 公式右边得到了之后，在把真正的当前状态输入进去，得到左边</span></span><br><span class=\"line\">    <span class=\"comment\"># 左边以右边作为标签进行学习。更新主网络的参数</span></span><br><span class=\"line\">    _ = sess.run(mainQN.updateModel, \\</span><br><span class=\"line\">                 feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">0</span>]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,<span class=\"number\">1</span>]})</span><br><span class=\"line\">    <span class=\"comment\"># 更新target网络的参数。</span></span><br><span class=\"line\">    updateTarget(targetOps,sess) <span class=\"comment\">#Update the target network toward the primary network.</span></span><br></pre></td></tr></tbody></table></figure>\n\n\n\n<p>target网络参数的更新方式代码</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTargetGraph</span>(<span class=\"params\">tfVars,tau</span>):</span></span><br><span class=\"line\">    total_vars = <span class=\"built_in\">len</span>(tfVars)</span><br><span class=\"line\">    op_holder = [] </span><br><span class=\"line\">    <span class=\"comment\"># 主网络是和target网络一样的，前一半参数正好是主网络的参数</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx,var <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tfVars[<span class=\"number\">0</span>:total_vars//<span class=\"number\">2</span>]):</span><br><span class=\"line\">        <span class=\"comment\"># idx+total_vars对应的时候后一半的参数也就是target网络的参数</span></span><br><span class=\"line\">        <span class=\"comment\"># 这里相当于是target参数 = 主网络参数 * tau + （1- tau）*target参数</span></span><br><span class=\"line\">        <span class=\"comment\"># 也就是说target网络在以一定的速度向主网络靠近</span></span><br><span class=\"line\">        <span class=\"comment\"># 结合前面的代码，主网络才是真正学习的网络，target网络的作用仅仅是得到等式</span></span><br><span class=\"line\">        <span class=\"comment\"># 右边的值，即标签，即使是等式右边，也不是target网络完全决定的</span></span><br><span class=\"line\">        <span class=\"comment\"># target网络得到了所有Action的reward，最大的Action是主网络选择的</span></span><br><span class=\"line\">        <span class=\"comment\"># 为什么要这么做，target网络也是在模仿主网络，只用主网络也能得到等式的右边</span></span><br><span class=\"line\">        <span class=\"comment\"># 理论上其实右边也应该是主网络决定，现在搞了个主网络的模仿者target网络</span></span><br><span class=\"line\">        <span class=\"comment\"># 是出于优化的考虑。我们后面叙述。</span></span><br><span class=\"line\">        op_holder.append(tfVars[idx+total_vars//<span class=\"number\">2</span>].assign\\\\</span><br><span class=\"line\">                         ((var.value()*tau) \\\\</span><br><span class=\"line\">                      + ((<span class=\"number\">1</span>-tau)*tfVars[idx+total_vars//<span class=\"number\">2</span>].value())))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> op_holder</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTarget</span>(<span class=\"params\">op_holder,sess</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> op <span class=\"keyword\">in</span> op_holder:</span><br><span class=\"line\">            sess.run(op)</span><br><span class=\"line\"></span><br></pre></td></tr></tbody></table></figure>\n\n<p>现在其实我们把关键的代码都看了，在这段代码实现中，引入了一些state of the art的trick，下面我们结合看过的代码在提一遍。</p>\n<ol>\n<li>引入卷积层，这段代码比较简单，我们没有看。环境状态是用图片的形式给的，用CNN提取特征是比较自然的。</li>\n<li>Experience replay。估值网络不像策略网络一样得到试验之后，用一次就扔掉再去制作新的试验（数据集）来训练。它把每次试验都放在一个试验池里面。试验池长度为N，如果超过了N，那就把最老的试验样本扔掉。每次 训练的时候从试验池里面随机选择batchsize个样本进行训练，保持了对样本的利用率，同时其实也增加了模型的稳定性，因为数据集是相对稳定的（相比于N=1而言，每次训练了就扔掉，进来的都是新的，不那么稳定）。</li>\n<li>使用target网络来辅助训练。<strong>之所以要用target网络来制造训练目标，用主网络来实际训练，是为了让Q-Learing训练的目标保持平稳。</strong>强化学习不像普通的监督学习，它的目标是变化的，<strong>因为学习目标的一部分就是模型本身输出的。</strong>每次更新模型参数都会导致学习目标发生变化，如果更新频繁，幅度很大，我们的训练过程就会变得非常不稳定并且失控。<strong>DQN的训练会陷入目标Q与预测Q的反馈循环中，震荡发散。</strong>所以用target网络来制造目标，target网络和主网络又不是矛盾的，因为target网络会逼近主网络，它是主网络的模仿者，所以它提供的目标Q也是有权威的。</li>\n<li>Double DQN。这个trick源于target网络选的最大Action不准。模仿的不够好，现在就让主网络来帮它选。也就是上面代码中我们看过了主网络输出action，选择target网络输出的reward，得到公式的右边。</li>\n<li>Dueling DQN。上代码</li>\n</ol>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">self.AW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,env.actions]))</span><br><span class=\"line\">self.VW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,<span class=\"number\">1</span>]))</span><br><span class=\"line\">self.Advantage = tf.matmul(self.streamA,self.AW)</span><br><span class=\"line\">self.Value = tf.matmul(self.streamV,self.VW)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Then combine them together to get our final Q-values.</span></span><br><span class=\"line\"><span class=\"comment\"># 看这里Qout是网络最终预测的所有Action的reward。它由两部分组成Value和Advantage</span></span><br><span class=\"line\"><span class=\"comment\"># 由最上面两行可以看出Value是一维的，是实数，advantage是#action维度的向量</span></span><br><span class=\"line\"><span class=\"comment\"># 所以，Dueling DQN就是将reward裁成了两部分，一部分是环境状态本身具有的价值Value</span></span><br><span class=\"line\"><span class=\"comment\"># 另一部分就是Action本身具有的价值，相加起来就是在这个环境下Action具有的价值。</span></span><br><span class=\"line\"><span class=\"comment\"># 其实我感觉这些解释都是人为的，具体是不是这样谁也不知道，可能只是这样优化的好。</span></span><br><span class=\"line\"><span class=\"comment\">#　因为即使不分为两部分，网络输出Ｑout的时候，输入也是环境状态，肯定都会把环境考虑</span></span><br><span class=\"line\"><span class=\"comment\"># 进去才有Action的价值。然而直接输出这个价值，发现优化的不好，分为两部分之后，发现</span></span><br><span class=\"line\"><span class=\"comment\"># 优化的好了。其实谁也不知道其中到底是什么原因起作用。</span></span><br><span class=\"line\">self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=<span class=\"number\">1</span>,keep_dims=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br></pre></td></tr></tbody></table></figure>\n\n<h2 id=\"策略网络和估值网络的不同\"><span class=\"post-title-index\">2.3. </span><a href=\"#策略网络和估值网络的不同\" class=\"headerlink\" title=\"策略网络和估值网络的不同\"></a>策略网络和估值网络的不同</h2><p>从策略网络和估值网络看，强化学习都有探索和对抗两个过程。通过探索，得到数据集，包含了各种可能性。通过对抗，让数据集中好的（reward高的试验胜出）。他们的不同点在于对抗的方法。</p>\n<ol>\n<li>策略网络：对抗的焦点在于选择Action的概率。</li>\n<li>估值网络：对抗的焦点在于评估Action的reward。</li>\n<li>两者都会影响数据集的制作，而且影响的方式是相同的：贪心和随机探索</li>\n<li>比起策略网络，估值网络更加没有对抗的感觉。因为策略网络在提高某一个Action的概率的时候，会抑制其他Action的概率（总的概率为1）。而估值网络在提高某一个Action的reward的时候，和其他Action是无关的。<strong>而且其实不能说是提高reward，它是预测出正确的reward。</strong>（这是两者的很大的不同处，策略网络更像是一个分类问题，而估值网络像是一个回归问题）</li>\n</ol>\n<h1 id=\"回顾\"><span class=\"post-title-index\">3. </span><a href=\"#回顾\" class=\"headerlink\" title=\"回顾\"></a>回顾</h1><p>强化学习的本质是连续决策。强化学习算法的关键是标签制作，数据集制作。</p>\n<p>连续决策问题是没有确定的标签的，它通过探索试验得到数据集和标签，为没有提供标签的问题，做了标签，让问题可以解决。</p>\n<p>深度学习无法优化离散的不可导的参数，强化学习也可以通过在离散的地方做数据集做标签，把它转换为可导的，可用sgd优化的问题。</p>\n<p>做标签和数据集的关键是随机性以及贪心，贪心让它立足于以往的知识，随机让它不刚愎自用，保持谦卑，给新的可能保留空间。</p>\n<p>无论是策略网络还是估值网络，在数据集的制作上都是一样的，随机性和贪心。估值网络的数据集制作，可以看出强化学习探索的本质（由于估值网络本身没有随机性，它在制作数据集的时候，显示的暴露了，探索和贪心的本质。策略网络这方面还不太好看出来，因为它是隐式的利用探索和贪心）。</p>\n<p>策略网络，增大所有行为的概率，但是对于reward大 的行为增大的权重大。这个思路在我得感觉上更加符合强化两个字。强化好的行为嘛。</p>\n<p>估值网络的本质是公式Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)，有点递归的感觉。</p>\n<p>策略网络和估值网络数据集也有一点不同，策略网络的数据集是纵向的，一串行为一起的。而估值网络的数据集是单个单个的。</p>\n<p>由下面代码可以看出，策略网络中，每一窜试验就会训练一次，只是网络参数更新会积累了好几次试验之后才更新。策略网络关心从开始到结束一系列行为。而估值网络只关心当前和下一状态。</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> done:</span><br><span class=\"line\">    episode_number += <span class=\"number\">1</span></span><br><span class=\"line\">    epx = np.vstack(xs)</span><br><span class=\"line\">    epy = np.vstack(ys)</span><br><span class=\"line\">    epr = np.vstack(drs)</span><br><span class=\"line\">    tfp = tfps</span><br><span class=\"line\">    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]</span><br><span class=\"line\"></span><br><span class=\"line\">    discounted_epr = discount_rewards(epr)</span><br><span class=\"line\">    discounted_epr -= np.mean(discounted_epr)</span><br><span class=\"line\">    discounted_epr //= np.std(discounted_epr)</span><br><span class=\"line\"></span><br><span class=\"line\">    tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ix,grad <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tGrad):</span><br><span class=\"line\">        gradBuffer[ix] += grad</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> episode_number % batch_size == <span class=\"number\">0</span>:</span><br><span class=\"line\">            sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[<span class=\"number\">0</span>],W1_1Grad:gradBuffer[<span class=\"number\">1</span>],W2Grad:gradBuffer[<span class=\"number\">2</span>]})</span><br></pre></td></tr></tbody></table></figure>\n\n<p>估值网络人为的控制随机强度，也是一个值得考虑的问题。</p>\n</body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"什么是强化学习，关于强化学习的几点疑问\"><a href=\"#什么是强化学习，关于强化学习的几点疑问\" class=\"headerlink\" title=\"什么是强化学习，关于强化学习的几点疑问\"></a>什么是强化学习，关于强化学习的几点疑问</h1><p><font color=green size=3>强化学习三要素：环境状态，行动，奖励</font></p>\n<p><font color=green size=3>目标：尽量多的获得奖励</font></p>\n<p><font color=green size=3>本质：连续决策</font></p>\n<p>基本的强化学习模型包括：</p>\n<ul>\n<li>环境状态的集合S</li>\n<li>动作的集合A</li>\n<li>状态之间的转换规则（是环境的一部分）</li>\n<li>规定转换后“即时奖励”的规则（是环境的一部分）</li>\n<li>描述主体（智能体）能够观察到什么的规则（是环境的一部分）</li>\n<li>能够做出决策/动作的主体（智能体）</li>\n</ul>\n<h2 id=\"区别于深度学习，强化学习的本质特点是什么？\"><a href=\"#区别于深度学习，强化学习的本质特点是什么？\" class=\"headerlink\" title=\"区别于深度学习，强化学习的本质特点是什么？\"></a>区别于深度学习，强化学习的本质特点是什么？</h2><p><strong>两个定义</strong></p>\n<p>1，强化学习是机器学习的一个重要分支，主要用来解决连续决策问题。</p>\n<p>2，强化学习又称 再励学习，评价学习或增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（agent）在于环境交互过程中通过学习策略以达成回报最大或实现特定目标的问题。</p>\n<p>强化学习的本质是<font color=red> 描述和解决智能体（agent）在于环境交互过程中通过学习策略以达成回报最大或实现特定目标的问题。</font>它本质是这样一种场景，在这种场景中它为了达到某种目的，做出连续的决策。这样符合这种场景，那就是强化学习。</p>\n<p>强化学习和深度学习这两个名词的维度是不一样的。深度学习描述的是算法本身的特点，深度够不够，是不是连接主义的模型。深度学习可以用来做无监督，也可以用来做半监督，也可以用来做弱监督，甚至可以用来作为强化学习算法的一部分。深度学习这个名词，不管应用场景只管模型本身是不是满足深度学习的特点。</p>\n<p>而强化学习描述的是应用场景的特点，只要能提供智能体决策的算法，管它是什么模型，什么结构，那就是强化学习的算法。从这个角度上讲，我现在认为强化学习和监督学习，无监督学习，弱监督学习，是并列的，是对应用场景的描述。和深度学习不是同一维度的。</p>\n<p><font color=red>强化学习的本质在于目标给定的形式，不像无监督学习那样完全没有学习目标，也不像监督学习那样有非常明确的目标（label），强化学习的目标一般是变化的，不明确的，甚至可能不存在绝对正确的目标。，强化学习的问题都可以抽象成，环境状态，行动和奖励。应该说只要能抽象为这三个要素，目标是获得最大奖励的模型，就是强化学习的模型。</font></p>\n<p>强化学习的最大的特点是“试错”，是尝试各种可能，而强化结果好的可能。（策略网络的特点，估值网络的特点是修正和预测获益）</p>\n<p>由于强化学习是一种决策学习，这个问题的特点就是离散型。但是离散并不就是强化学习（连续的决策才是，目标的模糊和不确定性是决策问题的特点）。深度学习本质是函数的拟合，所以连续可微是它的特点。并不能说连续可微的问题就是深度学习，离散的问题就是强化学习。 </p>\n<h2 id=\"深度学习用梯度下降的算法实现模型的优化，强化学习无法求导，甚至连学习目标都是模糊的如何优化模型参数？\"><a href=\"#深度学习用梯度下降的算法实现模型的优化，强化学习无法求导，甚至连学习目标都是模糊的如何优化模型参数？\" class=\"headerlink\" title=\"深度学习用梯度下降的算法实现模型的优化，强化学习无法求导，甚至连学习目标都是模糊的如何优化模型参数？\"></a>深度学习用梯度下降的算法实现模型的优化，强化学习无法求导，甚至连学习目标都是模糊的如何优化模型参数？</h2><p>这个问题要了解下一节，具体的强化学习方法。</p>\n<h2 id=\"强化学习与弱监督学习？\"><a href=\"#强化学习与弱监督学习？\" class=\"headerlink\" title=\"强化学习与弱监督学习？\"></a>强化学习与弱监督学习？</h2><p>弱监督是属于有监督学习，只是它学习的目标不是被给予的标签，而是比被给予标签更强的标签（强弱是指标签做含有的信息量），也就是说弱监督是根据少量信息的标签，推测出更多的信息。</p>\n<p>强化学习的本质是连续决策。连续决策的特点是目标的模糊和不确定性。</p>\n<p>所以，虽然弱监督和强化学习都没有给出最终准确的目标，但是他们任然很不同的</p>\n<h2 id=\"深度学习是死的，没有智能的机器学习算法，强化学习是“活的”，有智能的机器学习算法\"><a href=\"#深度学习是死的，没有智能的机器学习算法，强化学习是“活的”，有智能的机器学习算法\" class=\"headerlink\" title=\"深度学习是死的，没有智能的机器学习算法，强化学习是“活的”，有智能的机器学习算法\"></a>深度学习是死的，没有智能的机器学习算法，强化学习是“活的”，有智能的机器学习算法</h2><p>深度学习大部分我们用作有监督的学习算法。其实说深度学习是死的，不如说有监督学习是死的。</p>\n<p>有监督学习其实是完全的复刻标签里面含有的知识，它的本质就是一个函数拟合的问题，它无法摆脱对绝对的标签的依赖，无法超越标签。</p>\n<p>而强化学习，正由于它的目标是模糊和不确定的。使得算法在设计上必须具有随机性和探索性，它能够探索出人类从来没有到过的领域。就像在围棋上，下出人类完全无法理解的棋，人从来没有想过的一些下法。这就是强化学习算法探索出来的知识。所以我觉得它是活着的，拥有智能的算法。</p>\n<p><font color=green>从感性的层面，强化学习算法很接近人脑的行为：感知环境，探索环境，强化有益行为</font></p>\n<h2 id=\"关于深度学习，强化学习，连续可导性和离散不可导性的讨论\"><a href=\"#关于深度学习，强化学习，连续可导性和离散不可导性的讨论\" class=\"headerlink\" title=\"关于深度学习，强化学习，连续可导性和离散不可导性的讨论\"></a>关于深度学习，强化学习，连续可导性和离散不可导性的讨论</h2><ol>\n<li><p>从函数的角度，深度学习和强化学习都需要学习一个函数映射。深度学习是从输入到target的映射。强化学习是学从环境状态到Action的映射。这两个映射可以看成性质一样的，因为深度学习可以作为强化学习的智能体。所以从函数的角度，他们没有连续和离散的区别。（PS. 深度学习模型和强化学习的智能体都是连续可导的函数。target和Action都可以是离散的或者连续的。）</p>\n</li>\n<li><p>深度学习作为用梯度下降算法优化的模型，无法优化对loss不可导的参数。如在深度学习中，设计一个分支来决定模型是否应该包含某个模块，这个分支的参数是不可优化的。因为包含和不包含是离散的，对loss不可导。</p>\n<p>不过假设，如果包含与不包含是连续的。也就是说，可以以0.1的权重包含。那么，这些参数是可以优化的。从这点来看，离散就是导致模型参数不可优化的原因。</p>\n</li>\n<li><p>强化学习在离散的情况下解决2中的问题。由此，我得到了一个概念，强化学习解决离散的问题。</p>\n</li>\n<li><p>下面我们来分析一下这个场景。首先，决定一个模块该不该被使用，这个场景是一次Action的场景，不是连续决策的场景。也就是说Action一次，我就能知道最终reward多少了，只有单步的reward。</p>\n<p>限于这个单次决策的场景来看，如果Action相对于reward是连续可导的，那么深度学习就能解决这个问题。如果Action相对于reward是离散的，那么仅仅深度学习无法解决这个问题，要靠强化学习。</p>\n<p><font color=green>这里单次Action就知道reward，实际上这个问题就退化为了有监督学习，因为这个单步的reward就可以看成我的标签了。所以深度学习解决这个问题是很自然的。</font></p>\n</li>\n<li><p>多次决策的问题，无论是连续的还是离散的都只能用强化学习的方法。因为多次决策，这个问题就不可以退化为监督学习的问题了。它是一个真正的强化学习的问题。</p>\n</li>\n</ol>\n<p>综上所诉，强化学习解决深度学习解决不了的离散问题，那只是在单次决策的时候，这个问题退化为了有监督学习。强化学习的方法，恰好可以提供离散变量的学习。</p>\n<p>强化学习方法解决深度学习中的离散问题，仅仅是强化学习附带的一个小福利。</p>\n<p><strong><font color=green>因为它能把经验转化为可导的目标，就拿策略网络来说，从梯度的角度，它只管增加当前随机Action的概率，而加入advatage之后，自动就优化除了想要的大reward的行为。</font></strong></p>\n<h2 id=\"深度学习优化和强化学习优化的感性理解\"><a href=\"#深度学习优化和强化学习优化的感性理解\" class=\"headerlink\" title=\"深度学习优化和强化学习优化的感性理解\"></a>深度学习优化和强化学习优化的感性理解</h2><p>前面说了深度学习优化可导的参数，强化学习可以优化不可导的参数。这里说一下对深度学习优化方法和强化学习优化方法的感性理解。</p>\n<p>还是说前面包不会包含某模块的例子，由于连续可导，对于每一个参数值，深度学习模型其实都同时参与了两种Action（包含和不包含）。score = 0.1 包含，其实其中包含了含有的成分，也包含了不含有的成分。所以我们可以连续的变动score，看看包含多好，还是不好含多好。这其实就是梯度下降算法的方式。得益于每一个参数，其实我都对包含和不包含的情况都有了解，我当然知道哪个更好，就往那边移动（优化）。</p>\n<p>然而，对于离散的情况，要么只能包含，要么只能不包含。当选择包含的时候，模型对不包含的情况完全是无知的。可能更好，也可能差。当不包含的时候，也是一样的。无论哪种情况，我都没有办法优化，两种情况是完全隔离开的，信息不沟通的，是离散的。所以梯度下降算法无法优化它。</p>\n<p>强化学习用随机探索的方法让两者信息又沟通起来。包含一下试一下，然后，不包含也试一下。尝试的结果是哪种reward多，就增大哪种的概率。</p>\n<p><strong><font color=green>所以，无论哪种优化方法，信息的沟通都是必要的。要对所有的action都了解，才能知道选择哪种action。 只是深度学习是连续的，它的每一种参数，都包含了所有Action的信息（reward），每一种Action都参与了，所以它能直接连续的梯度下降的优化，不需要随机探索了。而对于离散的，每种action只能知道自己的reward，对其他Action一无所知的时候，梯度的优化是不行的。必须要探索各种Action，还是要知道了每一种Action的情况(reward)之后，才能优化。这是方法论。</font></strong></p>\n<p>更进一步，离散的地方，相对于reward一定是不可导的，所以深度学习不行。而强化学习，更准确的说是策略网络，相当于给离散的地方加了标签，这样它就在离散的地方有监督了，它就可以根据增加的标签优化。而标签的设计就是根据探索的结果，增大reward大的Action，reward大的Action就是它的标签，而且这个标签是动态的，是对抗得出的。</p>\n<p>强化学习方法算出的梯度是策略梯度。</p>\n<p><strong><font color=green>强化学习： 不知道选哪边了； 试试呗；按试出来reward大的Action优化它。</font></strong></p>\n<p><strong><font color=green>强化学习：它离散，对于reward不可导；不直接用reward优化它，给它加个标签，把试出来reward大的Action，作为标签去优化</font></strong></p>\n<p>在不可导的地方加标签。</p>\n<p><strong><font color=green>由此，强化学习算法的本质是制作标签，无论是连续决策，标签不确定的情况，还是它能解决离散问题的情况，它都是用制作标签的方法解决的。</font></strong></p>\n<h1 id=\"策略网络-Policy-Network-和估值网络-Value-Network\"><a href=\"#策略网络-Policy-Network-和估值网络-Value-Network\" class=\"headerlink\" title=\"策略网络(Policy Network)和估值网络(Value Network)\"></a>策略网络(Policy Network)和估值网络(Value Network)</h1><p>AlphaGo 使用了快速走子，策略网络，估值网络和蒙特卡洛搜索树等技术。</p>\n<p>强化学习算法的一个关键是<font color=green>随机性和探索性</font>，我们需要让算法通过试验样本自己学习什么才是某个环境状态下比较好的Action，而不是像有监督学习一样，告诉模型什么是好的Action，因为我们也不知道什么是好的Action.</p>\n<p>深度强化学习模型的本质是神经网络，神经网络是工具，根据问题转化以及建模的不同，主要分为策略网络和估值网络。</p>\n<p>强化学习中最重要的两类方法<strong>Policy-based,Value-based</strong>。第一种直接预测在某个环境下应该采取的行动（直接输出改采取Action的概率）。第二种预测在某个环境下所有行动的期望价值，然后通过选择q值最高的行动执行策略。</p>\n<p>他们都能完成决策，但由于建模的不同，估值网络包含有更多的信息，它不仅能提供决策，还预测了决策带来的收益。</p>\n<p><font color=red>策略网络是隐式的学习了某一Action所带来的全部获益（当前获益+后续获益），而估值网络直接显示的学习Action所带来的全部获益。</font>强化学习算法做出最佳抉择只需要知道哪个Action全部获益最大，策略网络就是这样做的，估值网络不仅学习了哪个Action全部获益最大，还把每个Action的全部获益给计算出来了。</p>\n<p><font color=green>相对来说，策略网络的性能会比估值网络好一些。</font></p>\n<p><font color=green>Value Based方法适合仅有少量Action的环境，而Policy Based方法更通用，适合Action种类非常多，或者具有连续取值的Action的环境。结合了深度学习之后，Policy Based方法就变成了策略网络，Value Based方法就变成了估值网络。</font></p>\n<h2 id=\"策略网络-Policy-Network\"><a href=\"#策略网络-Policy-Network\" class=\"headerlink\" title=\"策略网络(Policy Network)\"></a>策略网络(Policy Network)</h2><p>直接看一个例子，学习的目标是，左右用力使得木棍不倒地，<a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Policy_Network.py</a> <a href=\"policy_network.py\">Policy_Network.py</a></p>\n<p>关键代码</p>\n <figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">score = tf.matmul(layer1,W2)</span><br><span class=\"line\">probability = tf.nn.sigmoid(score)<span class=\"comment\">#网络输出采取Action 1的概率。</span></span><br><span class=\"line\">input_y = tf.placeholder(tf.float32,[<span class=\"literal\">None</span>,<span class=\"number\">1</span>], \\\\</span><br><span class=\"line\">                         name=<span class=\"string\">&quot;input_y&quot;</span>)<span class=\"comment\"># 输入采取过的行为，这个行为是随机生成的。</span></span><br><span class=\"line\">advantages = tf.placeholder(tf.float32,name=<span class=\"string\">&quot;reward_signal&quot;</span>) </span><br><span class=\"line\"><span class=\"comment\"># 输入获益</span></span><br><span class=\"line\">loglik = tf.log(input_y*(input_y - probability) + \\\\</span><br><span class=\"line\">                (<span class=\"number\">1</span> - input_y)*(input_y + probability)) </span><br><span class=\"line\"><span class=\"comment\"># 损失函数，如果行为是1，则增大</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#概率，如果行为是 0，则减小概率（相当于也是增加0的概率），也就是说这个损</span></span><br><span class=\"line\"><span class=\"comment\">#失函数无论当前行为是什么都会增大当前行为的概率。</span></span><br><span class=\"line\">loss = -tf.reduce_mean(loglik * advantages) <span class=\"comment\"># 这行代码很关键，</span></span><br><span class=\"line\"><span class=\"comment\">#相当于给损失函数成了一个权重advantages，得到最终的损失函数。</span></span><br><span class=\"line\"><span class=\"comment\">#advantages是当前试验的全部获益。如果全部获益大，将以更大的权重，增加 </span></span><br><span class=\"line\"><span class=\"comment\">#当前行为的概率。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#所以，策略网络其实也是一个对抗学习的过程，增加所有采取过行为的概率，只是</span></span><br><span class=\"line\"><span class=\"comment\">#获益多的行为以更大的权重增加。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 一个试验： 由初始状态开始，随机采取一连串的行为</span></span><br><span class=\"line\"><span class=\"comment\">#（Policy_Network.py 中是根据当前模型输出的概率,来生成随机的行为，但</span></span><br><span class=\"line\"><span class=\"comment\">#是我感觉直接用0.5的概率随机生成一连串的0和1的行为也是可以的，下面将实</span></span><br><span class=\"line\"><span class=\"comment\">#验一下），直到任务结束。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 由于每个试验，都可以一直行为到任务结束，所以每个action，我们都可以得</span></span><br><span class=\"line\"><span class=\"comment\">#到它在该试验中的全部获益（当前获益 + 之后所有行为的获益）</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 随机生成了n个试验，其中又各种各样的决策（随机探索），全部获益大的</span></span><br><span class=\"line\"><span class=\"comment\">#action，它的advantages也大，那么它的概率就增大的多，它被强化的厉害。</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">## 试验生成的过程，实际上就是数据集构建的过程。策略网络的数据集是由环境</span></span><br><span class=\"line\"><span class=\"comment\">#和一系列随机的行为构成的。它提供了环境在各种行为下的反应（获益）。模型</span></span><br><span class=\"line\"><span class=\"comment\">#学习为环境带来高获益的行为的规律。</span></span><br></pre></td></tr></table></figure>\n\n<p>由上面的代码可知，从策略网络的角度看强化学习的话，强化学习的关键其实是对数据集的构建—如何构建数据集。</p>\n<p>在构建数据集的时候，随机探索肯定是必要的。随机探索的结果会得到一系列好的行为，也会得到一系列不好的行为。如何强化好的行为就是算法设计的时候需要注意的。</p>\n<p>上面的代码在探索阶段借用了当前的模型，即根据当前模型输出的概率随机生成行为，从而形成数据集。如果完全的随机（一直使用0.5的概率随机的生成Action）会什么样呢？</p>\n<h3 id=\"数据集是否可以和模型无关（不随着模型变化）？\"><a href=\"#数据集是否可以和模型无关（不随着模型变化）？\" class=\"headerlink\" title=\"数据集是否可以和模型无关（不随着模型变化）？\"></a>数据集是否可以和模型无关（不随着模型变化）？</h3><p>关键修改代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#基于当前模型，根据当前的状态x，生成Action 1的概率</span></span><br><span class=\"line\">tfprob = sess.run(probability,feed_dict=&#123;observations: x&#125;)</span><br><span class=\"line\"><span class=\"comment\"># 基于预测概率，随机生成行为，并试探环境。生成数据集。</span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n\n<p>修改后</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 注释掉这句，并不需要根据当前模型生成概率</span></span><br><span class=\"line\"><span class=\"comment\"># tfprob = sess.run(probability,feed_dict=&#123;observations: x&#125;)</span></span><br><span class=\"line\"><span class=\"comment\"># 直接设置概率为0.5，随机完全随机探索生成数据集。</span></span><br><span class=\"line\">tfprob = <span class=\"number\">0.5</span></span><br><span class=\"line\">action = <span class=\"number\">1</span> <span class=\"keyword\">if</span> np.random.uniform() &lt; tfprob <span class=\"keyword\">else</span> <span class=\"number\">0</span></span><br></pre></td></tr></table></figure>\n\n<p>结果：修改后，模型无法收敛。</p>\n<p>完全随机很小的概率能探索出很好的试验，这些好的行为也很难持续的得到强化。</p>\n<p>所以，强化学习也有一种效果叠加的感觉。在完全随机的情况下 ，探索出相对好的action，再在这个相对好的action的基础上，在探索探索出更好的action。</p>\n<p>如果数据集不依赖模型，就是一直在完全随机的基础上探索。这样很难收敛。</p>\n<p>也可以这样看，完全随机的话，最多能学到前几步的策略（因为完全随机就走不了几步，探索的经验就只有那几步）。依赖于模型，探索的行为更有价值，因为是依赖于学到过的知识的，一方面确认了，按学到的知识走，确实获益多，一方面又在学到的知识的基础上，做了一些随机，探索更好的知识。</p>\n<h2 id=\"估值网络-Value-Network，Q-learning\"><a href=\"#估值网络-Value-Network，Q-learning\" class=\"headerlink\" title=\"估值网络(Value Network，Q-learning)\"></a>估值网络(Value Network，Q-learning)</h2><p>Q-Learing用神经网络实现，得到的模型就是估值网络。</p>\n<p>也看一个例子，<a href=\"https://github.com/xyegithub/myBlog/blob/main/2021/12/03/First-Step-to-RL/policy_network.py\">Value_Network.py</a></p>\n<p>学习每个Action所对应的reward的期望。</p>\n<p>我们先看看数据集的结构</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Save the experience to our episode buffer.</span></span><br><span class=\"line\">episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[<span class=\"number\">1</span>,<span class=\"number\">5</span>]))</span><br><span class=\"line\"><span class=\"comment\"># 其中s是当前时刻的环境状态，a是当前随机采取的Action，r是这个Action的当前reward</span></span><br><span class=\"line\"><span class=\"comment\"># s1是采取Action之后的下一状态，d是布尔型表示是否任务结束。  </span></span><br></pre></td></tr></table></figure>\n\n<ol>\n<li><p>现在目标是学习Q(s<sub>t</sub>, a<sub>t</sub>)，也就是当前环境状态，采取行为a的全部reward的期望。</p>\n</li>\n<li><p>现在假设我们有模型Q<sub>desird</sub>，可以预测全部reward了，那么这个模型应该满足条件，Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)，这有点递归的感觉了。</p>\n</li>\n<li><p>现在我们能不能根据这个公式 ， 来优化出Q<sub>desired</sub>。肯定是能的。对于探索过的所有试验，公式都满足的话，此时的模型就可以看成我们想要的模型了。我感觉这就是估值网络方法的核心。</p>\n</li>\n</ol>\n<p>直接看关键代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#Choose an action by greedily (with e chance of random action)</span></span><br><span class=\"line\"><span class=\"comment\"># from the Q-network</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> np.random.rand(<span class=\"number\">1</span>) &lt; e <span class=\"keyword\">or</span> total_steps &lt; pre_train_steps:</span><br><span class=\"line\">    a = np.random.randint(<span class=\"number\">0</span>,<span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"keyword\">else</span>:</span><br><span class=\"line\">    a = sess.run(mainQN.predict,feed_dict=&#123;mainQN.scalarInput:[s]&#125;)[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"comment\"># 这段代码其实是探索的代码，当最开始的时候是完全随机探索（total_steps &lt; pre_train_steps的时候）</span></span><br><span class=\"line\"><span class=\"comment\"># 当total_steps &gt;= pre_train_steps之后呢，就不是完全随机探索了。</span></span><br><span class=\"line\"><span class=\"comment\"># 有e的概率是随机探索的，（1-e）的概率是由训练好的模型决定之后的Action。 </span></span><br><span class=\"line\"><span class=\"comment\"># 这里是估值网络和策略网络的不同，策略网络本身就具有随机性，所以不需要引入</span></span><br><span class=\"line\"><span class=\"comment\"># 额外的参数e和pre_train_steps来控制随机探索的强度。</span></span><br><span class=\"line\"><span class=\"comment\"># 策略网络在训练的过程中，本身就是由随机性大，到慢慢的收敛到好的Action</span></span><br><span class=\"line\"><span class=\"comment\"># 所以可以直接得到好的探索的训练样本。估值网络没有这样好的性质，它连</span></span><br><span class=\"line\"><span class=\"comment\"># 随机性都没有，就需要人为的制造，满足从完全随机探索，到在好的Action的</span></span><br><span class=\"line\"><span class=\"comment\"># 基础上具有一定的随机性进行探索。</span></span><br></pre></td></tr></table></figure>\n\n<p><strong>这是策略网络和估值网络的共通之处，其实这也最上面那个注释”by greedily (with e chance of random action) from the Q-network”的意思。</strong></p>\n<p><strong><font color=green>“贪心”两个字完美的诠释了强化学习，无论是策略网络还是估值网络，在探索阶段，在生成数据集上的特点。</font></strong></p>\n<p>在策略网络那一节，我做的那个试验，和模型无关生成数据集。其实就是不贪心了，不贪心不行。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps &gt; pre_train_steps:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> e &gt; endE:</span><br><span class=\"line\">        e -= stepDrop</span><br><span class=\"line\"><span class=\"comment\"># 完全随机了之后，开始慢慢减小随机性。</span></span><br><span class=\"line\"><span class=\"comment\"># 模型约不可靠的时候，探索性和随机性越强。后来模型慢慢变得可靠就减弱随机性。</span></span><br><span class=\"line\"><span class=\"comment\"># 因为模型越来越可靠的时候，随机性大就会得到很多远远低于当前模型性能的试验</span></span><br><span class=\"line\"><span class=\"comment\"># 这些试验都是早就被pass了的，学不到什么东西，损坏模型的探索。</span></span><br><span class=\"line\"><span class=\"comment\"># endE=0.1 说明无论训练的多好，模型都保持了随机性，保持了探索性</span></span><br><span class=\"line\"><span class=\"comment\">#　人永远要有好奇心，永远要觉得自己的知识还可能不是最好的</span></span><br></pre></td></tr></table></figure>\n\n\n\n<p>下面的代码是将在数据集弄好的情况下，如何训练模型的。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> total_steps % (update_freq) == <span class=\"number\">0</span>:</span><br><span class=\"line\">    trainBatch = myBuffer.sample(batch_size) <span class=\"comment\">#Get a random batch of experiences.</span></span><br><span class=\"line\">    <span class=\"comment\">#Below we perform the Double-DQN update to the target Q-values</span></span><br><span class=\"line\">    <span class=\"comment\"># 主网络预测了下一刻需要采取的Action，trainBatch[:,3]是当前的下一刻的环境</span></span><br><span class=\"line\">    <span class=\"comment\"># 回顾公式，Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># 这里主函数预测的Action就是t+1时刻（下一时刻）获益值最大的Action</span></span><br><span class=\"line\">    Q1 = sess.run(mainQN.predict,feed_dict=&#123;mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])&#125;)</span><br><span class=\"line\">    <span class=\"comment\"># target网络预测了下一时刻的reward</span></span><br><span class=\"line\">    Q2 = sess.run(targetQN.Qout,feed_dict=&#123;targetQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">3</span>])&#125;)</span><br><span class=\"line\">    end_multiplier = -(trainBatch[:,<span class=\"number\">4</span>] - <span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 用主网络预测出的Action以及target网络预测出的所有行为的reward</span></span><br><span class=\"line\">    <span class=\"comment\"># 选择了最大的reward,也就是公式中的 Max Q(s_t+1, a)</span></span><br><span class=\"line\">    doubleQ = Q2[<span class=\"built_in\">range</span>(batch_size),Q1]</span><br><span class=\"line\">    <span class=\"comment\"># 这里得到的就是公式Q(st, at) = r + $\\lambda$ Max Q(s_t+1, a)</span></span><br><span class=\"line\">    <span class=\"comment\"># 的右边，当前reward加上乘以衰减系数之后的，下一步最大reward</span></span><br><span class=\"line\">    targetQ = trainBatch[:,<span class=\"number\">2</span>] + (y*doubleQ * end_multiplier)</span><br><span class=\"line\">    <span class=\"comment\">#Update the network with our target values.</span></span><br><span class=\"line\">    <span class=\"comment\"># 公式右边得到了之后，在把真正的当前状态输入进去，得到左边</span></span><br><span class=\"line\">    <span class=\"comment\"># 左边以右边作为标签进行学习。更新主网络的参数</span></span><br><span class=\"line\">    _ = sess.run(mainQN.updateModel, \\</span><br><span class=\"line\">                 feed_dict=&#123;mainQN.scalarInput:np.vstack(trainBatch[:,<span class=\"number\">0</span>]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,<span class=\"number\">1</span>]&#125;)</span><br><span class=\"line\">    <span class=\"comment\"># 更新target网络的参数。</span></span><br><span class=\"line\">    updateTarget(targetOps,sess) <span class=\"comment\">#Update the target network toward the primary network.</span></span><br></pre></td></tr></table></figure>\n\n\n\n<p>target网络参数的更新方式代码</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTargetGraph</span>(<span class=\"params\">tfVars,tau</span>):</span></span><br><span class=\"line\">    total_vars = <span class=\"built_in\">len</span>(tfVars)</span><br><span class=\"line\">    op_holder = [] </span><br><span class=\"line\">    <span class=\"comment\"># 主网络是和target网络一样的，前一半参数正好是主网络的参数</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> idx,var <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tfVars[<span class=\"number\">0</span>:total_vars//<span class=\"number\">2</span>]):</span><br><span class=\"line\">        <span class=\"comment\"># idx+total_vars对应的时候后一半的参数也就是target网络的参数</span></span><br><span class=\"line\">        <span class=\"comment\"># 这里相当于是target参数 = 主网络参数 * tau + （1- tau）*target参数</span></span><br><span class=\"line\">        <span class=\"comment\"># 也就是说target网络在以一定的速度向主网络靠近</span></span><br><span class=\"line\">        <span class=\"comment\"># 结合前面的代码，主网络才是真正学习的网络，target网络的作用仅仅是得到等式</span></span><br><span class=\"line\">        <span class=\"comment\"># 右边的值，即标签，即使是等式右边，也不是target网络完全决定的</span></span><br><span class=\"line\">        <span class=\"comment\"># target网络得到了所有Action的reward，最大的Action是主网络选择的</span></span><br><span class=\"line\">        <span class=\"comment\"># 为什么要这么做，target网络也是在模仿主网络，只用主网络也能得到等式的右边</span></span><br><span class=\"line\">        <span class=\"comment\"># 理论上其实右边也应该是主网络决定，现在搞了个主网络的模仿者target网络</span></span><br><span class=\"line\">        <span class=\"comment\"># 是出于优化的考虑。我们后面叙述。</span></span><br><span class=\"line\">        op_holder.append(tfVars[idx+total_vars//<span class=\"number\">2</span>].assign\\\\</span><br><span class=\"line\">                         ((var.value()*tau) \\\\</span><br><span class=\"line\">                      + ((<span class=\"number\">1</span>-tau)*tfVars[idx+total_vars//<span class=\"number\">2</span>].value())))</span><br><span class=\"line\">        <span class=\"keyword\">return</span> op_holder</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">updateTarget</span>(<span class=\"params\">op_holder,sess</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> op <span class=\"keyword\">in</span> op_holder:</span><br><span class=\"line\">            sess.run(op)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>现在其实我们把关键的代码都看了，在这段代码实现中，引入了一些state of the art的trick，下面我们结合看过的代码在提一遍。</p>\n<ol>\n<li>引入卷积层，这段代码比较简单，我们没有看。环境状态是用图片的形式给的，用CNN提取特征是比较自然的。</li>\n<li>Experience replay。估值网络不像策略网络一样得到试验之后，用一次就扔掉再去制作新的试验（数据集）来训练。它把每次试验都放在一个试验池里面。试验池长度为N，如果超过了N，那就把最老的试验样本扔掉。每次 训练的时候从试验池里面随机选择batchsize个样本进行训练，保持了对样本的利用率，同时其实也增加了模型的稳定性，因为数据集是相对稳定的（相比于N=1而言，每次训练了就扔掉，进来的都是新的，不那么稳定）。</li>\n<li>使用target网络来辅助训练。<strong>之所以要用target网络来制造训练目标，用主网络来实际训练，是为了让Q-Learing训练的目标保持平稳。</strong>强化学习不像普通的监督学习，它的目标是变化的，<strong>因为学习目标的一部分就是模型本身输出的。</strong>每次更新模型参数都会导致学习目标发生变化，如果更新频繁，幅度很大，我们的训练过程就会变得非常不稳定并且失控。<strong>DQN的训练会陷入目标Q与预测Q的反馈循环中，震荡发散。</strong>所以用target网络来制造目标，target网络和主网络又不是矛盾的，因为target网络会逼近主网络，它是主网络的模仿者，所以它提供的目标Q也是有权威的。</li>\n<li>Double DQN。这个trick源于target网络选的最大Action不准。模仿的不够好，现在就让主网络来帮它选。也就是上面代码中我们看过了主网络输出action，选择target网络输出的reward，得到公式的右边。</li>\n<li>Dueling DQN。上代码</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">self.AW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,env.actions]))</span><br><span class=\"line\">self.VW = tf.Variable(xavier_init([h_size//<span class=\"number\">2</span>,<span class=\"number\">1</span>]))</span><br><span class=\"line\">self.Advantage = tf.matmul(self.streamA,self.AW)</span><br><span class=\"line\">self.Value = tf.matmul(self.streamV,self.VW)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#Then combine them together to get our final Q-values.</span></span><br><span class=\"line\"><span class=\"comment\"># 看这里Qout是网络最终预测的所有Action的reward。它由两部分组成Value和Advantage</span></span><br><span class=\"line\"><span class=\"comment\"># 由最上面两行可以看出Value是一维的，是实数，advantage是#action维度的向量</span></span><br><span class=\"line\"><span class=\"comment\"># 所以，Dueling DQN就是将reward裁成了两部分，一部分是环境状态本身具有的价值Value</span></span><br><span class=\"line\"><span class=\"comment\"># 另一部分就是Action本身具有的价值，相加起来就是在这个环境下Action具有的价值。</span></span><br><span class=\"line\"><span class=\"comment\"># 其实我感觉这些解释都是人为的，具体是不是这样谁也不知道，可能只是这样优化的好。</span></span><br><span class=\"line\"><span class=\"comment\">#　因为即使不分为两部分，网络输出Ｑout的时候，输入也是环境状态，肯定都会把环境考虑</span></span><br><span class=\"line\"><span class=\"comment\"># 进去才有Action的价值。然而直接输出这个价值，发现优化的不好，分为两部分之后，发现</span></span><br><span class=\"line\"><span class=\"comment\"># 优化的好了。其实谁也不知道其中到底是什么原因起作用。</span></span><br><span class=\"line\">self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=<span class=\"number\">1</span>,keep_dims=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"策略网络和估值网络的不同\"><a href=\"#策略网络和估值网络的不同\" class=\"headerlink\" title=\"策略网络和估值网络的不同\"></a>策略网络和估值网络的不同</h2><p>从策略网络和估值网络看，强化学习都有探索和对抗两个过程。通过探索，得到数据集，包含了各种可能性。通过对抗，让数据集中好的（reward高的试验胜出）。他们的不同点在于对抗的方法。</p>\n<ol>\n<li>策略网络：对抗的焦点在于选择Action的概率。</li>\n<li>估值网络：对抗的焦点在于评估Action的reward。</li>\n<li>两者都会影响数据集的制作，而且影响的方式是相同的：贪心和随机探索</li>\n<li>比起策略网络，估值网络更加没有对抗的感觉。因为策略网络在提高某一个Action的概率的时候，会抑制其他Action的概率（总的概率为1）。而估值网络在提高某一个Action的reward的时候，和其他Action是无关的。<strong>而且其实不能说是提高reward，它是预测出正确的reward。</strong>（这是两者的很大的不同处，策略网络更像是一个分类问题，而估值网络像是一个回归问题）</li>\n</ol>\n<h1 id=\"回顾\"><a href=\"#回顾\" class=\"headerlink\" title=\"回顾\"></a>回顾</h1><p>强化学习的本质是连续决策。强化学习算法的关键是标签制作，数据集制作。</p>\n<p>连续决策问题是没有确定的标签的，它通过探索试验得到数据集和标签，为没有提供标签的问题，做了标签，让问题可以解决。</p>\n<p>深度学习无法优化离散的不可导的参数，强化学习也可以通过在离散的地方做数据集做标签，把它转换为可导的，可用sgd优化的问题。</p>\n<p>做标签和数据集的关键是随机性以及贪心，贪心让它立足于以往的知识，随机让它不刚愎自用，保持谦卑，给新的可能保留空间。</p>\n<p>无论是策略网络还是估值网络，在数据集的制作上都是一样的，随机性和贪心。估值网络的数据集制作，可以看出强化学习探索的本质（由于估值网络本身没有随机性，它在制作数据集的时候，显示的暴露了，探索和贪心的本质。策略网络这方面还不太好看出来，因为它是隐式的利用探索和贪心）。</p>\n<p>策略网络，增大所有行为的概率，但是对于reward大 的行为增大的权重大。这个思路在我得感觉上更加符合强化两个字。强化好的行为嘛。</p>\n<p>估值网络的本质是公式Q<sub>desired</sub>(s<sub>t</sub>, a<sub>t</sub>) = r + $\\lambda$ Max<sub>a</sub> Q<sub>desired</sub>(s<sub>t+1</sub>, a)，有点递归的感觉。</p>\n<p>策略网络和估值网络数据集也有一点不同，策略网络的数据集是纵向的，一串行为一起的。而估值网络的数据集是单个单个的。</p>\n<p>由下面代码可以看出，策略网络中，每一窜试验就会训练一次，只是网络参数更新会积累了好几次试验之后才更新。策略网络关心从开始到结束一系列行为。而估值网络只关心当前和下一状态。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span> done:</span><br><span class=\"line\">    episode_number += <span class=\"number\">1</span></span><br><span class=\"line\">    epx = np.vstack(xs)</span><br><span class=\"line\">    epy = np.vstack(ys)</span><br><span class=\"line\">    epr = np.vstack(drs)</span><br><span class=\"line\">    tfp = tfps</span><br><span class=\"line\">    xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]</span><br><span class=\"line\"></span><br><span class=\"line\">    discounted_epr = discount_rewards(epr)</span><br><span class=\"line\">    discounted_epr -= np.mean(discounted_epr)</span><br><span class=\"line\">    discounted_epr //= np.std(discounted_epr)</span><br><span class=\"line\"></span><br><span class=\"line\">    tGrad = sess.run(newGrads,feed_dict=&#123;observations: epx, input_y: epy, advantages: discounted_epr&#125;)</span><br><span class=\"line\">    <span class=\"keyword\">for</span> ix,grad <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(tGrad):</span><br><span class=\"line\">        gradBuffer[ix] += grad</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> episode_number % batch_size == <span class=\"number\">0</span>:</span><br><span class=\"line\">            sess.run(updateGrads,feed_dict=&#123;W1Grad: gradBuffer[<span class=\"number\">0</span>],W1_1Grad:gradBuffer[<span class=\"number\">1</span>],W2Grad:gradBuffer[<span class=\"number\">2</span>]&#125;)</span><br></pre></td></tr></table></figure>\n\n<p>估值网络人为的控制随机强度，也是一个值得考虑的问题。</p>\n"},{"title":"Personal Thought","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-15T08:01:28.000Z","password":null,"summary":null,"description":"记录自己在学术方面的一些想法。","_content":"\n\n\n# Deep Learning\n\n## 泛化性能\n\n### 泛化性能的本质是什么样的拟合曲面是好的，一定要依靠先验，现实中曲面有哪些特点。\n\n深度学习的本质就是函数的拟合。泛化性能是衡量深度学习模型性能的指标。\n\n在两个模型都完全拟合训练集的情况下，i.e.，两个模型所表示的函数在训练点处的函数值都是完全正确的。泛化能力要求两函数在测试点处也具有正确的函数值。\n\n这个问题其实只能依赖先验。对于拟合曲面的先验，也就是说，什么样的拟合曲面是好的。\n\n比如，如果点A处函数值为1，那么把A翻转，函数值也为1。这是源于目标识别常识的先验。\n\n数据增广方法mixup，发现不同图片之间函数值线性变换是好的。这是对函数曲面的另一个先验。在样本之间，函数线性变换好。\n\n所以，深度学习的本质问题是什么样的面是好的，如果我们知道什么样的面是好的。知道我们需要拟合的曲面的一些特点，那么就可以利用这些先验来进行数据增广，甚至进行模型的设计。\n\n比如，要拟合的曲面是几次的。是否需要高次非线性。如果需要我们就要引入特张图之间的乘法。强化学习的特征选择给模型带来另一种非线性。哪种非线性是好的？要回答这个问题，就要知道我们需要拟合的那个函数是几次的，是哪种非线性。\n\n**设想如果，有一种可视化方法，直接可视化高维数据。我们用它来可视化在MNIST, CIFAR-10等数据集上都达到100%准确率的函数。我们总结这些函数曲面有什么特点，光不光滑？是几次的？有哪些对称性？。总结出适用于所有数据集的共性的曲面特点。那么我们就能根据这些特点，这些先验，设计出好的算法，提高模型的泛化性能。卷积神经网络之所以在图像上那么成功，也是源于，它拟合的函数一定具有某种对称性（因为对图像平移不变），而这种对称性，就是真实图像对应的理想函数所具有的。**\n\n洞察真实数据是什么样的，是提高模型泛化能力的最终途径。\n\n### neural tangent kernel\n\n**最新有论文表示[^1]，泛化性能和正切核有关，还有关于神经网络特征值方面的研究。值得探索。**\n\n[^1]:NEURAL TANGENT KERNEL EIGENVALUES ACCURATELY PREDICT GENERALIZATION\n\n\n\n## 高次非线性与注意力机制\n\n### 特征图乘法\n\n特征图乘法会带来高次非线性。如果把ResNet里面的加法改成乘法。\n\n```python\nshortcut(x) + out\n# 修改为\nshortcut(x) * out\n```\n\n**可以从两个角度这个乘法**\n\n1. 把 高次非线性，引入到CNN中\n2. 把ResNet的分支做成一个3D的mask，形成一种无参数增加的attention module\n\n### 和数据集有关\n\n在有的数据集上效果挺好，比如Cifar-100。但是即使在Cifar-10上表现都一般。\n\n### 和优化有关？和表达的曲面性质有关？\n\n原因可能有两个\n\n1. 这样做乘法太粗糙，确实表达能力不行\n2. 优化的问题，乘法带来了梯度的问题，比如方差不可控等，导致优化的情况不好。\n\n现在其实也没能很清楚是是哪个问题，缺少研究这个问题的手段。\n\n为了减少优化上的缺陷。尝试过几种方法\n\n1. 为两端加bn。``bn(shortcut(x)) * bn(out)``\n\n2. 把bn的bias设置为1，保证bn(shortcut(x))和bn(out)的均值都是1，这样可以产生比较稳定的梯度。这个想法是源于 \n\n    **1. 原始的shortcut(x) + out。相当于shortcut(x)乘以了一个1的因子，out也乘以了一个1的因子。**\n\n   **2. 保持恒等映射。当out 是0的时候shorcut可以容易的实现恒等映射。**\n\n### sigmoid容易饱和？\n\n3. 像attention一样，把一边或者两边都用sigmoid函数。``bn(shortcut(x)) * out.sigmoid()``。这样的话，效果也不是很好，而且发现sigmoid 函数给人一种感觉。就是训练不充分的感觉，有点像学习率太小，或者减小的太快的感觉，所以陷入的局部最优点。猜想这是因为sigmoid函数太容易饱和了。\n4. soft attention经常利用sigmoid函数，是否也存在过饱和的情况。为什么它没有出现过饱和现象。1. 用的不多？ 2. 因为是2d或者1d的？就拿SE-Net举例，每个像素点其实也都是乘以了attention的。只是空间乘以的是同一个attention。\n\n\n\n","source":"_posts/Personal-Thought.md","raw":"---\ntitle: Personal Thought\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-15 16:01:28\npassword:\nsummary:\ndescription: 记录自己在学术方面的一些想法。\ncategories:\n- About Papers\ntags:\n- Personal Thought\n- Papers\n- private\n---\n\n\n\n# Deep Learning\n\n## 泛化性能\n\n### 泛化性能的本质是什么样的拟合曲面是好的，一定要依靠先验，现实中曲面有哪些特点。\n\n深度学习的本质就是函数的拟合。泛化性能是衡量深度学习模型性能的指标。\n\n在两个模型都完全拟合训练集的情况下，i.e.，两个模型所表示的函数在训练点处的函数值都是完全正确的。泛化能力要求两函数在测试点处也具有正确的函数值。\n\n这个问题其实只能依赖先验。对于拟合曲面的先验，也就是说，什么样的拟合曲面是好的。\n\n比如，如果点A处函数值为1，那么把A翻转，函数值也为1。这是源于目标识别常识的先验。\n\n数据增广方法mixup，发现不同图片之间函数值线性变换是好的。这是对函数曲面的另一个先验。在样本之间，函数线性变换好。\n\n所以，深度学习的本质问题是什么样的面是好的，如果我们知道什么样的面是好的。知道我们需要拟合的曲面的一些特点，那么就可以利用这些先验来进行数据增广，甚至进行模型的设计。\n\n比如，要拟合的曲面是几次的。是否需要高次非线性。如果需要我们就要引入特张图之间的乘法。强化学习的特征选择给模型带来另一种非线性。哪种非线性是好的？要回答这个问题，就要知道我们需要拟合的那个函数是几次的，是哪种非线性。\n\n**设想如果，有一种可视化方法，直接可视化高维数据。我们用它来可视化在MNIST, CIFAR-10等数据集上都达到100%准确率的函数。我们总结这些函数曲面有什么特点，光不光滑？是几次的？有哪些对称性？。总结出适用于所有数据集的共性的曲面特点。那么我们就能根据这些特点，这些先验，设计出好的算法，提高模型的泛化性能。卷积神经网络之所以在图像上那么成功，也是源于，它拟合的函数一定具有某种对称性（因为对图像平移不变），而这种对称性，就是真实图像对应的理想函数所具有的。**\n\n洞察真实数据是什么样的，是提高模型泛化能力的最终途径。\n\n### neural tangent kernel\n\n**最新有论文表示[^1]，泛化性能和正切核有关，还有关于神经网络特征值方面的研究。值得探索。**\n\n[^1]:NEURAL TANGENT KERNEL EIGENVALUES ACCURATELY PREDICT GENERALIZATION\n\n\n\n## 高次非线性与注意力机制\n\n### 特征图乘法\n\n特征图乘法会带来高次非线性。如果把ResNet里面的加法改成乘法。\n\n```python\nshortcut(x) + out\n# 修改为\nshortcut(x) * out\n```\n\n**可以从两个角度这个乘法**\n\n1. 把 高次非线性，引入到CNN中\n2. 把ResNet的分支做成一个3D的mask，形成一种无参数增加的attention module\n\n### 和数据集有关\n\n在有的数据集上效果挺好，比如Cifar-100。但是即使在Cifar-10上表现都一般。\n\n### 和优化有关？和表达的曲面性质有关？\n\n原因可能有两个\n\n1. 这样做乘法太粗糙，确实表达能力不行\n2. 优化的问题，乘法带来了梯度的问题，比如方差不可控等，导致优化的情况不好。\n\n现在其实也没能很清楚是是哪个问题，缺少研究这个问题的手段。\n\n为了减少优化上的缺陷。尝试过几种方法\n\n1. 为两端加bn。``bn(shortcut(x)) * bn(out)``\n\n2. 把bn的bias设置为1，保证bn(shortcut(x))和bn(out)的均值都是1，这样可以产生比较稳定的梯度。这个想法是源于 \n\n    **1. 原始的shortcut(x) + out。相当于shortcut(x)乘以了一个1的因子，out也乘以了一个1的因子。**\n\n   **2. 保持恒等映射。当out 是0的时候shorcut可以容易的实现恒等映射。**\n\n### sigmoid容易饱和？\n\n3. 像attention一样，把一边或者两边都用sigmoid函数。``bn(shortcut(x)) * out.sigmoid()``。这样的话，效果也不是很好，而且发现sigmoid 函数给人一种感觉。就是训练不充分的感觉，有点像学习率太小，或者减小的太快的感觉，所以陷入的局部最优点。猜想这是因为sigmoid函数太容易饱和了。\n4. soft attention经常利用sigmoid函数，是否也存在过饱和的情况。为什么它没有出现过饱和现象。1. 用的不多？ 2. 因为是2d或者1d的？就拿SE-Net举例，每个像素点其实也都是乘以了attention的。只是空间乘以的是同一个attention。\n\n\n\n","slug":"Personal-Thought","published":1,"updated":"2021-12-22T06:31:33.872Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f67000cvgulg32ravwr","content":"<div class=\"hbe hbe-container\" id=\"hexo-blog-encrypt\" data-wpm=\"Oh, this is an invalid password. Check and try again, please.\" data-whm=\"OOPS, these decrypted content may changed, but you can still have a look.\">\n  <script id=\"hbeData\" type=\"hbeData\" data-hmacdigest=\"98216d6198def0ba940ce9f6a3dfd619b8eb16d424225a3ab83f19f27dbb7633\">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</script>\n  <div class=\"hbe hbe-content\">\n    <div class=\"hbe hbe-input hbe-input-default\">\n      <input class=\"hbe hbe-input-field hbe-input-field-default\" type=\"password\" id=\"hbePass\">\n      <label class=\"hbe hbe-input-label hbe-input-label-default\" for=\"hbePass\">\n        <span class=\"hbe hbe-input-label-content hbe-input-label-content-default\">Hey, password is required here.</span>\n      </label>\n    </div>\n  </div>\n</div>\n<script data-pjax src=\"/myBlog/lib/hbe.js\"></script><link href=\"/myBlog/css/hbe.style.css\" rel=\"stylesheet\" type=\"text/css\">","site":{"data":{}},"excerpt":"这是一篇加密文章，需要密码才能继续阅读。","more":"这是一篇加密文章，需要密码才能继续阅读。","origin":"<html><head></head><body><h1 id=\"Deep-Learning\"><span class=\"post-title-index\">1. </span><a href=\"#Deep-Learning\" class=\"headerlink\" title=\"Deep Learning\"></a>Deep Learning</h1><h2 id=\"泛化性能\"><span class=\"post-title-index\">1.1. </span><a href=\"#泛化性能\" class=\"headerlink\" title=\"泛化性能\"></a>泛化性能</h2><h3 id=\"泛化性能的本质是什么样的拟合曲面是好的，一定要依靠先验，现实中曲面有哪些特点。\"><span class=\"post-title-index\">1.1.1. </span><a href=\"#泛化性能的本质是什么样的拟合曲面是好的，一定要依靠先验，现实中曲面有哪些特点。\" class=\"headerlink\" title=\"泛化性能的本质是什么样的拟合曲面是好的，一定要依靠先验，现实中曲面有哪些特点。\"></a>泛化性能的本质是什么样的拟合曲面是好的，一定要依靠先验，现实中曲面有哪些特点。</h3><p>深度学习的本质就是函数的拟合。泛化性能是衡量深度学习模型性能的指标。</p>\n<p>在两个模型都完全拟合训练集的情况下，i.e.，两个模型所表示的函数在训练点处的函数值都是完全正确的。泛化能力要求两函数在测试点处也具有正确的函数值。</p>\n<p>这个问题其实只能依赖先验。对于拟合曲面的先验，也就是说，什么样的拟合曲面是好的。</p>\n<p>比如，如果点A处函数值为1，那么把A翻转，函数值也为1。这是源于目标识别常识的先验。</p>\n<p>数据增广方法mixup，发现不同图片之间函数值线性变换是好的。这是对函数曲面的另一个先验。在样本之间，函数线性变换好。</p>\n<p>所以，深度学习的本质问题是什么样的面是好的，如果我们知道什么样的面是好的。知道我们需要拟合的曲面的一些特点，那么就可以利用这些先验来进行数据增广，甚至进行模型的设计。</p>\n<p>比如，要拟合的曲面是几次的。是否需要高次非线性。如果需要我们就要引入特张图之间的乘法。强化学习的特征选择给模型带来另一种非线性。哪种非线性是好的？要回答这个问题，就要知道我们需要拟合的那个函数是几次的，是哪种非线性。</p>\n<p><strong>设想如果，有一种可视化方法，直接可视化高维数据。我们用它来可视化在MNIST, CIFAR-10等数据集上都达到100%准确率的函数。我们总结这些函数曲面有什么特点，光不光滑？是几次的？有哪些对称性？。总结出适用于所有数据集的共性的曲面特点。那么我们就能根据这些特点，这些先验，设计出好的算法，提高模型的泛化性能。卷积神经网络之所以在图像上那么成功，也是源于，它拟合的函数一定具有某种对称性（因为对图像平移不变），而这种对称性，就是真实图像对应的理想函数所具有的。</strong></p>\n<p>洞察真实数据是什么样的，是提高模型泛化能力的最终途径。</p>\n<h3 id=\"neural-tangent-kernel\"><span class=\"post-title-index\">1.1.2. </span><a href=\"#neural-tangent-kernel\" class=\"headerlink\" title=\"neural tangent kernel\"></a>neural tangent kernel</h3><p><strong>最新有论文表示<sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup>，泛化性能和正切核有关，还有关于神经网络特征值方面的研究。值得探索。</strong></p>\n<h2 id=\"高次非线性与注意力机制\"><span class=\"post-title-index\">1.2. </span><a href=\"#高次非线性与注意力机制\" class=\"headerlink\" title=\"高次非线性与注意力机制\"></a>高次非线性与注意力机制</h2><h3 id=\"特征图乘法\"><span class=\"post-title-index\">1.2.1. </span><a href=\"#特征图乘法\" class=\"headerlink\" title=\"特征图乘法\"></a>特征图乘法</h3><p>特征图乘法会带来高次非线性。如果把ResNet里面的加法改成乘法。</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">shortcut(x) + out</span><br><span class=\"line\"><span class=\"comment\"># 修改为</span></span><br><span class=\"line\">shortcut(x) * out</span><br></pre></td></tr></tbody></table></figure>\n\n<p><strong>可以从两个角度这个乘法</strong></p>\n<ol>\n<li>把 高次非线性，引入到CNN中</li>\n<li>把ResNet的分支做成一个3D的mask，形成一种无参数增加的attention module</li>\n</ol>\n<h3 id=\"和数据集有关\"><span class=\"post-title-index\">1.2.2. </span><a href=\"#和数据集有关\" class=\"headerlink\" title=\"和数据集有关\"></a>和数据集有关</h3><p>在有的数据集上效果挺好，比如Cifar-100。但是即使在Cifar-10上表现都一般。</p>\n<h3 id=\"和优化有关？和表达的曲面性质有关？\"><span class=\"post-title-index\">1.2.3. </span><a href=\"#和优化有关？和表达的曲面性质有关？\" class=\"headerlink\" title=\"和优化有关？和表达的曲面性质有关？\"></a>和优化有关？和表达的曲面性质有关？</h3><p>原因可能有两个</p>\n<ol>\n<li>这样做乘法太粗糙，确实表达能力不行</li>\n<li>优化的问题，乘法带来了梯度的问题，比如方差不可控等，导致优化的情况不好。</li>\n</ol>\n<p>现在其实也没能很清楚是是哪个问题，缺少研究这个问题的手段。</p>\n<p>为了减少优化上的缺陷。尝试过几种方法</p>\n<ol>\n<li><p>为两端加bn。<code>bn(shortcut(x)) * bn(out)</code></p>\n</li>\n<li><p>把bn的bias设置为1，保证bn(shortcut(x))和bn(out)的均值都是1，这样可以产生比较稳定的梯度。这个想法是源于 </p>\n<p> <strong>1. 原始的shortcut(x) + out。相当于shortcut(x)乘以了一个1的因子，out也乘以了一个1的因子。</strong></p>\n<p><strong>2. 保持恒等映射。当out 是0的时候shorcut可以容易的实现恒等映射。</strong></p>\n</li>\n</ol>\n<h3 id=\"sigmoid容易饱和？\"><span class=\"post-title-index\">1.2.4. </span><a href=\"#sigmoid容易饱和？\" class=\"headerlink\" title=\"sigmoid容易饱和？\"></a>sigmoid容易饱和？</h3><ol start=\"3\">\n<li>像attention一样，把一边或者两边都用sigmoid函数。<code>bn(shortcut(x)) * out.sigmoid()</code>。这样的话，效果也不是很好，而且发现sigmoid 函数给人一种感觉。就是训练不充分的感觉，有点像学习率太小，或者减小的太快的感觉，所以陷入的局部最优点。猜想这是因为sigmoid函数太容易饱和了。</li>\n<li>soft attention经常利用sigmoid函数，是否也存在过饱和的情况。为什么它没有出现过饱和现象。1. 用的不多？ 2. 因为是2d或者1d的？就拿SE-Net举例，每个像素点其实也都是乘以了attention的。只是空间乘以的是同一个attention。</li>\n</ol>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style:none; padding-left: 0;\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">1.</span><span style=\"display: inline-block; vertical-align: top;\">NEURAL TANGENT KERNEL EIGENVALUES ACCURATELY PREDICT GENERALIZATION</span><a href=\"#fnref:1\" rev=\"footnote\"> ↩</a></li></ol></div></div></body></html>","encrypt":true},{"title":"Tips in Papers","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-14T08:33:03.000Z","password":null,"summary":null,"description":"读过的论文记录，记录读过论文里面，新获取的信息。","_content":"\n# Hard Attention\n\n## 2019 Scacader: Improving Accuracy of Hard Attention Models for Vision\n\n### Hard Attention选择相关的特征作为输入，是真正具有可解释性的。从下面第一段话，可以看出，hard attention是真正和性能挂钩的。也一定和特征重要性是吻合的。但是Soft Attention不具有可解释性，小权重的特征并不一定不重要<div id=\"ap\"></div> \n\n> Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.\n\n> Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the model's decision-making process, but the model's final decision may nonetheless rely on information provided by features with small weights [^1]\n\n[^1]:2018,Learn to pay attention.\n\n### 文中glimpse的含义，可以理解成每一个位置，即每一个patch，就是一个glimpse。\n\n> Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.\n\n### 文章提出了Hard Attention与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而Hard Attention是无标签的。\n\n> Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models. \n\n> These models operate by generating many region proposals and then applying a classification model to each proposal. \n\n> Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.\n\n**目标检测和Hard Attention的相似之处在于，他们都同时关注目标的位置和类别。相比于目标检测，Hard Attention可以做的更精细, i.e., 它可以像目标检测一样在图像域上挑选特征，它还可以在任意一个特征域里挑选特征；它不仅可以像目标检测一样，挑选空域的特征，还可以挑选通道域的特征。**\n\n### 总结构\n\n![](Saccader_Over.jpg)\n\n最上面的rep. net以及logits per location之前都属于representation network。这部分挺常规，但是使用了‘BagNet’[^2]的方法，该方法保持了输出特征图中每个像素的感受野大小。\n\n下面的atten. net就是几个卷积层的堆叠。没有attention机制。到Sacadder cell之前都是常规操作，除了一个what和where的concat得到mixed。\n\nSaccader cell是技术关键点。\n\n**coordinate at time t的slice操作，对于坐标的选择而言，是一个不可导的操作。这里是强化学习介入的地方。而且注意，这里t是一个序列，最后的prediction是求平均。**\n\n[^2]:2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.  \n\n### Saccader cell\n\n![](Saccader_Cell.jpg)\n\n值得注意的几点\n\n1. Cell state是一个state的序列，每个state都是一个经过2d softmax的logit。这个logit表示该state预测的位置。\n\n   > This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.\n\n2. 需要保证cell state中预测之间都是不同的。$C^t$记录了t时刻位置探索过的所有位置。那些位置的值是1。所以$C^{t - 1}$两次介入$C^t$的计算都乘以一个非常小的数$-10^5$ ，这样就保证了在2d softmax的时候，探索过的位置无法胜出。\n\n   > The cell includes a 2D state ($C^t$) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.\n\n   **从这里也看出state和sequence是不同的。state是记录探索过的位置。从途中可以看出state是非01的，而由1可知，sequence应该是图中右边产生logic。state的目的，其实也是为了产生logic。**\n\n3. 在制作$C^t$的过程中，信息来源有两个mixed feature和$C^{t - 1}$。最后得到的$C^t$通道是1，所以mixed feature空间维度的压缩是必然的。在压缩的时候，选用了channel attention机制。channel attention机制又需要先空间压缩，这里不像SE-Net一样直接压缩空间，而是又做了一个空间的mask 压缩空间，这个mask用了$C^{t - 1}$的信息，去除掉了已经探索的位置信息。\n\n4. > At test time, the model extracts the logits at time $t$ from the representation network at location $argmax_{i,j}(\\hat{R}^t_{i,j})$.The final prediction is obtained by averaging the extracted logits across all times.\n\n**Saccader Cell的关键就在于产生一系列的sequence。这些sequence可以用强化学习算法优化，使其可以预测物体的位置，从而就进行了feature的选择。**\n\n### 训练策略\n\n> The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.\n>\n> We performed a three step training procedure using only the training class lables as supervision.\n\n![](Saccader_eq1.jpg)\n\n1. 预训练了representation network\n\n   这个公式增大了目标$y_{target}$的概率。增大的某个位置上$y_{target}$的概率，而且所有位置连乘之后的概率。这里假设region of interest的概率将会被增大的最多。通过这种方法就自动学习到了一些好的position，在强化学习优化的时候，提供了一个好的探索的起点。\n\n   > Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.\n\n   > Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.\n\n![](Saccader_eq2.jpg)\n\n2. 训练了location network (attention network, $1 \\times 1$ mixing convolution and Sacader cell)\n\n   这是一个自监督的预训练。提高了前T = 12次预测的点的概率。\n\n   文中说排序了，我感觉排序的次序和预测的次序是一致的。第一次预测是概率最大的，最后一次预测是概率最小的。后面实验对比了该预训练对性能的影响。然而，<font color =red>现在还并不能完全了解这个预训练的作用机制。 </font>\n\n![](Saccader_eq3.jpg)\n\n3. > we trained the whole model to maximize the expected reward, where the reward ($r \\in \\{0, 1\\}$) represents whether the model final prediction after 6 glimpses (T = 6) is correct. \n\n<font color=red>这个公式并没有完全读懂。</font>\n\n用的是策略网络的方法。$l^t_s$是按saccader cell输出的位置概率生成的位置。公式的第一行就是增大这个概率的值。当模型最后做出了正确预测的时候，r权重大，损失函数以更大的权重增大这个预测的概率，这就是策略网络的方法。等式的第二行是修正分类网络，当预测正确的时候，那么有监督的增大这T个预测位置在$y_{target}$上的准确率。<div id=\"空间\"></div> \n\n**由公式1和公式3看出，对于图像空间，loss函数只是增大预测点$y_{target}$的概率，然后不会理会，空间上其他点预测的是什么。**\n\n如果saccader cell的性能非常好，那么模型是没问题的。如果saccader cell预测会有误差，比如抗干扰能力不好，预测到目标以外的patch上，那么性能会被影响。 \n\n**论文中说，[当预测的位置不存在的时候，模型的性能受到很大的影响。那么其实抑制预测之外的位置的，激活值会使得可以解释性更强？](#ap)**\n\n### 论文对比了ordered logits policy和Saccader\n\n> An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.\n\nOrdered logits policy选择了在空间上最大激活的位置，作为物体的位置，并把该预测作为物体类别的预测。\n\n> The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.\n>\n> However, accuracy is still lower than the learned Saccader model, and **performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)**\n\nordered logits policy的缺陷在于，只关注了最重要的少数的特征。它的预测都围绕在这些最重要的特征周围，所以增加glimpese的时候，很少新的信息引入，所以performance增长的很慢，而Saccader没有这个问题，它关注了完整的信息。\n\n### 空间上其他位置\n\n如[上文所说](#空间)，Sccader只优化选中的位置，不管没有被选中的位置。那么当增加glimplese的时候，它的性能应该会受到影响的。但是不是。\n\n> In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.\n\n![](Saccader_gl.jpg)\n\n如图可以看出，增大glimpses的时候，准确率反而在增加。说明在训练的时候，没有被训练到的地方，Saccader的预测也还可以，与[上文所说](#空间)的理解不同。<font color=red>原因是什么还有待探究。</font>\n\n## Hard Attention for Scalable Image Classification\n\n### 介绍\n\n![](Tnet_over.jpg)\n\n> Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).\n\n### 这篇文章使用和实现了Saccader\n\n# Regularization \n\n## ADCM: Attentnion Dropout Convolutional Module\n\n![ADCM](ADCM.jpg)\n\n在CBAM的基础上加入了正则化，把CBAM产生的attention weights作为Drop的概率引导，来对feature map进行drop。是一种对attention机制的正则化方法，很容以把它误解为hard attention。\n","source":"_posts/Tips-in-Papers.md","raw":"---\ntitle: Tips in Papers\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-14 16:33:03\npassword:\nsummary:\ndescription: 读过的论文记录，记录读过论文里面，新获取的信息。\ncategories:\n- About Papers\ntags:\n- Papers\n- Personal Thought\n---\n\n# Hard Attention\n\n## 2019 Scacader: Improving Accuracy of Hard Attention Models for Vision\n\n### Hard Attention选择相关的特征作为输入，是真正具有可解释性的。从下面第一段话，可以看出，hard attention是真正和性能挂钩的。也一定和特征重要性是吻合的。但是Soft Attention不具有可解释性，小权重的特征并不一定不重要<div id=\"ap\"></div> \n\n> Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.\n\n> Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the model's decision-making process, but the model's final decision may nonetheless rely on information provided by features with small weights [^1]\n\n[^1]:2018,Learn to pay attention.\n\n### 文中glimpse的含义，可以理解成每一个位置，即每一个patch，就是一个glimpse。\n\n> Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.\n\n### 文章提出了Hard Attention与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而Hard Attention是无标签的。\n\n> Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models. \n\n> These models operate by generating many region proposals and then applying a classification model to each proposal. \n\n> Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.\n\n**目标检测和Hard Attention的相似之处在于，他们都同时关注目标的位置和类别。相比于目标检测，Hard Attention可以做的更精细, i.e., 它可以像目标检测一样在图像域上挑选特征，它还可以在任意一个特征域里挑选特征；它不仅可以像目标检测一样，挑选空域的特征，还可以挑选通道域的特征。**\n\n### 总结构\n\n![](Saccader_Over.jpg)\n\n最上面的rep. net以及logits per location之前都属于representation network。这部分挺常规，但是使用了‘BagNet’[^2]的方法，该方法保持了输出特征图中每个像素的感受野大小。\n\n下面的atten. net就是几个卷积层的堆叠。没有attention机制。到Sacadder cell之前都是常规操作，除了一个what和where的concat得到mixed。\n\nSaccader cell是技术关键点。\n\n**coordinate at time t的slice操作，对于坐标的选择而言，是一个不可导的操作。这里是强化学习介入的地方。而且注意，这里t是一个序列，最后的prediction是求平均。**\n\n[^2]:2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.  \n\n### Saccader cell\n\n![](Saccader_Cell.jpg)\n\n值得注意的几点\n\n1. Cell state是一个state的序列，每个state都是一个经过2d softmax的logit。这个logit表示该state预测的位置。\n\n   > This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.\n\n2. 需要保证cell state中预测之间都是不同的。$C^t$记录了t时刻位置探索过的所有位置。那些位置的值是1。所以$C^{t - 1}$两次介入$C^t$的计算都乘以一个非常小的数$-10^5$ ，这样就保证了在2d softmax的时候，探索过的位置无法胜出。\n\n   > The cell includes a 2D state ($C^t$) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.\n\n   **从这里也看出state和sequence是不同的。state是记录探索过的位置。从途中可以看出state是非01的，而由1可知，sequence应该是图中右边产生logic。state的目的，其实也是为了产生logic。**\n\n3. 在制作$C^t$的过程中，信息来源有两个mixed feature和$C^{t - 1}$。最后得到的$C^t$通道是1，所以mixed feature空间维度的压缩是必然的。在压缩的时候，选用了channel attention机制。channel attention机制又需要先空间压缩，这里不像SE-Net一样直接压缩空间，而是又做了一个空间的mask 压缩空间，这个mask用了$C^{t - 1}$的信息，去除掉了已经探索的位置信息。\n\n4. > At test time, the model extracts the logits at time $t$ from the representation network at location $argmax_{i,j}(\\hat{R}^t_{i,j})$.The final prediction is obtained by averaging the extracted logits across all times.\n\n**Saccader Cell的关键就在于产生一系列的sequence。这些sequence可以用强化学习算法优化，使其可以预测物体的位置，从而就进行了feature的选择。**\n\n### 训练策略\n\n> The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.\n>\n> We performed a three step training procedure using only the training class lables as supervision.\n\n![](Saccader_eq1.jpg)\n\n1. 预训练了representation network\n\n   这个公式增大了目标$y_{target}$的概率。增大的某个位置上$y_{target}$的概率，而且所有位置连乘之后的概率。这里假设region of interest的概率将会被增大的最多。通过这种方法就自动学习到了一些好的position，在强化学习优化的时候，提供了一个好的探索的起点。\n\n   > Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.\n\n   > Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.\n\n![](Saccader_eq2.jpg)\n\n2. 训练了location network (attention network, $1 \\times 1$ mixing convolution and Sacader cell)\n\n   这是一个自监督的预训练。提高了前T = 12次预测的点的概率。\n\n   文中说排序了，我感觉排序的次序和预测的次序是一致的。第一次预测是概率最大的，最后一次预测是概率最小的。后面实验对比了该预训练对性能的影响。然而，<font color =red>现在还并不能完全了解这个预训练的作用机制。 </font>\n\n![](Saccader_eq3.jpg)\n\n3. > we trained the whole model to maximize the expected reward, where the reward ($r \\in \\{0, 1\\}$) represents whether the model final prediction after 6 glimpses (T = 6) is correct. \n\n<font color=red>这个公式并没有完全读懂。</font>\n\n用的是策略网络的方法。$l^t_s$是按saccader cell输出的位置概率生成的位置。公式的第一行就是增大这个概率的值。当模型最后做出了正确预测的时候，r权重大，损失函数以更大的权重增大这个预测的概率，这就是策略网络的方法。等式的第二行是修正分类网络，当预测正确的时候，那么有监督的增大这T个预测位置在$y_{target}$上的准确率。<div id=\"空间\"></div> \n\n**由公式1和公式3看出，对于图像空间，loss函数只是增大预测点$y_{target}$的概率，然后不会理会，空间上其他点预测的是什么。**\n\n如果saccader cell的性能非常好，那么模型是没问题的。如果saccader cell预测会有误差，比如抗干扰能力不好，预测到目标以外的patch上，那么性能会被影响。 \n\n**论文中说，[当预测的位置不存在的时候，模型的性能受到很大的影响。那么其实抑制预测之外的位置的，激活值会使得可以解释性更强？](#ap)**\n\n### 论文对比了ordered logits policy和Saccader\n\n> An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.\n\nOrdered logits policy选择了在空间上最大激活的位置，作为物体的位置，并把该预测作为物体类别的预测。\n\n> The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.\n>\n> However, accuracy is still lower than the learned Saccader model, and **performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)**\n\nordered logits policy的缺陷在于，只关注了最重要的少数的特征。它的预测都围绕在这些最重要的特征周围，所以增加glimpese的时候，很少新的信息引入，所以performance增长的很慢，而Saccader没有这个问题，它关注了完整的信息。\n\n### 空间上其他位置\n\n如[上文所说](#空间)，Sccader只优化选中的位置，不管没有被选中的位置。那么当增加glimplese的时候，它的性能应该会受到影响的。但是不是。\n\n> In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.\n\n![](Saccader_gl.jpg)\n\n如图可以看出，增大glimpses的时候，准确率反而在增加。说明在训练的时候，没有被训练到的地方，Saccader的预测也还可以，与[上文所说](#空间)的理解不同。<font color=red>原因是什么还有待探究。</font>\n\n## Hard Attention for Scalable Image Classification\n\n### 介绍\n\n![](Tnet_over.jpg)\n\n> Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).\n\n### 这篇文章使用和实现了Saccader\n\n# Regularization \n\n## ADCM: Attentnion Dropout Convolutional Module\n\n![ADCM](ADCM.jpg)\n\n在CBAM的基础上加入了正则化，把CBAM产生的attention weights作为Drop的概率引导，来对feature map进行drop。是一种对attention机制的正则化方法，很容以把它误解为hard attention。\n","slug":"Tips-in-Papers","published":1,"updated":"2021-12-22T08:29:56.008Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f69000dvgulgn16h2wx","content":"<html><head></head><body><h1 id=\"Hard-Attention\"><span class=\"post-title-index\">1. </span><a href=\"#Hard-Attention\" class=\"headerlink\" title=\"Hard Attention\"></a>Hard Attention</h1><h2 id=\"2019-Scacader-Improving-Accuracy-of-Hard-Attention-Models-for-Vision\"><span class=\"post-title-index\">1.1. </span><a href=\"#2019-Scacader-Improving-Accuracy-of-Hard-Attention-Models-for-Vision\" class=\"headerlink\" title=\"2019 Scacader: Improving Accuracy of Hard Attention Models for Vision\"></a>2019 Scacader: Improving Accuracy of Hard Attention Models for Vision</h2><h3 id=\"Hard-Attention选择相关的特征作为输入，是真正具有可解释性的。从下面第一段话，可以看出，hard-attention是真正和性能挂钩的。也一定和特征重要性是吻合的。但是Soft-Attention不具有可解释性，小权重的特征并不一定不重要\"><span class=\"post-title-index\">1.1.1. </span><a href=\"#Hard-Attention选择相关的特征作为输入，是真正具有可解释性的。从下面第一段话，可以看出，hard-attention是真正和性能挂钩的。也一定和特征重要性是吻合的。但是Soft-Attention不具有可解释性，小权重的特征并不一定不重要\" class=\"headerlink\" title=\"Hard Attention选择相关的特征作为输入，是真正具有可解释性的。从下面第一段话，可以看出，hard attention是真正和性能挂钩的。也一定和特征重要性是吻合的。但是Soft Attention不具有可解释性，小权重的特征并不一定不重要\"></a>Hard Attention选择相关的特征作为输入，是真正具有可解释性的。从下面第一段话，可以看出，hard attention是真正和性能挂钩的。也一定和特征重要性是吻合的。但是Soft Attention不具有可解释性，小权重的特征并不一定不重要<div id=\"ap\"></div></h3><blockquote>\n<p>Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.</p>\n</blockquote>\n<blockquote>\n<p>Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the model’s decision-making process, but the model’s final decision may nonetheless rely on information provided by features with small weights <sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></p>\n</blockquote>\n<h3 id=\"文中glimpse的含义，可以理解成每一个位置，即每一个patch，就是一个glimpse。\"><span class=\"post-title-index\">1.1.2. </span><a href=\"#文中glimpse的含义，可以理解成每一个位置，即每一个patch，就是一个glimpse。\" class=\"headerlink\" title=\"文中glimpse的含义，可以理解成每一个位置，即每一个patch，就是一个glimpse。\"></a>文中glimpse的含义，可以理解成每一个位置，即每一个patch，就是一个glimpse。</h3><blockquote>\n<p>Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.</p>\n</blockquote>\n<h3 id=\"文章提出了Hard-Attention与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而Hard-Attention是无标签的。\"><span class=\"post-title-index\">1.1.3. </span><a href=\"#文章提出了Hard-Attention与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而Hard-Attention是无标签的。\" class=\"headerlink\" title=\"文章提出了Hard Attention与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而Hard Attention是无标签的。\"></a>文章提出了Hard Attention与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而Hard Attention是无标签的。</h3><blockquote>\n<p>Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models. </p>\n</blockquote>\n<blockquote>\n<p>These models operate by generating many region proposals and then applying a classification model to each proposal. </p>\n</blockquote>\n<blockquote>\n<p>Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.</p>\n</blockquote>\n<p><strong>目标检测和Hard Attention的相似之处在于，他们都同时关注目标的位置和类别。相比于目标检测，Hard Attention可以做的更精细, i.e., 它可以像目标检测一样在图像域上挑选特征，它还可以在任意一个特征域里挑选特征；它不仅可以像目标检测一样，挑选空域的特征，还可以挑选通道域的特征。</strong></p>\n<h3 id=\"总结构\"><span class=\"post-title-index\">1.1.4. </span><a href=\"#总结构\" class=\"headerlink\" title=\"总结构\"></a>总结构</h3><p><img src=\"Saccader_Over.jpg\"></p>\n<p>最上面的rep. net以及logits per location之前都属于representation network。这部分挺常规，但是使用了‘BagNet’<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\">2</a></sup>的方法，该方法保持了输出特征图中每个像素的感受野大小。</p>\n<p>下面的atten. net就是几个卷积层的堆叠。没有attention机制。到Sacadder cell之前都是常规操作，除了一个what和where的concat得到mixed。</p>\n<p>Saccader cell是技术关键点。</p>\n<p><strong>coordinate at time t的slice操作，对于坐标的选择而言，是一个不可导的操作。这里是强化学习介入的地方。而且注意，这里t是一个序列，最后的prediction是求平均。</strong></p>\n<h3 id=\"Saccader-cell\"><span class=\"post-title-index\">1.1.5. </span><a href=\"#Saccader-cell\" class=\"headerlink\" title=\"Saccader cell\"></a>Saccader cell</h3><p><img src=\"Saccader_Cell.jpg\"></p>\n<p>值得注意的几点</p>\n<ol>\n<li><p>Cell state是一个state的序列，每个state都是一个经过2d softmax的logit。这个logit表示该state预测的位置。</p>\n<blockquote>\n<p>This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.</p>\n</blockquote>\n</li>\n<li><p>需要保证cell state中预测之间都是不同的。$C^t$记录了t时刻位置探索过的所有位置。那些位置的值是1。所以$C^{t - 1}$两次介入$C^t$的计算都乘以一个非常小的数$-10^5$ ，这样就保证了在2d softmax的时候，探索过的位置无法胜出。</p>\n<blockquote>\n<p>The cell includes a 2D state ($C^t$) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.</p>\n</blockquote>\n<p><strong>从这里也看出state和sequence是不同的。state是记录探索过的位置。从途中可以看出state是非01的，而由1可知，sequence应该是图中右边产生logic。state的目的，其实也是为了产生logic。</strong></p>\n</li>\n<li><p>在制作$C^t$的过程中，信息来源有两个mixed feature和$C^{t - 1}$。最后得到的$C^t$通道是1，所以mixed feature空间维度的压缩是必然的。在压缩的时候，选用了channel attention机制。channel attention机制又需要先空间压缩，这里不像SE-Net一样直接压缩空间，而是又做了一个空间的mask 压缩空间，这个mask用了$C^{t - 1}$的信息，去除掉了已经探索的位置信息。</p>\n</li>\n<li><blockquote>\n<p>At test time, the model extracts the logits at time $t$ from the representation network at location $argmax_{i,j}(\\hat{R}^t_{i,j})$.The final prediction is obtained by averaging the extracted logits across all times.</p>\n</blockquote>\n</li>\n</ol>\n<p><strong>Saccader Cell的关键就在于产生一系列的sequence。这些sequence可以用强化学习算法优化，使其可以预测物体的位置，从而就进行了feature的选择。</strong></p>\n<h3 id=\"训练策略\"><span class=\"post-title-index\">1.1.6. </span><a href=\"#训练策略\" class=\"headerlink\" title=\"训练策略\"></a>训练策略</h3><blockquote>\n<p>The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.</p>\n<p>We performed a three step training procedure using only the training class lables as supervision.</p>\n</blockquote>\n<p><img src=\"Saccader_eq1.jpg\"></p>\n<ol>\n<li><p>预训练了representation network</p>\n<p>这个公式增大了目标$y_{target}$的概率。增大的某个位置上$y_{target}$的概率，而且所有位置连乘之后的概率。这里假设region of interest的概率将会被增大的最多。通过这种方法就自动学习到了一些好的position，在强化学习优化的时候，提供了一个好的探索的起点。</p>\n<blockquote>\n<p>Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.</p>\n</blockquote>\n<blockquote>\n<p>Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.</p>\n</blockquote>\n</li>\n</ol>\n<p><img src=\"Saccader_eq2.jpg\"></p>\n<ol start=\"2\">\n<li><p>训练了location network (attention network, $1 \\times 1$ mixing convolution and Sacader cell)</p>\n<p>这是一个自监督的预训练。提高了前T = 12次预测的点的概率。</p>\n<p>文中说排序了，我感觉排序的次序和预测的次序是一致的。第一次预测是概率最大的，最后一次预测是概率最小的。后面实验对比了该预训练对性能的影响。然而，<font color=\"red\">现在还并不能完全了解这个预训练的作用机制。 </font></p>\n</li>\n</ol>\n<p><img src=\"Saccader_eq3.jpg\"></p>\n<ol start=\"3\">\n<li><blockquote>\n<p>we trained the whole model to maximize the expected reward, where the reward ($r \\in {0, 1}$) represents whether the model final prediction after 6 glimpses (T = 6) is correct. </p>\n</blockquote>\n</li>\n</ol>\n<p><font color=\"red\">这个公式并没有完全读懂。</font></p>\n<p>用的是策略网络的方法。$l^t_s$是按saccader cell输出的位置概率生成的位置。公式的第一行就是增大这个概率的值。当模型最后做出了正确预测的时候，r权重大，损失函数以更大的权重增大这个预测的概率，这就是策略网络的方法。等式的第二行是修正分类网络，当预测正确的时候，那么有监督的增大这T个预测位置在$y_{target}$上的准确率。</p><div id=\"空间\"></div> <p></p>\n<p><strong>由公式1和公式3看出，对于图像空间，loss函数只是增大预测点$y_{target}$的概率，然后不会理会，空间上其他点预测的是什么。</strong></p>\n<p>如果saccader cell的性能非常好，那么模型是没问题的。如果saccader cell预测会有误差，比如抗干扰能力不好，预测到目标以外的patch上，那么性能会被影响。 </p>\n<p><strong>论文中说，<a href=\"#ap\">当预测的位置不存在的时候，模型的性能受到很大的影响。那么其实抑制预测之外的位置的，激活值会使得可以解释性更强？</a></strong></p>\n<h3 id=\"论文对比了ordered-logits-policy和Saccader\"><span class=\"post-title-index\">1.1.7. </span><a href=\"#论文对比了ordered-logits-policy和Saccader\" class=\"headerlink\" title=\"论文对比了ordered logits policy和Saccader\"></a>论文对比了ordered logits policy和Saccader</h3><blockquote>\n<p>An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.</p>\n</blockquote>\n<p>Ordered logits policy选择了在空间上最大激活的位置，作为物体的位置，并把该预测作为物体类别的预测。</p>\n<blockquote>\n<p>The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.</p>\n<p>However, accuracy is still lower than the learned Saccader model, and <strong>performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)</strong></p>\n</blockquote>\n<p>ordered logits policy的缺陷在于，只关注了最重要的少数的特征。它的预测都围绕在这些最重要的特征周围，所以增加glimpese的时候，很少新的信息引入，所以performance增长的很慢，而Saccader没有这个问题，它关注了完整的信息。</p>\n<h3 id=\"空间上其他位置\"><span class=\"post-title-index\">1.1.8. </span><a href=\"#空间上其他位置\" class=\"headerlink\" title=\"空间上其他位置\"></a>空间上其他位置</h3><p>如<a href=\"#%E7%A9%BA%E9%97%B4\">上文所说</a>，Sccader只优化选中的位置，不管没有被选中的位置。那么当增加glimplese的时候，它的性能应该会受到影响的。但是不是。</p>\n<blockquote>\n<p>In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.</p>\n</blockquote>\n<p><img src=\"Saccader_gl.jpg\"></p>\n<p>如图可以看出，增大glimpses的时候，准确率反而在增加。说明在训练的时候，没有被训练到的地方，Saccader的预测也还可以，与<a href=\"#%E7%A9%BA%E9%97%B4\">上文所说</a>的理解不同。<font color=\"red\">原因是什么还有待探究。</font></p>\n<h2 id=\"Hard-Attention-for-Scalable-Image-Classification\"><span class=\"post-title-index\">1.2. </span><a href=\"#Hard-Attention-for-Scalable-Image-Classification\" class=\"headerlink\" title=\"Hard Attention for Scalable Image Classification\"></a>Hard Attention for Scalable Image Classification</h2><h3 id=\"介绍\"><span class=\"post-title-index\">1.2.1. </span><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><p><img src=\"Tnet_over.jpg\"></p>\n<blockquote>\n<p>Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).</p>\n</blockquote>\n<h3 id=\"这篇文章使用和实现了Saccader\"><span class=\"post-title-index\">1.2.2. </span><a href=\"#这篇文章使用和实现了Saccader\" class=\"headerlink\" title=\"这篇文章使用和实现了Saccader\"></a>这篇文章使用和实现了Saccader</h3><h1 id=\"Regularization\"><span class=\"post-title-index\">2. </span><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h1><h2 id=\"ADCM-Attentnion-Dropout-Convolutional-Module\"><span class=\"post-title-index\">2.1. </span><a href=\"#ADCM-Attentnion-Dropout-Convolutional-Module\" class=\"headerlink\" title=\"ADCM: Attentnion Dropout Convolutional Module\"></a>ADCM: Attentnion Dropout Convolutional Module</h2><p><img src=\"ADCM.jpg\" alt=\"ADCM\"></p>\n<p>在CBAM的基础上加入了正则化，把CBAM产生的attention weights作为Drop的概率引导，来对feature map进行drop。是一种对attention机制的正则化方法，很容以把它误解为hard attention。</p>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style:none; padding-left: 0;\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">1.</span><span style=\"display: inline-block; vertical-align: top;\">2018,Learn to pay attention.</span><a href=\"#fnref:1\" rev=\"footnote\"> ↩</a></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">2.</span><span style=\"display: inline-block; vertical-align: top;\">2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.</span><a href=\"#fnref:2\" rev=\"footnote\"> ↩</a></li></ol></div></div></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":"<h1 id=\"Hard-Attention\"><a href=\"#Hard-Attention\" class=\"headerlink\" title=\"Hard Attention\"></a>Hard Attention</h1><h2 id=\"2019-Scacader-Improving-Accuracy-of-Hard-Attention-Models-for-Vision\"><a href=\"#2019-Scacader-Improving-Accuracy-of-Hard-Attention-Models-for-Vision\" class=\"headerlink\" title=\"2019 Scacader: Improving Accuracy of Hard Attention Models for Vision\"></a>2019 Scacader: Improving Accuracy of Hard Attention Models for Vision</h2><h3 id=\"Hard-Attention选择相关的特征作为输入，是真正具有可解释性的。从下面第一段话，可以看出，hard-attention是真正和性能挂钩的。也一定和特征重要性是吻合的。但是Soft-Attention不具有可解释性，小权重的特征并不一定不重要\"><a href=\"#Hard-Attention选择相关的特征作为输入，是真正具有可解释性的。从下面第一段话，可以看出，hard-attention是真正和性能挂钩的。也一定和特征重要性是吻合的。但是Soft-Attention不具有可解释性，小权重的特征并不一定不重要\" class=\"headerlink\" title=\"Hard Attention选择相关的特征作为输入，是真正具有可解释性的。从下面第一段话，可以看出，hard attention是真正和性能挂钩的。也一定和特征重要性是吻合的。但是Soft Attention不具有可解释性，小权重的特征并不一定不重要\"></a>Hard Attention选择相关的特征作为输入，是真正具有可解释性的。从下面第一段话，可以看出，hard attention是真正和性能挂钩的。也一定和特征重要性是吻合的。但是Soft Attention不具有可解释性，小权重的特征并不一定不重要<div id=\"ap\"></div></h3><blockquote>\n<p>Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.We further demonstrate that occluding the image patches prposed by the Saccader model highly impairs classification, thus confirming these patches strong relevance to the classification task.</p>\n</blockquote>\n<blockquote>\n<p>Typical soft attention mechanisms rescale features at one or more stages of the network. The soft mask used for rescaling often to provide some insight into the model’s decision-making process, but the model’s final decision may nonetheless rely on information provided by features with small weights <sup id=\"fnref:1\"><a href=\"#fn:1\" rel=\"footnote\">1</a></sup></p>\n</blockquote>\n<h3 id=\"文中glimpse的含义，可以理解成每一个位置，即每一个patch，就是一个glimpse。\"><a href=\"#文中glimpse的含义，可以理解成每一个位置，即每一个patch，就是一个glimpse。\" class=\"headerlink\" title=\"文中glimpse的含义，可以理解成每一个位置，即每一个patch，就是一个glimpse。\"></a>文中glimpse的含义，可以理解成每一个位置，即每一个patch，就是一个glimpse。</h3><blockquote>\n<p>Models that employ hard attention make decisions based on only a subset of pixel in the input image, typically in the form of a series of glimpses.</p>\n</blockquote>\n<h3 id=\"文章提出了Hard-Attention与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而Hard-Attention是无标签的。\"><a href=\"#文章提出了Hard-Attention与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而Hard-Attention是无标签的。\" class=\"headerlink\" title=\"文章提出了Hard Attention与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而Hard Attention是无标签的。\"></a>文章提出了Hard Attention与双阶段目标检测算法的相似性。他们都是截取图像块作为输入，然后进行分类。不同的是，目标检测算法对于目标的位置是有标签的，而Hard Attention是无标签的。</h3><blockquote>\n<p>Altough our aim in this work is to perform classification with only image-level class labels, out approach bears some resembalance to two-stage object detection models. </p>\n</blockquote>\n<blockquote>\n<p>These models operate by generating many region proposals and then applying a classification model to each proposal. </p>\n</blockquote>\n<blockquote>\n<p>Unlike our work, these approaches use ground-truth bounding boxes to train the classification model, and modern architectures also use bounding boxes to supervise the proposal generator.</p>\n</blockquote>\n<p><strong>目标检测和Hard Attention的相似之处在于，他们都同时关注目标的位置和类别。相比于目标检测，Hard Attention可以做的更精细, i.e., 它可以像目标检测一样在图像域上挑选特征，它还可以在任意一个特征域里挑选特征；它不仅可以像目标检测一样，挑选空域的特征，还可以挑选通道域的特征。</strong></p>\n<h3 id=\"总结构\"><a href=\"#总结构\" class=\"headerlink\" title=\"总结构\"></a>总结构</h3><p><img src=\"Saccader_Over.jpg\"></p>\n<p>最上面的rep. net以及logits per location之前都属于representation network。这部分挺常规，但是使用了‘BagNet’<sup id=\"fnref:2\"><a href=\"#fn:2\" rel=\"footnote\">2</a></sup>的方法，该方法保持了输出特征图中每个像素的感受野大小。</p>\n<p>下面的atten. net就是几个卷积层的堆叠。没有attention机制。到Sacadder cell之前都是常规操作，除了一个what和where的concat得到mixed。</p>\n<p>Saccader cell是技术关键点。</p>\n<p><strong>coordinate at time t的slice操作，对于坐标的选择而言，是一个不可导的操作。这里是强化学习介入的地方。而且注意，这里t是一个序列，最后的prediction是求平均。</strong></p>\n<h3 id=\"Saccader-cell\"><a href=\"#Saccader-cell\" class=\"headerlink\" title=\"Saccader cell\"></a>Saccader cell</h3><p><img src=\"Saccader_Cell.jpg\"></p>\n<p>值得注意的几点</p>\n<ol>\n<li><p>Cell state是一个state的序列，每个state都是一个经过2d softmax的logit。这个logit表示该state预测的位置。</p>\n<blockquote>\n<p>This cell produces a sequence of locations. Elements in the sequence correctpond to target locations.</p>\n</blockquote>\n</li>\n<li><p>需要保证cell state中预测之间都是不同的。$C^t$记录了t时刻位置探索过的所有位置。那些位置的值是1。所以$C^{t - 1}$两次介入$C^t$的计算都乘以一个非常小的数$-10^5$ ，这样就保证了在2d softmax的时候，探索过的位置无法胜出。</p>\n<blockquote>\n<p>The cell includes a 2D state ($C^t$) that keeps memory of the visited locations until time t by placing 1 in the corresponding location in the cell state. We use this state to prevent the network from returning to previously seen locations.</p>\n</blockquote>\n<p><strong>从这里也看出state和sequence是不同的。state是记录探索过的位置。从途中可以看出state是非01的，而由1可知，sequence应该是图中右边产生logic。state的目的，其实也是为了产生logic。</strong></p>\n</li>\n<li><p>在制作$C^t$的过程中，信息来源有两个mixed feature和$C^{t - 1}$。最后得到的$C^t$通道是1，所以mixed feature空间维度的压缩是必然的。在压缩的时候，选用了channel attention机制。channel attention机制又需要先空间压缩，这里不像SE-Net一样直接压缩空间，而是又做了一个空间的mask 压缩空间，这个mask用了$C^{t - 1}$的信息，去除掉了已经探索的位置信息。</p>\n</li>\n<li><blockquote>\n<p>At test time, the model extracts the logits at time $t$ from the representation network at location $argmax_{i,j}(\\hat{R}^t_{i,j})$.The final prediction is obtained by averaging the extracted logits across all times.</p>\n</blockquote>\n</li>\n</ol>\n<p><strong>Saccader Cell的关键就在于产生一系列的sequence。这些sequence可以用强化学习算法优化，使其可以预测物体的位置，从而就进行了feature的选择。</strong></p>\n<h3 id=\"训练策略\"><a href=\"#训练策略\" class=\"headerlink\" title=\"训练策略\"></a>训练策略</h3><blockquote>\n<p>The goal of our training is to learn a policy that predicts a sequence of visual attentnion locations that is useful to the downsteam task (here image classification) in absence of location labels.</p>\n<p>We performed a three step training procedure using only the training class lables as supervision.</p>\n</blockquote>\n<p><img src=\"Saccader_eq1.jpg\"></p>\n<ol>\n<li><p>预训练了representation network</p>\n<p>这个公式增大了目标$y_{target}$的概率。增大的某个位置上$y_{target}$的概率，而且所有位置连乘之后的概率。这里假设region of interest的概率将会被增大的最多。通过这种方法就自动学习到了一些好的position，在强化学习优化的时候，提供了一个好的探索的起点。</p>\n<blockquote>\n<p>Key to Saccader is a pretraining step that require only class lables and provides initial attention locations for policy gradient optimization.</p>\n</blockquote>\n<blockquote>\n<p>Our pretraining procedure overcomes the sparse-reward problem that makes hard attention models difficult to optimize. It requires access to only class lables and prvides initial attention locations.These initial locations provide better rewards for the policy gradient learning.</p>\n</blockquote>\n</li>\n</ol>\n<p><img src=\"Saccader_eq2.jpg\"></p>\n<ol start=\"2\">\n<li><p>训练了location network (attention network, $1 \\times 1$ mixing convolution and Sacader cell)</p>\n<p>这是一个自监督的预训练。提高了前T = 12次预测的点的概率。</p>\n<p>文中说排序了，我感觉排序的次序和预测的次序是一致的。第一次预测是概率最大的，最后一次预测是概率最小的。后面实验对比了该预训练对性能的影响。然而，<font color =red>现在还并不能完全了解这个预训练的作用机制。 </font></p>\n</li>\n</ol>\n<p><img src=\"Saccader_eq3.jpg\"></p>\n<ol start=\"3\">\n<li><blockquote>\n<p>we trained the whole model to maximize the expected reward, where the reward ($r \\in {0, 1}$) represents whether the model final prediction after 6 glimpses (T = 6) is correct. </p>\n</blockquote>\n</li>\n</ol>\n<p><font color=red>这个公式并没有完全读懂。</font></p>\n<p>用的是策略网络的方法。$l^t_s$是按saccader cell输出的位置概率生成的位置。公式的第一行就是增大这个概率的值。当模型最后做出了正确预测的时候，r权重大，损失函数以更大的权重增大这个预测的概率，这就是策略网络的方法。等式的第二行是修正分类网络，当预测正确的时候，那么有监督的增大这T个预测位置在$y_{target}$上的准确率。<div id=\"空间\"></div> </p>\n<p><strong>由公式1和公式3看出，对于图像空间，loss函数只是增大预测点$y_{target}$的概率，然后不会理会，空间上其他点预测的是什么。</strong></p>\n<p>如果saccader cell的性能非常好，那么模型是没问题的。如果saccader cell预测会有误差，比如抗干扰能力不好，预测到目标以外的patch上，那么性能会被影响。 </p>\n<p><strong>论文中说，<a href=\"#ap\">当预测的位置不存在的时候，模型的性能受到很大的影响。那么其实抑制预测之外的位置的，激活值会使得可以解释性更强？</a></strong></p>\n<h3 id=\"论文对比了ordered-logits-policy和Saccader\"><a href=\"#论文对比了ordered-logits-policy和Saccader\" class=\"headerlink\" title=\"论文对比了ordered logits policy和Saccader\"></a>论文对比了ordered logits policy和Saccader</h3><blockquote>\n<p>An ordered logits policy uses the BagNet model to pick the top K locations based on the largest class logits.</p>\n</blockquote>\n<p>Ordered logits policy选择了在空间上最大激活的位置，作为物体的位置，并把该预测作为物体类别的预测。</p>\n<blockquote>\n<p>The ordered logits policy strats off with accuracy much higher than a random policy, suggesting that the patches it initially picks are meaningful to classification.</p>\n<p>However, accuracy is still lower than the learned Saccader model, and <strong>performacne improves only slowly with additional glimpese. The ordered logits policy is able to capture some of the features relevant to classification, but it is a greedy policy that produces glimpses that cluster around a few top features (i.e., with low image coverage)</strong></p>\n</blockquote>\n<p>ordered logits policy的缺陷在于，只关注了最重要的少数的特征。它的预测都围绕在这些最重要的特征周围，所以增加glimpese的时候，很少新的信息引入，所以performance增长的很慢，而Saccader没有这个问题，它关注了完整的信息。</p>\n<h3 id=\"空间上其他位置\"><a href=\"#空间上其他位置\" class=\"headerlink\" title=\"空间上其他位置\"></a>空间上其他位置</h3><p>如<a href=\"#%E7%A9%BA%E9%97%B4\">上文所说</a>，Sccader只优化选中的位置，不管没有被选中的位置。那么当增加glimplese的时候，它的性能应该会受到影响的。但是不是。</p>\n<blockquote>\n<p>In fact, increasing the number of glimpses beyond the number used for DRAM policy training leads to drop in performane, ulike the Saccader model that generalizes to greater umbers of glimpses.</p>\n</blockquote>\n<p><img src=\"Saccader_gl.jpg\"></p>\n<p>如图可以看出，增大glimpses的时候，准确率反而在增加。说明在训练的时候，没有被训练到的地方，Saccader的预测也还可以，与<a href=\"#%E7%A9%BA%E9%97%B4\">上文所说</a>的理解不同。<font color=red>原因是什么还有待探究。</font></p>\n<h2 id=\"Hard-Attention-for-Scalable-Image-Classification\"><a href=\"#Hard-Attention-for-Scalable-Image-Classification\" class=\"headerlink\" title=\"Hard Attention for Scalable Image Classification\"></a>Hard Attention for Scalable Image Classification</h2><h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><p><img src=\"Tnet_over.jpg\"></p>\n<blockquote>\n<p>Muti-scale processing in the proposed TNet architecture. Starting from level 1, the image is processed in low resolution to get a coarse description of its content (red cube). Extracted features are used for (hard) selection of image regions worth processing in higher resolution. The process is repeated recursively (here to 2 additional levels). Features from all levels are combined (arrows on the right) to create the final image representation used for classificaiton (blue cube).</p>\n</blockquote>\n<h3 id=\"这篇文章使用和实现了Saccader\"><a href=\"#这篇文章使用和实现了Saccader\" class=\"headerlink\" title=\"这篇文章使用和实现了Saccader\"></a>这篇文章使用和实现了Saccader</h3><h1 id=\"Regularization\"><a href=\"#Regularization\" class=\"headerlink\" title=\"Regularization\"></a>Regularization</h1><h2 id=\"ADCM-Attentnion-Dropout-Convolutional-Module\"><a href=\"#ADCM-Attentnion-Dropout-Convolutional-Module\" class=\"headerlink\" title=\"ADCM: Attentnion Dropout Convolutional Module\"></a>ADCM: Attentnion Dropout Convolutional Module</h2><p><img src=\"ADCM.jpg\" alt=\"ADCM\"></p>\n<p>在CBAM的基础上加入了正则化，把CBAM产生的attention weights作为Drop的概率引导，来对feature map进行drop。是一种对attention机制的正则化方法，很容以把它误解为hard attention。</p>\n<div id=\"footnotes\"><hr><div id=\"footnotelist\"><ol style=\"list-style:none; padding-left: 0;\"><li id=\"fn:1\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">1.</span><span style=\"display: inline-block; vertical-align: top;\">2018,Learn to pay attention.</span><a href=\"#fnref:1\" rev=\"footnote\"> ↩</a></li><li id=\"fn:2\"><span style=\"display: inline-block; vertical-align: top; padding-right: 10px;\">2.</span><span style=\"display: inline-block; vertical-align: top;\">2019, Approximating CNNs with bag-of-local-features models works surprisingly well on ImageNet.</span><a href=\"#fnref:2\" rev=\"footnote\"> ↩</a></li></ol></div></div>"},{"title":"Transformer and BERT","top":false,"cover":false,"toc":true,"mathjax":true,"date":"2021-12-18T02:00:56.000Z","password":null,"summary":null,"description":"作为NLP的入门，介绍Transformer和BERT.","_content":"","source":"_posts/Transformer-and-BERT.md","raw":"---\ntitle: Transformer and BERT\ntop: false\ncover: false\ntoc: true\nmathjax: true\ndate: 2021-12-18 10:00:56\npassword:\nsummary:\ndescription: 作为NLP的入门，介绍Transformer和BERT.\ncategories:\n- Natural Language Processing\ntags:\n- Natural Language Processing\n- Transformer\n- BERT\n---\n","slug":"Transformer-and-BERT","published":1,"updated":"2021-12-18T02:03:12.773Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckxk28f6b000hvgul9j0og86t","content":"<html><head></head><body></body></html>","site":{"data":{}},"excerpt":"<html><head></head><body></body></html>","more":""}],"PostAsset":[{"_id":"source/_posts/An-Introduction-to-Git/git.jpg","post":"ckxk28f600003vgulcnsib3rm","slug":"git.jpg","modified":1,"renderable":1},{"_id":"source/_posts/First-Step-to-RL/policy_network.py","post":"ckxk28f660009vgul7dlkdslm","slug":"policy_network.py","modified":1,"renderable":1},{"_id":"source/_posts/First-Step-to-RL/q_learning.py","post":"ckxk28f660009vgul7dlkdslm","slug":"q_learning.py","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/ADCM.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"ADCM.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_Cell.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_Cell.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq1.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_eq1.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq2.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_eq2.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_eq3.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_eq3.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_gl.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_gl.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Saccader_Over.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Saccader_Over.jpg","modified":1,"renderable":1},{"_id":"source/_posts/Tips-in-Papers/Tnet_over.jpg","post":"ckxk28f69000dvgulgn16h2wx","slug":"Tnet_over.jpg","modified":1,"renderable":1}],"PostCategory":[{"post_id":"ckxk28f5w0001vgul3cz5ei18","category_id":"ckxk28f620004vgulbzdg99xl","_id":"ckxk28f69000evgulghih80k5"},{"post_id":"ckxk28f650008vgulfvge1air","category_id":"ckxk28f6c000kvguld0yedyr8","_id":"ckxk28f6e000qvgul943e6p3r"},{"post_id":"ckxk28f660009vgul7dlkdslm","category_id":"ckxk28f6d000nvguldl55d8rm","_id":"ckxk28f6e000tvguld7y17v4t"},{"post_id":"ckxk28f67000cvgulg32ravwr","category_id":"ckxk28f6e000rvgulecm21eqa","_id":"ckxk28f6h000wvgula5on7jgi"},{"post_id":"ckxk28f600003vgulcnsib3rm","category_id":"ckxk28f67000avgul58bubcpm","_id":"ckxk28f6i0011vgul1ej49wni"},{"post_id":"ckxk28f600003vgulcnsib3rm","category_id":"ckxk28f6f000uvgul8ijm9jpg","_id":"ckxk28f6i0014vgul2k0uamn1"},{"post_id":"ckxk28f69000dvgulgn16h2wx","category_id":"ckxk28f6e000rvgulecm21eqa","_id":"ckxk28f6j0016vgul0qedbl6i"},{"post_id":"ckxk28f6b000hvgul9j0og86t","category_id":"ckxk28f6i0010vgul60i0cjje","_id":"ckxk28f6j001avgul9qiq6qx2"},{"post_id":"ckxk28f640007vgul77d21ugl","category_id":"ckxk28f67000avgul58bubcpm","_id":"ckxk28f6k001cvgulaywceds7"},{"post_id":"ckxk28f640007vgul77d21ugl","category_id":"ckxk28f6j0018vgul53s85s60","_id":"ckxk28f6k001evgul2nd62nr5"}],"PostTag":[{"post_id":"ckxk28f5w0001vgul3cz5ei18","tag_id":"ckxk28f630005vgul7b1g8o48","_id":"ckxk28f6c000ivgulh0xk5ksd"},{"post_id":"ckxk28f5w0001vgul3cz5ei18","tag_id":"ckxk28f67000bvgulhp7bdewl","_id":"ckxk28f6c000jvgul9ocb1osp"},{"post_id":"ckxk28f600003vgulcnsib3rm","tag_id":"ckxk28f69000gvgul5qspek8t","_id":"ckxk28f6d000mvgulgmcpcm4v"},{"post_id":"ckxk28f640007vgul77d21ugl","tag_id":"ckxk28f6c000lvgul06f017k1","_id":"ckxk28f6e000pvgulcvx35tl4"},{"post_id":"ckxk28f650008vgulfvge1air","tag_id":"ckxk28f6d000ovgul3g6m5a5t","_id":"ckxk28f6h000zvgul4mng3in1"},{"post_id":"ckxk28f650008vgulfvge1air","tag_id":"ckxk28f6e000svgul5izm0zxs","_id":"ckxk28f6i0012vgul6a6ydbzc"},{"post_id":"ckxk28f650008vgulfvge1air","tag_id":"ckxk28f6g000vvgulal2o9a9s","_id":"ckxk28f6i0015vgul8vcs7ato"},{"post_id":"ckxk28f660009vgul7dlkdslm","tag_id":"ckxk28f6h000yvgul27vm2ane","_id":"ckxk28f6j0017vgulaie4211c"},{"post_id":"ckxk28f67000cvgulg32ravwr","tag_id":"ckxk28f6d000ovgul3g6m5a5t","_id":"ckxk28f6k001fvgul7jkxanvc"},{"post_id":"ckxk28f67000cvgulg32ravwr","tag_id":"ckxk28f6j0019vgul3jk0atkl","_id":"ckxk28f6k001gvgul9hc39d15"},{"post_id":"ckxk28f67000cvgulg32ravwr","tag_id":"ckxk28f6g000vvgulal2o9a9s","_id":"ckxk28f6l001ivgul7rehcsis"},{"post_id":"ckxk28f69000dvgulgn16h2wx","tag_id":"ckxk28f6j0019vgul3jk0atkl","_id":"ckxk28f6l001kvgul7zcfceu3"},{"post_id":"ckxk28f69000dvgulgn16h2wx","tag_id":"ckxk28f6d000ovgul3g6m5a5t","_id":"ckxk28f6l001lvgulbild3jeg"},{"post_id":"ckxk28f6b000hvgul9j0og86t","tag_id":"ckxk28f6l001jvgul9bmoe1es","_id":"ckxk28f6m001ovgul3ba1062m"},{"post_id":"ckxk28f6b000hvgul9j0og86t","tag_id":"ckxk28f6m001mvgulbulk2n08","_id":"ckxk28f6m001pvgul99zwekk6"},{"post_id":"ckxk28f6b000hvgul9j0og86t","tag_id":"ckxk28f6m001nvgulfx5pc0xu","_id":"ckxk28f6m001qvgulf7wkgeix"}],"Tag":[{"name":"Algorithm","_id":"ckxk28f630005vgul7b1g8o48"},{"name":"Programming","_id":"ckxk28f67000bvgulhp7bdewl"},{"name":"Git","_id":"ckxk28f69000gvgul5qspek8t"},{"name":"Hexo","_id":"ckxk28f6c000lvgul06f017k1"},{"name":"Personal Thought","_id":"ckxk28f6d000ovgul3g6m5a5t"},{"name":"Experiments","_id":"ckxk28f6e000svgul5izm0zxs"},{"name":"private","_id":"ckxk28f6g000vvgulal2o9a9s"},{"name":"Reinforcement Learning","_id":"ckxk28f6h000yvgul27vm2ane"},{"name":"Papers","_id":"ckxk28f6j0019vgul3jk0atkl"},{"name":"Natural Language Processing","_id":"ckxk28f6l001jvgul9bmoe1es"},{"name":"Transformer","_id":"ckxk28f6m001mvgulbulk2n08"},{"name":"BERT","_id":"ckxk28f6m001nvgulfx5pc0xu"}]}}